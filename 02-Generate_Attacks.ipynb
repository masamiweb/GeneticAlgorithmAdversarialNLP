{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to latest model version number\n",
    "\n",
    "def set_model_version_number():\n",
    "    version_number = []\n",
    "    global MODEL_VERSION\n",
    "    global MODEL_PATH\n",
    "\n",
    "    if os.path.exists(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):   \n",
    "        for entry in os.listdir(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):\n",
    "            version_number.append(entry)       \n",
    "        MODEL_VERSION = version_number[-1]\n",
    "        MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config/main.conf')\n",
    "\n",
    "DATASET = 1\n",
    "MODEL_VERSION =  \"0001\"\n",
    "DOWNLOAD_GOOGLE_LM = False\n",
    "\n",
    "if DATASET == 1:\n",
    "    set_dataset = \"imdb\"\n",
    "if DATASET == 2:\n",
    "    set_dataset = \"s140\"\n",
    "\n",
    "DATASET_URL = (config[set_dataset]['DATASET_URL'])\n",
    "\n",
    "DATASET_FOLDER = config[set_dataset]['DATASET_FOLDER']\n",
    "DATASET_TAR_FILE_NAME = config[set_dataset]['DATASET_TAR_FILE_NAME']\n",
    "DATASET_NAME = config[set_dataset]['DATASET_NAME']\n",
    "\n",
    "MODEL_NAME = config[set_dataset]['MODEL_NAME']\n",
    "\n",
    "CLEAN_DATA_FILE = os.path.join(DATASET_FOLDER,\"normalized_dataset.csv\")\n",
    "TAR_FILE_PATH = os.path.join(DATASET_FOLDER,DATASET_TAR_FILE_NAME)\n",
    "DATA_SET_LOCATION = os.path.join(DATASET_FOLDER,DATASET_NAME)\n",
    "\n",
    "MODEL_SAVE_DIRECTORY = config[set_dataset]['MODEL_SAVE_DIRECTORY']\n",
    "# Create the model save directory\n",
    "if not os.path.exists(MODEL_SAVE_DIRECTORY):\n",
    "    os.makedirs(MODEL_SAVE_DIRECTORY)\n",
    "    \n",
    "IMAGE_SAVE_FOLDER = config[set_dataset]['IMAGE_SAVE_FOLDER']\n",
    "    \n",
    "GLOVE_EMBEDDINGS = config[set_dataset]['GLOVE_EMBEDDINGS']\n",
    "COUNTER_FITTED_VECTORS = config[set_dataset]['COUNTER_FITTED_VECTORS']\n",
    "\n",
    "GLOVE_EMBEDDINGS_MATRIX = config[set_dataset]['GLOVE_EMBEDDINGS_MATRIX']\n",
    "COUNTER_FITTED_EMBEDDINGS_MATRIX = config[set_dataset]['COUNTER_FITTED_EMBEDDINGS_MATRIX']\n",
    "\n",
    "LM_URLS = config[set_dataset]['LM_URLS']\n",
    "LM_DIRECTORY = config[set_dataset]['LM_DIRECTORY']\n",
    "\n",
    "####### files required to reconstruct the final trained model ##############################\n",
    "MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "set_model_version_number()\n",
    "\n",
    "ASSESTS_FOLDER = os.path.join(MODEL_PATH,\"assets\")\n",
    "MODEL_ASSETS_VOCABULARY_FILE = os.path.join(ASSESTS_FOLDER,\"vocab\")\n",
    "MODEL_ASSETS_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"imdb_glove_embeddings_matrix\")\n",
    "MODEL_ASSETS_COUNTER_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"counter_embeddings_matrix\")\n",
    "MODEL_ASSETS_DISTANCE_MATRIX = os.path.join(ASSESTS_FOLDER,\"distance_matrix.npy\")\n",
    "MODEL_ASSETS_SAVE_BEST_WEIGHTS = os.path.join(ASSESTS_FOLDER, \"cp.ckpt\")\n",
    "MODEL_TRAINING_HISORTY_FILE = os.path.join(ASSESTS_FOLDER, \"training_history.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01) Load our pre trained sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7efc64141d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # in Tensorflow 2.1 and above\n",
    "import pickle \n",
    "\n",
    "MAX_VOCABULARY_SIZE = 50000\n",
    "DIMENSION = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "from manny_modules import tf_normalize_data as tfnd\n",
    "from manny_modules import return_model as rmodel\n",
    "\n",
    "saved_vocab = pickle.load(open(MODEL_ASSETS_VOCABULARY_FILE, 'rb'))\n",
    "saved_word_index = dict(zip(saved_vocab, range(len(saved_vocab))))\n",
    "\n",
    "saved_embeddings_matric = pickle.load(open(MODEL_ASSETS_EMBEDDINGS_FILE, 'rb'))\n",
    "\n",
    "\n",
    "vectorizer_layer = TextVectorization(\n",
    "    standardize=tfnd.normlize_data, \n",
    "    max_tokens=MAX_VOCABULARY_SIZE, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=300)\n",
    "\n",
    "# build vocabulary, will also run the normalize_data() \n",
    "vectorizer_layer.set_vocabulary(saved_vocab)\n",
    "\n",
    "\n",
    "saved_model = rmodel.create_model(vectorizer_layer,\n",
    "                                  saved_embeddings_matric,\n",
    "                                  saved_vocab,\n",
    "                                  dimension=DIMENSION, \n",
    "                                  lrate=LEARNING_RATE)\n",
    "\n",
    "# load the weights\n",
    "saved_model.load_weights(MODEL_ASSETS_SAVE_BEST_WEIGHTS) # loads best weights saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model - check predictions for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive confidence:  [[0.01855881]]  Negative confidence:  [[0.9814412]]\n",
      "Positive confidence:  [[0.9553545]]  Negative confidence:  [[0.04464549]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p_2 = [[\"Seriously, don't bother if you're over 12. This looks like a kids show designed purely to sell merchandise, theme park rides, etc. No logic, holes all over the shop, no characters motivation and really crap acting to top it off... just rubbish, really\"]]\n",
    "prob_positive = saved_model.predict(p_2)\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"It's one thing to bring back elements, characters, settings and stories, and to flash them in front of the audience to cash in on the nostalgia and/or recognisable memorabilia but without using it to further the plot and other to do exactly the opposite. It was about time that Star Wars directives understood that it is too unique a product to be lend to corporate filmmakers. Star Wars needs to be understood and its uniqueness has to be acknowledged in order to make the new stories feel like they belong. This may sound too obvious but if you ever wondered why the new SW movies are so controversial this may be the reason.Like with 'Spider-Man: Into the Spider-verse (2018)' and their comicbook-industry experts participation, the creators behind The Mandalorian were experts of the industry, connoisseurs of the Star Wars Universe and even long time fans. So they were able to not only recapture the aesthetic of the grimy, battered Star Wars but also build upon it taking the most 'subtle' things into account. Things like the predominancy of puppets and practical effects over CGI, settings you can feel and touch over green screens and the abundancy of not only known elements previously seen in Star Wars, but a whole batch of new creatures, designs and overall plot elements that felt like they belong to this universe and had always been there. Exceeding expectations are not only the visual aspects but the narrative too. It might be too late for some story elements now, but it is of great importance that from now on you try to watch the unraveling of the story unspoiled. I was lucky to have seen the premiere of the show before the 'memefication' of a certain 'element' that went viral and became one of the biggest highlights of the show. But for me I saw the reveal of this element unspoiled and I was pleasantly shocked, a memory I'll always carry with me. The ability of these creators to generate such shock value and deep moments it's often baffling to me. This is proof that the creators behind the narrative are fully aware of the complexities of the universe they are tampering with and like an experienced surgeon, they are able to tweak, traverse and call back any Star Wars element as they please and with astonishing results.\"]]\n",
    "prob_positive = saved_model.predict(p)\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the distance matrix from disk (load this before running below tests)\n",
    "- This is a large file (~20GB), so will take time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_matrix = np.load(MODEL_ASSETS_DISTANCE_MATRIX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word =  saved_word_index['scotland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import nearest_neighbour as nn\n",
    "\n",
    "nearest_neighbour, distance_to_neighbour = nn.closest_neighbours(target_word, distance_matrix, number_of_words_to_return=5, max_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words closest to `scotland` are `['scot', 'scots', 'scottish', 'scotsman', 'scotch']` \n"
     ]
    }
   ],
   "source": [
    "closest_word = [saved_vocab[x] for x in nearest_neighbour]\n",
    "\n",
    "print(\"Words closest to `%s` are `%s` \" % (saved_vocab[target_word], closest_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02) Load data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the saved sample dataset - use the same dataset for both BERT and distance matrix GA attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "dtypes = {'sentiment': 'int', 'text': 'str'}\n",
    "\n",
    "# if True then load saved sample dataset, else create a sample dataset\n",
    "SAVED_DATASET = True\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "def load_sample_dataset(): \n",
    "    global SAMPLE_SIZE\n",
    "    if SAVED_DATASET:\n",
    "        # load the same data-sample used in original attack\n",
    "        data_sample = pd.read_csv('imdb_dataset/sample_dataset.csv',dtype=dtypes).dropna()\n",
    "        print(\"Number of data items in sample: \",len(data_sample))\n",
    "        SAMPLE_SIZE = len(data_sample)\n",
    "    else:\n",
    "        #generate new sample from original data set\n",
    "        data_frame = pd.read_csv(CLEAN_DATA_FILE,dtype=dtypes)\n",
    "        data_sample = data_frame.sample(n = SAMPLE_SIZE).dropna()\n",
    "        \n",
    "        # if we generate a new sample dataset - save it with a date stamp\n",
    "        data_sample.to_csv('imdb_dataset/sample_dataset_'+ str(date.today()) + '_.csv', index = False)\n",
    "        SAMPLE_SIZE = len(data_sample)\n",
    "    return data_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new columns to hold results after genetic attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ga_columns(data_sample):\n",
    "\n",
    "    data_sample['probs'] = 0\n",
    "    data_sample['probs'] = data_sample['probs'].astype(float) # has to be of type float, to store probability values\n",
    "    \n",
    "    # copy current sentiment values to new columns, we can then later go through and compare any values that have been changed during the GA attack\n",
    "    data_sample['ga_sentiment'] = data_sample['sentiment']\n",
    "\n",
    "    # create new ga_text to hold perturbed text\n",
    "    data_sample['ga_text'] = \"\"\n",
    "    \n",
    "    # create ga_probs to hold new probability values after GA Attack, fill with current values\n",
    "    data_sample['ga_probs'] = 0.0\n",
    "    data_sample['ga_probs'] = data_sample['ga_probs'].astype(float)\n",
    "\n",
    "    # create ga_num_changes to hold the number of words changed\n",
    "    data_sample['ga_num_changes'] = 0\n",
    "    data_sample['ga_num_changes'] = data_sample['ga_num_changes'].astype(int)\n",
    "\n",
    "    # create ga_lev_ratio to hold the Levenshtein ratio\n",
    "    data_sample['ga_lev_ratio'] = 0.0\n",
    "    data_sample['ga_lev_ratio'] = data_sample['ga_lev_ratio'].astype(float)\n",
    "\n",
    "    # add field to indicate if sentiment was flipped on review text\n",
    "    data_sample['ga_flipped_sentiment'] = 'N'\n",
    "    data_sample['ga_flipped_sentiment'] = data_sample['ga_flipped_sentiment'].astype(str)\n",
    "\n",
    "\n",
    "    # percentage of words changed in sentence\n",
    "    data_sample['ga_percent_change'] = 0.0\n",
    "    data_sample['ga_percent_change'] = data_sample['ga_percent_change'].astype(float)\n",
    "\n",
    "    data_sample = data_sample.reset_index(drop=True) # reindex so we start from 0 in the sample data set\n",
    "    \n",
    "    return data_sample\n",
    "\n",
    "def add_ga_columns_stats(data_sample):\n",
    "\n",
    "    data_sample['probs'] = 0\n",
    "    data_sample['probs'] = data_sample['probs'].astype(float) # has to be of type float, to store probability values\n",
    "    \n",
    "    # copy current sentiment values to new columns, we can then later go through and compare any values that have been changed during the GA attack\n",
    "    data_sample['ga_sentiment'] = data_sample['sentiment']\n",
    "\n",
    "    # create new ga_text to hold perturbed text\n",
    "    data_sample['ga_text'] = data_sample['text']\n",
    "    \n",
    "    \n",
    "    # create ga_probs to hold new probability values after GA Attack, fill with current values\n",
    "    data_sample['ga_probs'] = 0.0\n",
    "    data_sample['ga_probs'] = data_sample['ga_probs'].astype(float)\n",
    "\n",
    "    # create ga_num_changes to hold the number of words changed\n",
    "    data_sample['ga_num_changes'] = 0\n",
    "    data_sample['ga_num_changes'] = data_sample['ga_num_changes'].astype(int)\n",
    "\n",
    "    # create ga_lev_ratio to hold the Levenshtein ratio\n",
    "    data_sample['ga_lev_ratio'] = 0.0\n",
    "    data_sample['ga_lev_ratio'] = data_sample['ga_lev_ratio'].astype(float)\n",
    "\n",
    "    # add field to indicate if sentiment was flipped on review text\n",
    "    data_sample['ga_flipped_sentiment'] = 'N'\n",
    "    data_sample['ga_flipped_sentiment'] = data_sample['ga_flipped_sentiment'].astype(str)\n",
    "\n",
    "\n",
    "    # percentage of words changed in sentence\n",
    "    data_sample['ga_percent_change'] = 0.0\n",
    "    data_sample['ga_percent_change'] = data_sample['ga_percent_change'].astype(float)\n",
    "    \n",
    "    # create ga_num_changes to hold the number of words changed\n",
    "    data_sample['ga_generations'] = 0\n",
    "    data_sample['ga_generations'] = data_sample['ga_generations'].astype(int)\n",
    "    \n",
    "    # create ga_lev_ratio to hold the Levenshtein ratio\n",
    "    data_sample['ga_time_taken'] = 0\n",
    "    data_sample['ga_time_taken'] = data_sample['ga_time_taken'].astype(int)\n",
    "\n",
    "    data_sample = data_sample.reset_index(drop=True) # reindex so we start from 0 in the sample data set\n",
    "    \n",
    "    return data_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now run each one against model and store current probabilities that text is a positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_current_probabilities(data_sample): \n",
    "    print(\"Calculating probabilities for sample dataset...\")\n",
    "    for i in data_sample.index:\n",
    "        p = saved_model.predict([data_sample.iloc[i]['text']])\n",
    "        data_sample.at[i,'probs']= p   \n",
    "        \n",
    "    # copy over current values to ga_probs column\n",
    "    data_sample['ga_probs'] = data_sample['probs']\n",
    "    return data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the predictions are correct, if not then drop row from data set\n",
    "### we only want to keep correctly classified data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_predictions(data_sample):\n",
    "    \n",
    "    original_size = len(data_sample)\n",
    "    drop_indexes = []\n",
    "    print(\"Checking proabilities for sample dataset are correctly labelled...\")\n",
    "    for i in data_sample.index:\n",
    "        if data_sample.iloc[i]['sentiment'] == 1 and data_sample.iloc[i]['probs'] > 0.5:\n",
    "            continue\n",
    "        if data_sample.iloc[i]['sentiment'] == 0 and data_sample.iloc[i]['probs'] <= 0.5:\n",
    "            continue\n",
    "        else:\n",
    "            drop_indexes.append(i)\n",
    "    # drop data items that are not correctly classified by our model\n",
    "    # for our test sample we want to be sure all samples start out correctly classified\n",
    "    data_sample = data_sample.drop(drop_indexes)\n",
    "    data_sample = data_sample.reset_index(drop=True) # reindex dataframe to start from 0\n",
    "\n",
    "    # check how many rows we dropped due to incorrect classification\n",
    "    print(\"Number of data items dropped from sample: \",(original_size - len(data_sample)))\n",
    "    print(\"Number of data items kept in sample: \",(len(data_sample)))\n",
    "    \n",
    "    return data_sample\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03) GA functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from manny_modules import nearest_neighbour as nn\n",
    "from random import randrange\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# TODO: READ\n",
    "# https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/\n",
    "\n",
    "bertMaskedLM = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "bertMaskedLM.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "#TODO: READ\n",
    "# https://huggingface.co/bert-base-cased?text=I+%5BMASK%5D+salad+for+lunch\n",
    "#https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "# create pipeline to process masked words\n",
    "# also re-try using bert-large-uncased\n",
    "# mask_word_pipeline = pipeline('fill-mask', model='bert-large-uncased') \n",
    "mask_word_pipeline = pipeline('fill-mask', model='bert-large-uncased')  \n",
    "# TODO: READ\n",
    "# https://huggingface.co/transformers/main_classes/pipelines.html#:~:text=These%20pipelines%20are%20objects%20that,Feature%20Extraction%20and%20Question%20Answering.\n",
    "\n",
    "\n",
    "\n",
    "# # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#################### START BERT functions #############################################################\n",
    "\n",
    "def return_masked_words(p, masked_sentence):\n",
    "    '''return list of possible masked words'''\n",
    "    return p(masked_sentence)\n",
    "\n",
    "def get_sentence_score(sentence):\n",
    "    ''' get a sentence score: the lower the score i.e. more valid the sentence'''\n",
    "    tokenized_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_input)])\n",
    "    predictions = bertMaskedLM(tensor_input)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(predictions.squeeze(),tensor_input.squeeze()).data\n",
    "    return math.exp(loss)\n",
    "\n",
    "#################### END BERT functions #############################################################\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#################### START GA functions #############################################################\n",
    "\n",
    "def fitness(population_list, model):\n",
    "    \n",
    "    # dataframe to store population text and probabilities\n",
    "    population_probs = pd.DataFrame(columns=['ga_text','ga_probs'])\n",
    "    \n",
    "    # make sure columns have the correct types\n",
    "    population_probs['ga_text'] = population_probs['ga_text'].astype(str)\n",
    "    population_probs['ga_probs'] = population_probs['ga_probs'].astype(float)\n",
    "    \n",
    "    for i in range(len(population_list)):\n",
    "        new_row = {'ga_text':population_list[i], 'ga_probs':model.predict([population_list[i]])}\n",
    "        #append row to the dataframe\n",
    "        population_probs = population_probs.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # sort the array by probabilities column, i.e column 2 \n",
    "    return  population_probs.sort_values('ga_probs') # return sorted df, sorted by probability lowest to highest\n",
    "    \n",
    "\n",
    "def found_solution(target_label, population_df):\n",
    "        \n",
    "    if target_label == 1:\n",
    "        if population_df.iloc[-1]['ga_probs'] > 0.5:\n",
    "            return True\n",
    "    \n",
    "    if target_label == 0:\n",
    "        if population_df.iloc[0]['ga_probs'] <= 0.5:\n",
    "            return True \n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "def number_of_changes_made(review_before, review_after):\n",
    "    word_count = 0\n",
    "    r_before = review_before.split()\n",
    "    r_after = review_after.split()\n",
    "    for i in range(len(r_before)):\n",
    "        if r_before[i] == r_after[i]:\n",
    "            pass\n",
    "        else:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def prediction_probability(model, text_review):\n",
    "    '''return probability of this string being a positive sentiment'''\n",
    "    return model.predict([text_review])\n",
    "\n",
    "\n",
    "def crossover(parent_one, parent_two):\n",
    "    p_one = parent_one.split()\n",
    "    p_two = parent_two.split()\n",
    "    \n",
    "    new_offspring = p_one.copy()\n",
    "    \n",
    "    text_len = min(len(new_offspring), len(p_two))\n",
    "    # use random uniform distribution to select which words to replace \n",
    "    # when creating the new offspring for our two parent strings\n",
    "    # Draw samples from a uniform distribution, i.e. each word is equally likely to be selected\n",
    "    # probability density function:  p(x) = 1 / (b - a)\n",
    "    for i in range(text_len):\n",
    "        if np.random.uniform() < 0.5:\n",
    "            new_offspring[i] = p_two[i]\n",
    "    return ' '.join(new_offspring)\n",
    "    \n",
    "    \n",
    "def mutation_distance(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):\n",
    "    '''returns the string after swapping max_perturbations nearest neighbours\n",
    "    of each string'''\n",
    "    \n",
    "    # keep track of list index of the which word we have already changed\n",
    "    selected_index = []\n",
    "    \n",
    "    #split string so we can iterate over each word\n",
    "    t_split = text_review.split()\n",
    "    \n",
    "    # select a random index value\n",
    "    indx = randrange(len(t_split))\n",
    "    \n",
    "    for i in range(max_perturbations):\n",
    "        # get a random index number for word list\n",
    "        indx = randrange(len(t_split))\n",
    "        \n",
    "        found_in_vocab = False\n",
    "        # skip over all stop words and any indexes we have already selected\n",
    "        while t_split[indx] in stop_words or indx in selected_index  or not found_in_vocab:     \n",
    "            \n",
    "            indx = randrange(len(t_split))\n",
    "            # if word is not found in vocabulary, skip it and try next word\n",
    "            try:\n",
    "                target_word = saved_word_index[t_split[indx]]\n",
    "                found_in_vocab = True\n",
    "            except KeyError:\n",
    "                found_in_vocab = False\n",
    "        \n",
    "        # now we have a word that is not a stop word and has not already been selected\n",
    "        selected_index.append(indx) # add to our list\n",
    "        \n",
    "        # we want to now get a list of the closest max_neighbours synonyms\n",
    "        target_word = saved_word_index[t_split[indx]]\n",
    "     \n",
    "        nearest_neighbour, _ = nn.closest_neighbours(target_word, distance_matrix, number_of_words_to_return=max_neighbours)\n",
    "        \n",
    "        # create a list of the closest words returned\n",
    "        closest_word = [saved_vocab[x] for x in nearest_neighbour]\n",
    "        \n",
    "        # now we need to substitute each word and find the new probability after each substitution\n",
    "        # we need the original label and the target label we are aiming for\n",
    "       \n",
    "        original_word = t_split[indx]\n",
    "        word_prob_dict = dict()\n",
    "        word_prob_dict[original_word] = model.predict([' '.join(t_split)])\n",
    "        for w in closest_word:\n",
    "            if not w: # if we have an empty string then do nothing, we don't want to remove a word from the string\n",
    "                continue\n",
    "            t_split[indx] = w\n",
    "            word_prob_dict[w] = model.predict([' '.join(t_split)])\n",
    "            \n",
    "        # didn't find any suitable words\n",
    "        if len(word_prob_dict) == 0:\n",
    "            continue\n",
    "        \n",
    "        # the word we decided to substitute is based on the probability returned by the model\n",
    "        # if we have target_label == 1 then we are trying to go from negative to positive sentiment\n",
    "        # therefore we want to keep the highest probability returned\n",
    "        # NB the probability returned by our model is the probability that the review is positive, higher values == more positive sentiment\n",
    "        if target_label == 1:\n",
    "            sub_word = max(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "            \n",
    "        # if our target_label == 0 i.e. negative, then we are going from positive to negative\n",
    "        # hence we want to keep only the lowest value\n",
    "        else:\n",
    "            sub_word = min(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "        \n",
    "        \n",
    "        # add selected index to selected_index list\n",
    "        selected_index.append(indx)\n",
    "\n",
    "    return ' '.join(t_split)\n",
    "    \n",
    "    \n",
    "def mutation_bert(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):\n",
    "    '''after picking word that best fits in the context of the partial sentence and select the one that takes us closest to out target label'''\n",
    "    \n",
    "    # keep track of list index of the which word we have already changed\n",
    "    selected_index = []\n",
    "    \n",
    "    #split string so we can iterate over each word\n",
    "    t_split = text_review.split()\n",
    "    \n",
    "    # select a random index value\n",
    "    indx = randrange(3, len(t_split) - 4)\n",
    "    \n",
    "    for i in range(max_perturbations):\n",
    "        # get a random index number for word list\n",
    "        # start at 4th word and upto 4th last word index\n",
    "        indx = randrange(3, len(t_split) - 4)\n",
    "        \n",
    "        found_in_vocab = False\n",
    "        # skip over all stop words and any indexes we have already selected\n",
    "        while t_split[indx] in stop_words or indx in selected_index  or not found_in_vocab:     \n",
    "            \n",
    "            indx = randrange(3, len(t_split) - 4)\n",
    "            # if word is not found in vocabulary, skip it and try next word\n",
    "            try:\n",
    "                target_word = saved_word_index[t_split[indx]]\n",
    "                found_in_vocab = True\n",
    "            except KeyError:\n",
    "                found_in_vocab = False\n",
    "        \n",
    "        # now we have a word that is not a stop word and has not already been selected\n",
    "        selected_index.append(indx) # add to our list     \n",
    "       \n",
    "        # we want to now get a list of the closest max_neighbours using BERT - these may NOT be synonyms\n",
    "        # we use the two words before and 2 after so we have context for chosen word\n",
    "        # and then use BERT to return a list of possible substitutions for our masked word\n",
    "        join_masked = [t_split[indx  - 3],t_split[indx  - 2],t_split[indx  - 1],'[MASK]',t_split[indx  + 1], t_split[indx  + 2],t_split[indx  + 3]]\n",
    "        masked_word_string = ' '.join(join_masked) \n",
    "        \n",
    "        # now we need to pass this string to BERT and get a list of possible substitutions back\n",
    "        possible_substitutions = return_masked_words(mask_word_pipeline, masked_word_string)\n",
    "        \n",
    "        # if one of the words returned is the same as the original word - we want to remove it from our list\n",
    "        # also remove any non alpha returns\n",
    "        # generate a list containing the closest words i.e. best substitutions\n",
    "        closest_word = []\n",
    "        for i in range(len(possible_substitutions)):\n",
    "            if possible_substitutions[i]['token_str'] == t_split[indx] or not possible_substitutions[i]['token_str'].isalpha():\n",
    "                continue\n",
    "            else:\n",
    "                closest_word.append(possible_substitutions[i]['token_str'])\n",
    "            \n",
    "        # substitute each word in turn and then test return from our model and add to dictionary\n",
    "        original_word = t_split[indx]\n",
    "        word_prob_dict = dict()\n",
    "        word_prob_dict[original_word] = model.predict([' '.join(t_split)])\n",
    "        for w in closest_word:\n",
    "            if not w: # if we have an empty string then do nothing, we don't want to remove a word from the string\n",
    "                continue\n",
    "            t_split[indx] = w\n",
    "            \n",
    "            # generate probabilities using each word and add to dict()\n",
    "            word_prob_dict[w] = model.predict([' '.join(t_split)])\n",
    "            \n",
    "        # didn't find any suitable words\n",
    "        if len(word_prob_dict) == 0:\n",
    "            continue\n",
    "        \n",
    "        # the word we decided to substitute is based on the probability returned by the model\n",
    "        # if we have target_label == 1 then we are trying to go from negative to positive sentiment\n",
    "        # therefore we want to keep the highest probability returned\n",
    "        # NB the probability returned by our model is the probability that the review is positive, higher values == more positive sentiment\n",
    "        if target_label == 1:\n",
    "            sub_word = max(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "            \n",
    "        # if our target_label == 0 i.e. negative, then we are going from positive to negative\n",
    "        # hence we want to keep only the lowest value\n",
    "        else:\n",
    "            sub_word = min(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "        \n",
    "        \n",
    "        # add selected index to selected_index list\n",
    "        selected_index.append(indx)\n",
    "\n",
    "    # return the string after substituting the best fit word, i.e the one that takes us towards our target label\n",
    "    return ' '.join(t_split)\n",
    "\n",
    "def mutation_bert_and_distance(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):\n",
    "    '''returns the string after swapping the synonym that best fits and produces the most gramatically correct sentence'''\n",
    "    \n",
    "    # keep track of list index of the which word we have already changed\n",
    "    selected_index = []\n",
    "    \n",
    "    #split string so we can iterate over each word\n",
    "    t_split = text_review.split()\n",
    "    \n",
    "    # select a random index value make sure to leave 2 index values before and at the end\n",
    "    indx = randrange(3, len(t_split) - 4)\n",
    "    \n",
    "    for i in range(max_perturbations):\n",
    "        # get a random index number for word list\n",
    "        indx = randrange(3, len(t_split) - 4)\n",
    "        \n",
    "        found_in_vocab = False\n",
    "        # skip over all stop words and any indexes we have already selected\n",
    "        while t_split[indx] in stop_words or indx in selected_index  or not found_in_vocab:     \n",
    "            \n",
    "            indx = randrange(3, len(t_split) - 4)\n",
    "            # if word is not found in vocabulary, skip it and try next word\n",
    "            try:\n",
    "                target_word = saved_word_index[t_split[indx]]\n",
    "                found_in_vocab = True\n",
    "            except KeyError:\n",
    "                found_in_vocab = False\n",
    "        \n",
    "        # now we have a word that is not a stop word and has not already been selected\n",
    "        selected_index.append(indx) # add to our list\n",
    "        \n",
    "        # we want to now get a list of the closest max_neighbours synonyms\n",
    "        target_word = saved_word_index[t_split[indx]]\n",
    "     \n",
    "        nearest_neighbour, _ = nn.closest_neighbours(target_word, distance_matrix, number_of_words_to_return=max_neighbours)\n",
    "        \n",
    "        # create a list of the closest words returned\n",
    "        closest_word = [saved_vocab[x] for x in nearest_neighbour]\n",
    "        \n",
    "        \n",
    "        # now we need to substitute each word and find the new probability after each substitution\n",
    "        # we need the original label and the target label we are aiming for\n",
    "       \n",
    "        original_word = t_split[indx]\n",
    "        word_sentence_score = dict()\n",
    "\n",
    "        for w in closest_word:\n",
    "            if not w: # if we have an empty string then do nothing, we don't want to remove a word from the string\n",
    "                continue\n",
    "            t_split[indx] = w\n",
    "            # construct short sentence with 2 words before and 2 after our substituted word\n",
    "            sentence_list = [t_split[indx - 3],t_split[indx - 2], t_split[indx - 1],t_split[indx], t_split[indx + 1], t_split[indx + 2],t_split[indx + 3]]\n",
    "            sentence_string = ' '.join(sentence_list)\n",
    "            # now check the sentence score for each synonym and add the score to dict()\n",
    "            word_sentence_score[w] = get_sentence_score(sentence_string)\n",
    "        \n",
    "        # didn't find any suitable words\n",
    "        if len(word_sentence_score) == 0:\n",
    "            continue\n",
    "        \n",
    "        # find the word that gives the best score for sentence structure\n",
    "        # and replace our original word with it\n",
    "        sub_word = min(word_sentence_score, key=word_sentence_score.get)\n",
    "        t_split[indx] = sub_word\n",
    "            \n",
    "        # add selected index to selected_index list so we don't pick the same word again\n",
    "        selected_index.append(indx)\n",
    "\n",
    "    return ' '.join(t_split)\n",
    "    \n",
    "    \n",
    "\n",
    "def generate_population_bert(model, text_review, population_size, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):  \n",
    "    '''return list of strings of size population_size'''       \n",
    "    return [mutation_bert(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab) for _ in range(population_size)]\n",
    "\n",
    "def generate_population_distance(model, text_review, population_size, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):  \n",
    "    '''return list of strings of size population_size'''       \n",
    "    return [mutation_distance(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab) for _ in range(population_size)]\n",
    "\n",
    "def generate_population_bert_and_distance(model, text_review, population_size, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):  \n",
    "    '''return list of strings of size population_size'''       \n",
    "    return [mutation_bert_and_distance(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab) for _ in range(population_size)]\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#################### END GA functions ###############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA attack utility functions, stats and load/save functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import normalize_dataset as nd\n",
    "import Levenshtein as lev\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#################### START STATS functions ##########################################################\n",
    "\n",
    "def calculate_levenshtein_ratios(data_sample):\n",
    "    # calculate Levenshtein ratio for text and ga_text\n",
    "    # text == initial review text, ga_text == review text after GA Attack\n",
    "    # value closer to 1.0 indicates more similarity, i.e. less changes made to original text\n",
    "    for i in range (len(data_sample)):\n",
    "        data_sample.at[i,'ga_lev_ratio'] = lev.ratio(data_sample.at[i,'text'],data_sample.at[i,'ga_text'])\n",
    "        num_words_changes = number_of_changes_made(data_sample.at[i, \"text\"], data_sample.at[i, \"ga_text\"])\n",
    "        total_words_in_review = len(data_sample.at[i, \"text\"].split())\n",
    "        data_sample.at[i,'ga_percent_change'] = num_words_changes/total_words_in_review\n",
    "    \n",
    "    return data_sample\n",
    "\n",
    "def set_labels_changed(data_sample):\n",
    "\n",
    "    flipped_count = 0\n",
    "    failed_flipped_count = 0\n",
    "    for i in range(len(data_sample)):\n",
    "        if  data_sample.at[i,'sentiment'] != data_sample.at[i,'ga_sentiment'] and (data_sample.at[i,'ga_percent_change'] <= 0.2):\n",
    "            data_sample.at[i,'ga_flipped_sentiment'] = 'Y'\n",
    "            flipped_count += 1\n",
    "        else:\n",
    "            failed_flipped_count += 1\n",
    "            data_sample.at[i,'ga_flipped_sentiment'] = 'N'\n",
    "        \n",
    "    print(\"Percentage of reviews where sentiment was changed after attack:\", round(flipped_count/len(data_sample) * 100, 2),\"%\", \"changed sentiment. i.e.\", flipped_count, \" out of \",len(data_sample))\n",
    "    print(\"Percentage of reviews failed to change sentiment: \", round(failed_flipped_count/len(data_sample) * 100, 2), \"%\",\"did not change, i.e.\", failed_flipped_count, \" out of \",len(data_sample))\n",
    "    \n",
    "    return data_sample\n",
    "\n",
    "def deleted_words_from_text(data_sample):\n",
    "    '''\n",
    "    check to make sure no words were completely removed form reviews\n",
    "    '''\n",
    "    \n",
    "    len_diff_count = 0\n",
    "    for i in range(len(data_sample)):\n",
    "        if  len(data_sample.at[i,'text'].split()) != len(data_sample.at[i,'ga_text'].split()):\n",
    "            len_diff_count += 1\n",
    "            \n",
    "    return False if len_diff_count == 0 else True\n",
    "\n",
    "def set_percentage_modified(data_sample):\n",
    "\n",
    "    word_total_count = 0\n",
    "    percent_modified_total = 0.0\n",
    "    for i in range(len(data_sample)):\n",
    "        review_before = data_sample.at[i,'text'].split()\n",
    "        review_after = data_sample.at[i,'ga_text'].split()\n",
    "        word_count = 0\n",
    "        for j in range(len(review_before)):\n",
    "            if review_before[j] != review_after[j]:\n",
    "                word_count += 1\n",
    "        data_sample.at[i, 'ga_num_changes'] = word_count\n",
    "        percent_modified_total += (word_count / len(review_before))\n",
    "        data_sample.at[i, 'ga_percent_change'] = round(word_count / len(review_before), 2)\n",
    "        word_total_count += word_count\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"Avg. num of words changed changes made: \", (int)(word_total_count / len(data_sample)))\n",
    "    print(\"Avg. percentage modified: \", (round(percent_modified_total / len(data_sample), 2) * 100),\"%\")\n",
    "    \n",
    "    return data_sample\n",
    "\n",
    "#################### END STATS functions ############################################################\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "\n",
    "def load_raw_dataset(file_name):\n",
    "    # load raw dataset i.e. before we normalised it\n",
    "    dtypes = {'sentiment': 'int', \n",
    "              'text': 'str' \n",
    "             }\n",
    "\n",
    "    raw_data_sample = pd.read_csv('imdb_dataset/'+file_name, dtype=dtypes)\n",
    "    rds = raw_data_sample.copy()\n",
    "\n",
    "    # normalise the dataset\n",
    "    normalized_dataset = nd.clean_and_return(rds, 'text')\n",
    "    \n",
    "    return raw_data_sample, normalized_dataset\n",
    "\n",
    "def matching_indexes(data_sample,normalized_dataset):\n",
    "    '''match up index values from data_sample to raw data file'''\n",
    "    index_values_rawdata = []\n",
    "    for i in range(len(data_sample)):\n",
    "        for j in range(len(normalized_dataset)): \n",
    "            if normalized_dataset.at[j,'text'] == data_sample.at[i, 'text']:\n",
    "               # print(raw_data_sample.at[j,'text'],\"\\n\",data_sample.at[i, 'text'],\"\\n\\n\")\n",
    "                index_values_rawdata.append(j)\n",
    "                break\n",
    "    return index_values_rawdata\n",
    "\n",
    "\n",
    "def save_ga_attack_results(data_sample, file_name):\n",
    "    # save the final set of results\n",
    "    data_sample.to_csv('ga_attack_results/'+file_name, index = False)\n",
    "\n",
    "    # load saved file, so we can remove any results that were not processed\n",
    "    dtypes = {'sentiment': 'int', \n",
    "              'text': 'str', \n",
    "              'probs': 'float', \n",
    "              'ga_sentiment': 'int', \n",
    "              'ga_text': 'str',\n",
    "              'ga_probs': 'float', \n",
    "              'ga_num_changes': 'int', \n",
    "              'ga_lev_ratio': 'float', \n",
    "              'ga_flipped_sentiment': 'str',\n",
    "             'ga_percent_change': 'float'}\n",
    "\n",
    "    # make sure we can read and load the file\n",
    "    #data_sample = pd.read_csv('ga_attack_results/'+file_name)\n",
    "    data_sample = pd.read_csv('ga_attack_results/'+file_name, dtype=dtypes)\n",
    "\n",
    "    # drop any rows with nan value - i.e. data items not processed due to Jupyter notebook crash\n",
    "    # so we don't have to re-run the whole GA Attack again\n",
    "    data_sample = data_sample.dropna()\n",
    "\n",
    "    # save the final set of results\n",
    "    data_sample.to_csv('ga_attack_results/'+file_name, index = False)\n",
    "    print(\"Results saved to:\\t\",'ga_attack_results/'+file_name)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_ga_attack_results(file_name):\n",
    "     # load saved file, so we can remove any results that were not processed\n",
    "    dtypes = {'sentiment': 'int', \n",
    "              'text': 'str', \n",
    "              'probs': 'float', \n",
    "              'ga_sentiment': 'int', \n",
    "              'ga_text': 'str',\n",
    "              'ga_probs': 'float', \n",
    "              'ga_num_changes': 'int', \n",
    "              'ga_lev_ratio': 'float', \n",
    "              'ga_flipped_sentiment': 'str',\n",
    "             'ga_percent_change': 'float'}\n",
    "\n",
    "    #return pd.read_csv('ga_attack_results/'+file_name)\n",
    "    return pd.read_csv('ga_attack_results/'+file_name, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA Attack functions for BERT and DISTANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "POPULATION_SIZE = 20 # max population size to create \n",
    "MAXIMUM_ITERATIONS = 20 # stopping condition for loop if we do not find an optimal solution increased to 300 for BERT attack\n",
    "MAX_PERTURBATIONS = 5 # maximum number of changes to make for each population member increase from 5 to 6 for BERT\n",
    "MAX_NEIGHBOURS = 4 # maximum number of neighbouring words to return and check against\n",
    "\n",
    "\n",
    "\n",
    "def GA_Attack(data_sample, type_of_attack=2):\n",
    "    data_sample_len = len(data_sample)\n",
    "\n",
    "    for i in range(data_sample_len):\n",
    "    \n",
    "        print(\"####### Data Item: \",i+1,\" #######\")\n",
    "        target_label = 0 if data_sample.iloc[i]['sentiment'] == 1 else 1\n",
    "        current_prediction = data_sample.iloc[i]['probs']\n",
    "        \n",
    "        # calculate how long it takes us to process each data item (in seconds)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # generate a new population of possible solutions\n",
    "        if type_of_attack == 1:\n",
    "            p = generate_population_bert(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 2:\n",
    "            p = generate_population_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 3:\n",
    "            p = generate_population_bert_and_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "    \n",
    "        population_dataframe = fitness(p, saved_model)\n",
    "\n",
    "    \n",
    "        ## GA Attack START\n",
    "        # need to run through and do crossover and mutation, recheck the label and if it has flipped then we stop, other wise keep going\n",
    "        # also on each iteration keep updating the ga_text and ga_prob and if label has changed update the ga_sentiment column and stop\n",
    "        for j in range(MAXIMUM_ITERATIONS):\n",
    "        \n",
    "        \n",
    "            # first check if we have found a solution, if yes then we are done so save results and break\n",
    "            # and move onto next\n",
    "            if found_solution(target_label, population_dataframe):\n",
    "                #save solution\n",
    "                if target_label == 1:   \n",
    "                    data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "                    data_sample.at[i, \"ga_sentiment\"] = 1\n",
    "                    break\n",
    "                if target_label == 0:\n",
    "                    data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[0]['ga_text']\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "                    data_sample.at[i, \"ga_sentiment\"] = 0\n",
    "                    break\n",
    "        \n",
    "            # select the two best parents from the population\n",
    "            if target_label == 1:\n",
    "                parent_one = population_dataframe.iloc[-1]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[-2]['ga_text']\n",
    "            else: \n",
    "                parent_one = population_dataframe.iloc[0]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[1]['ga_text']\n",
    "        \n",
    "            # save progress so far\n",
    "            if target_label == 1 and (current_prediction < population_dataframe.iloc[-1]['ga_probs']):\n",
    "                data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "            \n",
    "            if target_label == 0 and (current_prediction > population_dataframe.iloc[0]['ga_probs']):\n",
    "                data_sample.at[i, \"ga_text\"]  = population_dataframe.iloc[0]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "        \n",
    "            # we didn't find a solution yet, so we do crossover and then generate a new population of possible solutions \n",
    "            x_over = crossover(parent_one, parent_two)\n",
    "            \n",
    "            # generate a new population of possible solutions\n",
    "            if type_of_attack == 1:\n",
    "                p = generate_population_bert(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 2:\n",
    "                p = generate_population_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 3:\n",
    "                p = generate_population_bert_and_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "                \n",
    "            population_dataframe = fitness(p, saved_model)\n",
    "        \n",
    "        num_words_changes = number_of_changes_made(data_sample.at[i, \"text\"], data_sample.at[i, \"ga_text\"])\n",
    "        total_words_in_review = len(data_sample.at[i, \"text\"].split())\n",
    "        \n",
    "        print(\"\\tElapsed time: \", round(time.time() - t0), \" seconds\")\n",
    "        print(\"\\tNumber of words in review: \",total_words_in_review )\n",
    "        print(\"\\tNumber of words swapped: \", num_words_changes)\n",
    "        print(\"\\tPercentage modified: \", round((num_words_changes / total_words_in_review ),2) * 100,\"%\")\n",
    "        print(\"\\tProb. before and after: \", data_sample.at[i, \"probs\"],\" : \", data_sample.at[i, \"ga_probs\"],\"\\n\")\n",
    "        \n",
    "    return data_sample\n",
    "\n",
    "\n",
    "def GA_Attack_Store_Fitness(data_sample, type_of_attack=2):\n",
    "    data_sample_len = len(data_sample)\n",
    "\n",
    "    #for i in range(data_sample_len):\n",
    "    for i in range(5):\n",
    "    \n",
    "        print(\"####### Data Item: \",i+1,\" #######\")\n",
    "        target_label = 0 if data_sample.iloc[i]['sentiment'] == 1 else 1\n",
    "        current_prediction = data_sample.iloc[i]['probs']\n",
    "        \n",
    "        # calculate how long it takes us to process each data item (in seconds)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # generate a new population of possible solutions\n",
    "        if type_of_attack == 1:\n",
    "            p = generate_population_bert(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 2:\n",
    "            p = generate_population_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 3:\n",
    "            p = generate_population_bert_and_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "    \n",
    "        population_dataframe = fitness(p, saved_model)\n",
    "\n",
    "    \n",
    "        ## GA Attack START\n",
    "        # need to run through and do crossover and mutation, recheck the label and if it has flipped then we stop, other wise keep going\n",
    "        # also on each iteration keep updating the ga_text and ga_prob and if label has changed update the ga_sentiment column and stop\n",
    "        for j in range(MAXIMUM_ITERATIONS):\n",
    "        \n",
    "        \n",
    "            # first check if we have found a solution, if yes then we are done so save results and break\n",
    "            # and move onto next\n",
    "            if found_solution(target_label, population_dataframe):\n",
    "                #save solution\n",
    "                if target_label == 1:   \n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "                    print(\"Target and Current Fitness: \",target_label,\" : \",current_prediction, \" Generation: \",j, \" Fitness: \",data_sample.at[i, \"ga_probs\"])\n",
    "                    break\n",
    "                if target_label == 0:\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "                    print(\"Target and Current Fitness: \",target_label,\" : \",current_prediction, \" Generation: \",j, \" Fitness: \",data_sample.at[i, \"ga_probs\"])\n",
    "                    break\n",
    "            else:\n",
    "                # save progress so far\n",
    "                if target_label == 1:\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "            \n",
    "                if target_label == 0:\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "        \n",
    "                print(\"Target and Current Fitness: \",target_label,\" : \",current_prediction, \" Generation: \",j, \" Fitness: \",data_sample.at[i, \"ga_probs\"])\n",
    "        \n",
    "        \n",
    "            # select the two best parents from the population\n",
    "            if target_label == 1:\n",
    "                parent_one = population_dataframe.iloc[-1]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[-2]['ga_text']\n",
    "            else: \n",
    "                parent_one = population_dataframe.iloc[0]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[1]['ga_text']\n",
    "                \n",
    "            \n",
    "           \n",
    "        \n",
    "            # we didn't find a solution yet, so we do crossover and then generate a new population of possible solutions \n",
    "            x_over = crossover(parent_one, parent_two)\n",
    "            \n",
    "            # generate a new population of possible solutions\n",
    "            if type_of_attack == 1:\n",
    "                p = generate_population_bert(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 2:\n",
    "                p = generate_population_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 3:\n",
    "                p = generate_population_bert_and_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "                \n",
    "            population_dataframe = fitness(p, saved_model)\n",
    "        \n",
    "def GA_Attack_Stats(data_sample, type_of_attack=2):\n",
    "    data_sample_len = len(data_sample)\n",
    "\n",
    "    t0 = 0\n",
    "    generation = 0\n",
    "    for i in range(data_sample_len):\n",
    "    \n",
    "        print(\"####### Data Item: \",i+1,\" #######\")\n",
    "        target_label = 0 if data_sample.iloc[i]['sentiment'] == 1 else 1\n",
    "        current_prediction = data_sample.iloc[i]['probs']\n",
    "        \n",
    "        t0 = 0\n",
    "        generation = 0\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # generate a new population of possible solutions\n",
    "        if type_of_attack == 1:\n",
    "            p = generate_population_bert(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 2:\n",
    "            p = generate_population_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        if type_of_attack == 3:\n",
    "            p = generate_population_bert_and_distance(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "    \n",
    "        population_dataframe = fitness(p, saved_model)\n",
    "        \n",
    "        # calculate how long it takes us to process each data item (in seconds)\n",
    "        \n",
    "    \n",
    "        ## GA Attack START\n",
    "        # need to run through and do crossover and mutation, recheck the label and if it has flipped then we stop, other wise keep going\n",
    "        # also on each iteration keep updating the ga_text and ga_prob and if label has changed update the ga_sentiment column and stop\n",
    "        for j in range(MAXIMUM_ITERATIONS):\n",
    "            \n",
    "        \n",
    "            # first check if we have found a solution, if yes then we are done so save results and break\n",
    "            # and move onto next\n",
    "            if found_solution(target_label, population_dataframe):\n",
    "                generation = j + 1\n",
    "                #save solution\n",
    "                if target_label == 1:\n",
    "                    data_sample.at[i,'ga_generations'] = generation\n",
    "                    data_sample.at[i,'ga_time_taken'] = round(time.time() - t0)\n",
    "                    data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "                    data_sample.at[i, \"ga_sentiment\"] = 1\n",
    "                    break\n",
    "                if target_label == 0:\n",
    "                    data_sample.at[i,'ga_generations'] = generation\n",
    "                    data_sample.at[i,'ga_time_taken'] = round(time.time() - t0)\n",
    "                    data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[0]['ga_text']\n",
    "                    data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "                    data_sample.at[i, \"ga_sentiment\"] = 0\n",
    "                    break\n",
    "        \n",
    "            # select the two best parents from the population\n",
    "            if target_label == 1:\n",
    "                parent_one = population_dataframe.iloc[-1]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[-2]['ga_text']\n",
    "            else: \n",
    "                parent_one = population_dataframe.iloc[0]['ga_text']\n",
    "                parent_two = population_dataframe.iloc[1]['ga_text']\n",
    "        \n",
    "            # save progress so far\n",
    "            if target_label == 1 and (current_prediction < population_dataframe.iloc[-1]['ga_probs']):\n",
    "                data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "            \n",
    "            if target_label == 0 and (current_prediction > population_dataframe.iloc[0]['ga_probs']):\n",
    "                data_sample.at[i, \"ga_text\"]  = population_dataframe.iloc[0]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "        \n",
    "            # we didn't find a solution yet, so we do crossover and then generate a new population of possible solutions \n",
    "            x_over = crossover(parent_one, parent_two)\n",
    "            \n",
    "            # generate a new population of possible solutions\n",
    "            if type_of_attack == 1:\n",
    "                p = generate_population_bert(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 2:\n",
    "                p = generate_population_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "            if type_of_attack == 3:\n",
    "                p = generate_population_bert_and_distance(saved_model, x_over, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "                \n",
    "            population_dataframe = fitness(p, saved_model)\n",
    "        \n",
    "        num_words_changes = number_of_changes_made(data_sample.at[i, \"text\"], data_sample.at[i, \"ga_text\"])\n",
    "        total_words_in_review = len(data_sample.at[i, \"text\"].split())\n",
    "        \n",
    "        print(\"\\tElapsed time: \", round(time.time() - t0), \" seconds\")\n",
    "        print(\"\\tGeneration: \", generation)\n",
    "        print(\"\\tNumber of words in review: \",total_words_in_review )\n",
    "        print(\"\\tNumber of words swapped: \", num_words_changes)\n",
    "        print(\"\\tPercentage modified: \", round((num_words_changes / total_words_in_review ),2) * 100,\"%\")\n",
    "        print(\"\\tProb. before and after: \", data_sample.at[i, \"probs\"],\" : \", data_sample.at[i, \"ga_probs\"],\"\\n\")\n",
    "        \n",
    "    return data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04) Prepare GA attack dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items in sample:  1006\n",
      "Calculating probabilities for sample dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_sample = load_sample_dataset()\n",
    "data_sample = add_ga_columns(data_sample)\n",
    "data_sample = generate_current_probabilities(data_sample)\n",
    "data_sample = check_predictions(data_sample)\n",
    "\n",
    "data_sample.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05) start the GA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Data Item:  984  #######\n",
      "\tElapsed time:  458  seconds\n",
      "\tNumber of words in review:  303\n",
      "\tNumber of words swapped:  37\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.9751801490783691  :  0.49372774362564087 \n",
      "\n",
      "####### Data Item:  985  #######\n",
      "\tElapsed time:  274  seconds\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  22\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.935371994972229  :  0.4273940622806549 \n",
      "\n",
      "####### Data Item:  986  #######\n",
      "\tElapsed time:  383  seconds\n",
      "\tNumber of words in review:  158\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.9640346169471741  :  0.3361856937408447 \n",
      "\n",
      "####### Data Item:  987  #######\n",
      "\tElapsed time:  652  seconds\n",
      "\tNumber of words in review:  295\n",
      "\tNumber of words swapped:  56\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.9573256373405457  :  0.4712672531604767 \n",
      "\n",
      "####### Data Item:  988  #######\n",
      "\tElapsed time:  1014  seconds\n",
      "\tNumber of words in review:  205\n",
      "\tNumber of words swapped:  58\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.9982122182846069  :  0.545425534248352 \n",
      "\n",
      "####### Data Item:  989  #######\n",
      "\tElapsed time:  656  seconds\n",
      "\tNumber of words in review:  172\n",
      "\tNumber of words swapped:  49\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.9985150694847107  :  0.38442757725715637 \n",
      "\n",
      "####### Data Item:  990  #######\n",
      "\tElapsed time:  324  seconds\n",
      "\tNumber of words in review:  293\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.09500005096197128  :  0.5573738217353821 \n",
      "\n",
      "####### Data Item:  991  #######\n",
      "\tElapsed time:  484  seconds\n",
      "\tNumber of words in review:  144\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.9461686015129089  :  0.4644583761692047 \n",
      "\n",
      "####### Data Item:  992  #######\n",
      "\tElapsed time:  47  seconds\n",
      "\tNumber of words in review:  215\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.5885463953018188  :  0.2941916584968567 \n",
      "\n",
      "####### Data Item:  993  #######\n",
      "\tElapsed time:  276  seconds\n",
      "\tNumber of words in review:  122\n",
      "\tNumber of words swapped:  25\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.1121017262339592  :  0.5771145224571228 \n",
      "\n",
      "####### Data Item:  994  #######\n",
      "\tElapsed time:  180  seconds\n",
      "\tNumber of words in review:  282\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.924653172492981  :  0.45259928703308105 \n",
      "\n",
      "####### Data Item:  995  #######\n",
      "\tElapsed time:  946  seconds\n",
      "\tNumber of words in review:  174\n",
      "\tNumber of words swapped:  60\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9933526515960693  :  0.8955666422843933 \n",
      "\n",
      "####### Data Item:  996  #######\n",
      "\tElapsed time:  1019  seconds\n",
      "\tNumber of words in review:  142\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.9912501573562622  :  0.7111316323280334 \n",
      "\n",
      "####### Data Item:  997  #######\n",
      "\tElapsed time:  139  seconds\n",
      "\tNumber of words in review:  58\n",
      "\tNumber of words swapped:  12\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.8660787343978882  :  0.2840360105037689 \n",
      "\n",
      "####### Data Item:  998  #######\n",
      "\tElapsed time:  964  seconds\n",
      "\tNumber of words in review:  203\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  28.999999999999996 %\n",
      "\tProb. before and after:  0.0019006830407306552  :  0.0046312082558870316 \n",
      "\n",
      "####### Data Item:  999  #######\n",
      "\tElapsed time:  196  seconds\n",
      "\tNumber of words in review:  332\n",
      "\tNumber of words swapped:  17\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.7615659832954407  :  0.4514646530151367 \n",
      "\n",
      "####### Data Item:  1000  #######\n",
      "\tElapsed time:  714  seconds\n",
      "\tNumber of words in review:  950\n",
      "\tNumber of words swapped:  76\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.9572864174842834  :  0.4801267683506012 \n",
      "\n",
      "####### Data Item:  1001  #######\n",
      "\tElapsed time:  1005  seconds\n",
      "\tNumber of words in review:  295\n",
      "\tNumber of words swapped:  76\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.982030987739563  :  0.5576531291007996 \n",
      "\n",
      "####### Data Item:  1002  #######\n",
      "\tElapsed time:  985  seconds\n",
      "\tNumber of words in review:  136\n",
      "\tNumber of words swapped:  42\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.9856457114219666  :  0.688693642616272 \n",
      "\n",
      "####### Data Item:  1003  #######\n",
      "\tElapsed time:  49  seconds\n",
      "\tNumber of words in review:  191\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.5956876277923584  :  0.40100985765457153 \n",
      "\n",
      "####### Data Item:  1004  #######\n",
      "\tElapsed time:  384  seconds\n",
      "\tNumber of words in review:  126\n",
      "\tNumber of words swapped:  33\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.008624284528195858  :  0.5487279891967773 \n",
      "\n",
      "####### Data Item:  1005  #######\n",
      "\tElapsed time:  493  seconds\n",
      "\tNumber of words in review:  212\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.9895702600479126  :  0.35525891184806824 \n",
      "\n",
      "####### Data Item:  1006  #######\n",
      "\tElapsed time:  51  seconds\n",
      "\tNumber of words in review:  136\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.46394601464271545  :  0.6776870489120483 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' TYPE_OF_ATTACK:\n",
    "    1 == BERT i.e. using context to generate replacement words\n",
    "    2 == Distance Matrix i.e. using only nearest neighbour - synonyms to generate replacement words\n",
    "    3 == Distance Matric and BERT sentence score i.e. find synonyms and then use sentence structure to score partial sentence for each replacement word\n",
    "'''\n",
    "\n",
    "ga_results_BERT = \"ga_results_BERT.csv\"\n",
    "ga_results_DISTANCE = \"ga_results_DISTANCE.csv\"\n",
    "ga_results_DISTANCE_BERT = \"ga_results_DISTANCE_BERT.csv\"\n",
    "\n",
    "\n",
    "TYPE_OF_ATTACK = 3\n",
    "\n",
    "data_sample = GA_Attack(data_sample, type_of_attack=TYPE_OF_ATTACK)\n",
    "\n",
    "\n",
    "        \n",
    "if TYPE_OF_ATTACK == 1:        \n",
    "    save_ga_attack_results(data_sample, ga_results_BERT)\n",
    "if TYPE_OF_ATTACK == 2:\n",
    "    save_ga_attack_results(data_sample, ga_results_DISTANCE)\n",
    "if TYPE_OF_ATTACK == 3:\n",
    "    save_ga_attack_results(data_sample, ga_results_DISTANCE_BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a smaller sample so we can calculate additional statistics statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items in sample:  1006\n",
      "Reduce sample size to:  250\n",
      "Calculating probabilities for sample dataset...\n",
      "Checking proabilities for sample dataset are correctly labelled...\n",
      "Number of data items dropped from sample:  0\n",
      "Number of data items kept in sample:  250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             0  well i must say this is probably the worst fil...  0.025771   \n",
       "1             1  if christopher nolan had made memento before f...  0.769358   \n",
       "2             1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3             0  i have been watching lost with my family since...  0.476023   \n",
       "4             1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0               0           0.0                    N                0.0   \n",
       "1               0           0.0                    N                0.0   \n",
       "2               0           0.0                    N                0.0   \n",
       "3               0           0.0                    N                0.0   \n",
       "4               0           0.0                    N                0.0   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               0              0  \n",
       "1               0              0  \n",
       "2               0              0  \n",
       "3               0              0  \n",
       "4               0              0  "
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_sample = load_sample_dataset()\n",
    "\n",
    "# take a sample of 250 items\n",
    "data_sample = data_sample.sample(n = 250) \n",
    "print(\"Reduce sample size to: \",len(data_sample))\n",
    "\n",
    "data_sample = add_ga_columns_stats(data_sample)\n",
    "data_sample = generate_current_probabilities(data_sample)\n",
    "data_sample = check_predictions(data_sample)\n",
    "\n",
    "data_sample.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save this sample dataset - so we can use the smne one for all 3 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             0  well i must say this is probably the worst fil...  0.025771   \n",
       "1             1  if christopher nolan had made memento before f...  0.769358   \n",
       "2             1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3             0  i have been watching lost with my family since...  0.476023   \n",
       "4             1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0               0           0.0                    N                0.0   \n",
       "1               0           0.0                    N                0.0   \n",
       "2               0           0.0                    N                0.0   \n",
       "3               0           0.0                    N                0.0   \n",
       "4               0           0.0                    N                0.0   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               0              0  \n",
       "1               0              0  \n",
       "2               0              0  \n",
       "3               0              0  \n",
       "4               0              0  "
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save_ga_attack_results(data_sample, 'sample_stats_set_250.csv')\n",
    "\n",
    "\n",
    "# load it back just to check it saved correctly\n",
    "data_sample = load_ga_attack_results('sample_stats_set_250.csv')\n",
    "\n",
    "#data_sample = pd.read_csv('ga_attack_results/sample_stats_set_250.csv')\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the GA_Attack on smaller sample set to collect additional stats statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Data Item:  1  #######\n",
      "\tElapsed time:  264  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  155\n",
      "\tNumber of words swapped:  20\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.02577141486108303  :  0.7685383558273315 \n",
      "\n",
      "####### Data Item:  2  #######\n",
      "\tElapsed time:  107  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  304\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.7693580985069275  :  0.39587557315826416 \n",
      "\n",
      "####### Data Item:  3  #######\n",
      "\tElapsed time:  106  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  187\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.8079519271850586  :  0.42656242847442627 \n",
      "\n",
      "####### Data Item:  4  #######\n",
      "\tElapsed time:  53  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  370\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  1.0 %\n",
      "\tProb. before and after:  0.4760232865810394  :  0.6372390389442444 \n",
      "\n",
      "####### Data Item:  5  #######\n",
      "\tElapsed time:  256  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  153\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.9428611397743224  :  0.29481270909309387 \n",
      "\n",
      "####### Data Item:  6  #######\n",
      "\tElapsed time:  1073  seconds\n",
      "\tGeneration:  20\n",
      "\tNumber of words in review:  220\n",
      "\tNumber of words swapped:  65\n",
      "\tPercentage modified:  30.0 %\n",
      "\tProb. before and after:  0.007910426706075668  :  0.5827938318252563 \n",
      "\n",
      "####### Data Item:  7  #######\n",
      "\tElapsed time:  532  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  407\n",
      "\tNumber of words swapped:  43\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.9810193181037904  :  0.4704085886478424 \n",
      "\n",
      "####### Data Item:  8  #######\n",
      "\tElapsed time:  462  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  489\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.05036283656954765  :  0.5052801370620728 \n",
      "\n",
      "####### Data Item:  9  #######\n",
      "\tElapsed time:  641  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  140\n",
      "\tNumber of words swapped:  41\n",
      "\tPercentage modified:  28.999999999999996 %\n",
      "\tProb. before and after:  0.002802524250000716  :  0.5848064422607422 \n",
      "\n",
      "####### Data Item:  10  #######\n",
      "\tElapsed time:  97  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  40\n",
      "\tNumber of words swapped:  11\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.8586663007736206  :  0.1774769425392151 \n",
      "\n",
      "####### Data Item:  11  #######\n",
      "\tElapsed time:  557  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  125\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.005909522529691458  :  0.5328331589698792 \n",
      "\n",
      "####### Data Item:  12  #######\n",
      "\tElapsed time:  54  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  232\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.5314435958862305  :  0.39487355947494507 \n",
      "\n",
      "####### Data Item:  13  #######\n",
      "\tElapsed time:  215  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  122\n",
      "\tNumber of words swapped:  19\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.1121017262339592  :  0.5679659843444824 \n",
      "\n",
      "####### Data Item:  14  #######\n",
      "\tElapsed time:  113  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  302\n",
      "\tNumber of words swapped:  7\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.3804309666156769  :  0.5065115690231323 \n",
      "\n",
      "####### Data Item:  15  #######\n",
      "\tElapsed time:  1120  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  162\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.0060756588354706756  :  0.1116267591714859 \n",
      "\n",
      "####### Data Item:  16  #######\n",
      "\tElapsed time:  205  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  182\n",
      "\tNumber of words swapped:  16\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.9208897948265076  :  0.44332021474838257 \n",
      "\n",
      "####### Data Item:  17  #######\n",
      "\tElapsed time:  242  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  265\n",
      "\tNumber of words swapped:  23\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.07208222150802612  :  0.5157703757286072 \n",
      "\n",
      "####### Data Item:  18  #######\n",
      "\tElapsed time:  599  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  344\n",
      "\tNumber of words swapped:  51\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.008051271550357342  :  0.5015153884887695 \n",
      "\n",
      "####### Data Item:  19  #######\n",
      "\tElapsed time:  54  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.3944404721260071  :  0.6458479166030884 \n",
      "\n",
      "####### Data Item:  20  #######\n",
      "\tElapsed time:  565  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  134\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.015006957575678825  :  0.5090388059616089 \n",
      "\n",
      "####### Data Item:  21  #######\n",
      "\tElapsed time:  376  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  153\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.9665619134902954  :  0.40161293745040894 \n",
      "\n",
      "####### Data Item:  22  #######\n",
      "\tElapsed time:  1051  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  356\n",
      "\tNumber of words swapped:  71\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.9993700385093688  :  0.6689895987510681 \n",
      "\n",
      "####### Data Item:  23  #######\n",
      "\tElapsed time:  156  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  126\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.9260008931159972  :  0.27842801809310913 \n",
      "\n",
      "####### Data Item:  24  #######\n",
      "\tElapsed time:  376  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  139\n",
      "\tNumber of words swapped:  28\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.9703093767166138  :  0.49215444922447205 \n",
      "\n",
      "####### Data Item:  25  #######\n",
      "\tElapsed time:  100  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  101\n",
      "\tNumber of words swapped:  9\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.8794736862182617  :  0.4982542097568512 \n",
      "\n",
      "####### Data Item:  26  #######\n",
      "\tElapsed time:  1121  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  709\n",
      "\tNumber of words swapped:  90\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.013748758472502232  :  0.3079250752925873 \n",
      "\n",
      "####### Data Item:  27  #######\n",
      "\tElapsed time:  161  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  114\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.1052192896604538  :  0.594158411026001 \n",
      "\n",
      "####### Data Item:  28  #######\n",
      "\tElapsed time:  784  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  690\n",
      "\tNumber of words swapped:  58\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.035415396094322205  :  0.5144451260566711 \n",
      "\n",
      "####### Data Item:  29  #######\n",
      "\tElapsed time:  212  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  239\n",
      "\tNumber of words swapped:  23\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.9109609723091124  :  0.33513128757476807 \n",
      "\n",
      "####### Data Item:  30  #######\n",
      "\tElapsed time:  50  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  134\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.5750336647033691  :  0.3471645712852478 \n",
      "\n",
      "####### Data Item:  31  #######\n",
      "\tElapsed time:  106  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  162\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  6.0 %\n",
      "\tProb. before and after:  0.8479402661323547  :  0.19328567385673523 \n",
      "\n",
      "####### Data Item:  32  #######\n",
      "\tElapsed time:  912  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  138\n",
      "\tNumber of words swapped:  48\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.9763654470443726  :  0.4423711597919464 \n",
      "\n",
      "####### Data Item:  33  #######\n",
      "\tElapsed time:  558  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  192\n",
      "\tNumber of words swapped:  42\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.9869678616523744  :  0.3902912139892578 \n",
      "\n",
      "####### Data Item:  34  #######\n",
      "\tElapsed time:  1154  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  128\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.003947785589843988  :  0.4015655219554901 \n",
      "\n",
      "####### Data Item:  35  #######\n",
      "\tElapsed time:  1172  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  166\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  36.0 %\n",
      "\tProb. before and after:  0.9932684302330016  :  0.6124799847602844 \n",
      "\n",
      "####### Data Item:  36  #######\n",
      "\tElapsed time:  266  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  79\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.9650508761405944  :  0.34852540493011475 \n",
      "\n",
      "####### Data Item:  37  #######\n",
      "\tElapsed time:  153  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  156\n",
      "\tNumber of words swapped:  11\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.0873078778386116  :  0.5134403705596924 \n",
      "\n",
      "####### Data Item:  38  #######\n",
      "\tElapsed time:  558  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  159\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.9852797985076904  :  0.4496804475784302 \n",
      "\n",
      "####### Data Item:  39  #######\n",
      "\tElapsed time:  919  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  135\n",
      "\tNumber of words swapped:  47\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.0008738029864616692  :  0.7213233709335327 \n",
      "\n",
      "####### Data Item:  40  #######\n",
      "\tElapsed time:  1112  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  145\n",
      "\tNumber of words swapped:  49\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.0018999151652678847  :  0.14787603914737701 \n",
      "\n",
      "####### Data Item:  41  #######\n",
      "\tElapsed time:  388  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  326\n",
      "\tNumber of words swapped:  33\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.1824033558368683  :  0.5948172807693481 \n",
      "\n",
      "####### Data Item:  42  #######\n",
      "\tElapsed time:  1077  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  902\n",
      "\tNumber of words swapped:  66\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.97307026386261  :  0.5557483434677124 \n",
      "\n",
      "####### Data Item:  43  #######\n",
      "\tElapsed time:  108  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  203\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.8386113047599792  :  0.3742770850658417 \n",
      "\n",
      "####### Data Item:  44  #######\n",
      "\tElapsed time:  927  seconds\n",
      "\tGeneration:  18\n",
      "\tNumber of words in review:  442\n",
      "\tNumber of words swapped:  72\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.0044936095364391795  :  0.5273241996765137 \n",
      "\n",
      "####### Data Item:  45  #######\n",
      "\tElapsed time:  379  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  139\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.004485400393605232  :  0.7024194002151489 \n",
      "\n",
      "####### Data Item:  46  #######\n",
      "\tElapsed time:  843  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  917\n",
      "\tNumber of words swapped:  78\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.01808794587850571  :  0.5236793160438538 \n",
      "\n",
      "####### Data Item:  47  #######\n",
      "\tElapsed time:  255  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  166\n",
      "\tNumber of words swapped:  16\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.9735668301582336  :  0.3685064911842346 \n",
      "\n",
      "####### Data Item:  48  #######\n",
      "\tElapsed time:  485  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  138\n",
      "\tNumber of words swapped:  28\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.974309504032135  :  0.45769667625427246 \n",
      "\n",
      "####### Data Item:  49  #######\n",
      "\tElapsed time:  105  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  143\n",
      "\tNumber of words swapped:  7\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.2760487198829651  :  0.6270735859870911 \n",
      "\n",
      "####### Data Item:  50  #######\n",
      "\tElapsed time:  414  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  309\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.07660411298274994  :  0.5558866858482361 \n",
      "\n",
      "####### Data Item:  51  #######\n",
      "\tElapsed time:  1109  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  203\n",
      "\tNumber of words swapped:  68\n",
      "\tPercentage modified:  33.0 %\n",
      "\tProb. before and after:  0.0012972356053069234  :  0.14942075312137604 \n",
      "\n",
      "####### Data Item:  52  #######\n",
      "\tElapsed time:  374  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  332\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.038619305938482285  :  0.59697425365448 \n",
      "\n",
      "####### Data Item:  53  #######\n",
      "\tElapsed time:  791  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  216\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.9748616218566896  :  0.3111714720726013 \n",
      "\n",
      "####### Data Item:  54  #######\n",
      "\tElapsed time:  137  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  432\n",
      "\tNumber of words swapped:  13\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.7718695402145386  :  0.490988552570343 \n",
      "\n",
      "####### Data Item:  55  #######\n",
      "\tElapsed time:  643  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  128\n",
      "\tNumber of words swapped:  38\n",
      "\tPercentage modified:  30.0 %\n",
      "\tProb. before and after:  0.015183914452791212  :  0.5251264572143555 \n",
      "\n",
      "####### Data Item:  56  #######\n",
      "\tElapsed time:  639  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  142\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.984166145324707  :  0.4626884460449219 \n",
      "\n",
      "####### Data Item:  57  #######\n",
      "\tElapsed time:  1040  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  76\n",
      "\tNumber of words swapped:  26\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9896700978279114  :  0.623626172542572 \n",
      "\n",
      "####### Data Item:  58  #######\n",
      "\tElapsed time:  966  seconds\n",
      "\tGeneration:  20\n",
      "\tNumber of words in review:  140\n",
      "\tNumber of words swapped:  53\n",
      "\tPercentage modified:  38.0 %\n",
      "\tProb. before and after:  0.9982861876487732  :  0.40237361192703247 \n",
      "\n",
      "####### Data Item:  59  #######\n",
      "\tElapsed time:  592  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  238\n",
      "\tNumber of words swapped:  43\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.98002290725708  :  0.43934357166290283 \n",
      "\n",
      "####### Data Item:  60  #######\n",
      "\tElapsed time:  507  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  390\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.05301350727677345  :  0.5484699010848999 \n",
      "\n",
      "####### Data Item:  61  #######\n",
      "\tElapsed time:  208  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  22\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.9750559329986572  :  0.2383320927619934 \n",
      "\n",
      "####### Data Item:  62  #######\n",
      "\tElapsed time:  252  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  218\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.9144740700721741  :  0.46938449144363403 \n",
      "\n",
      "####### Data Item:  63  #######\n",
      "\tElapsed time:  805  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  30.0 %\n",
      "\tProb. before and after:  0.006765776313841343  :  0.5281674861907959 \n",
      "\n",
      "####### Data Item:  64  #######\n",
      "\tElapsed time:  215  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  141\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.9481713175773621  :  0.35895034670829773 \n",
      "\n",
      "####### Data Item:  65  #######\n",
      "\tElapsed time:  411  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  187\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.03969476744532585  :  0.5203427076339722 \n",
      "\n",
      "####### Data Item:  66  #######\n",
      "\tElapsed time:  1117  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  138\n",
      "\tNumber of words swapped:  41\n",
      "\tPercentage modified:  30.0 %\n",
      "\tProb. before and after:  0.9927661418914796  :  0.8242865800857544 \n",
      "\n",
      "####### Data Item:  67  #######\n",
      "\tElapsed time:  101  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  171\n",
      "\tNumber of words swapped:  11\n",
      "\tPercentage modified:  6.0 %\n",
      "\tProb. before and after:  0.11045315861701964  :  0.6906034350395203 \n",
      "\n",
      "####### Data Item:  68  #######\n",
      "\tElapsed time:  722  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  127\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.0017009006114676595  :  0.6528080105781555 \n",
      "\n",
      "####### Data Item:  69  #######\n",
      "\tElapsed time:  626  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  388\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.9473495483398438  :  0.3844689726829529 \n",
      "\n",
      "####### Data Item:  70  #######\n",
      "\tElapsed time:  191  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  205\n",
      "\tNumber of words swapped:  19\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.05558184534311295  :  0.5247451066970825 \n",
      "\n",
      "####### Data Item:  71  #######\n",
      "\tElapsed time:  504  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  248\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.014920605346560478  :  0.574532687664032 \n",
      "\n",
      "####### Data Item:  72  #######\n",
      "\tElapsed time:  163  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  264\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.7559544444084167  :  0.2864031195640564 \n",
      "\n",
      "####### Data Item:  73  #######\n",
      "\tElapsed time:  570  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  130\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.002736802911385895  :  0.7330166697502136 \n",
      "\n",
      "####### Data Item:  74  #######\n",
      "\tElapsed time:  454  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  365\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.032337408512830734  :  0.5849385857582092 \n",
      "\n",
      "####### Data Item:  75  #######\n",
      "\tElapsed time:  842  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  224\n",
      "\tNumber of words swapped:  56\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9937090873718262  :  0.4726972281932831 \n",
      "\n",
      "####### Data Item:  76  #######\n",
      "\tElapsed time:  261  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  162\n",
      "\tNumber of words swapped:  27\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.9239623546600342  :  0.3701307475566864 \n",
      "\n",
      "####### Data Item:  77  #######\n",
      "\tElapsed time:  264  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  158\n",
      "\tNumber of words swapped:  23\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.9640346169471741  :  0.3304160237312317 \n",
      "\n",
      "####### Data Item:  78  #######\n",
      "\tElapsed time:  500  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  154\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.9938609600067142  :  0.3281131982803345 \n",
      "\n",
      "####### Data Item:  79  #######\n",
      "\tElapsed time:  426  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  186\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.9605617523193359  :  0.3988325595855713 \n",
      "\n",
      "####### Data Item:  80  #######\n",
      "\tElapsed time:  268  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  162\n",
      "\tNumber of words swapped:  22\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9888721108436584  :  0.4742019474506378 \n",
      "\n",
      "####### Data Item:  81  #######\n",
      "\tElapsed time:  900  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  176\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.9936281442642212  :  0.33267736434936523 \n",
      "\n",
      "####### Data Item:  82  #######\n",
      "\tElapsed time:  1091  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  395\n",
      "\tNumber of words swapped:  78\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.005042633507400751  :  0.47273942828178406 \n",
      "\n",
      "####### Data Item:  83  #######\n",
      "\tElapsed time:  569  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  113\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.02681430801749229  :  0.5452854633331299 \n",
      "\n",
      "####### Data Item:  84  #######\n",
      "\tElapsed time:  633  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  419\n",
      "\tNumber of words swapped:  51\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.9790592193603516  :  0.4241715669631958 \n",
      "\n",
      "####### Data Item:  85  #######\n",
      "\tElapsed time:  871  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  247\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.015292116440832615  :  0.5197861194610596 \n",
      "\n",
      "####### Data Item:  86  #######\n",
      "\tElapsed time:  328  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  287\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.03897062316536904  :  0.5700374841690063 \n",
      "\n",
      "####### Data Item:  87  #######\n",
      "\tElapsed time:  1072  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  366\n",
      "\tNumber of words swapped:  77\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.990029752254486  :  0.7295622825622559 \n",
      "\n",
      "####### Data Item:  88  #######\n",
      "\tElapsed time:  1113  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  175\n",
      "\tNumber of words swapped:  60\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9940775632858276  :  0.866449236869812 \n",
      "\n",
      "####### Data Item:  89  #######\n",
      "\tElapsed time:  715  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  110\n",
      "\tNumber of words swapped:  38\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.0017107778694480658  :  0.5495452284812927 \n",
      "\n",
      "####### Data Item:  90  #######\n",
      "\tElapsed time:  667  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  389\n",
      "\tNumber of words swapped:  58\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.029129333794116974  :  0.5180858373641968 \n",
      "\n",
      "####### Data Item:  91  #######\n",
      "\tElapsed time:  209  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  274\n",
      "\tNumber of words swapped:  17\n",
      "\tPercentage modified:  6.0 %\n",
      "\tProb. before and after:  0.916740655899048  :  0.4756085276603699 \n",
      "\n",
      "####### Data Item:  92  #######\n",
      "\tElapsed time:  679  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  165\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.030773110687732693  :  0.5181567072868347 \n",
      "\n",
      "####### Data Item:  93  #######\n",
      "\tElapsed time:  464  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  100\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.9891976714134216  :  0.40312278270721436 \n",
      "\n",
      "####### Data Item:  94  #######\n",
      "\tElapsed time:  268  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  50\n",
      "\tNumber of words swapped:  14\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.05535921826958656  :  0.6213347911834717 \n",
      "\n",
      "####### Data Item:  95  #######\n",
      "\tElapsed time:  1097  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  109\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.006471100728958845  :  0.16654811799526215 \n",
      "\n",
      "####### Data Item:  96  #######\n",
      "\tElapsed time:  461  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  522\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.020413745194673538  :  0.5783077478408813 \n",
      "\n",
      "####### Data Item:  97  #######\n",
      "\tElapsed time:  750  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  795\n",
      "\tNumber of words swapped:  66\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.9842775464057922  :  0.33968088030815125 \n",
      "\n",
      "####### Data Item:  98  #######\n",
      "\tElapsed time:  155  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  208\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.2510852515697479  :  0.6289141774177551 \n",
      "\n",
      "####### Data Item:  99  #######\n",
      "\tElapsed time:  211  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  128\n",
      "\tNumber of words swapped:  19\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.011694381944835186  :  0.5183883905410767 \n",
      "\n",
      "####### Data Item:  100  #######\n",
      "\tElapsed time:  205  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  663\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.6812374591827393  :  0.3608511984348297 \n",
      "\n",
      "####### Data Item:  101  #######\n",
      "\tElapsed time:  379  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  208\n",
      "\tNumber of words swapped:  33\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.021296611055731773  :  0.6621770858764648 \n",
      "\n",
      "####### Data Item:  102  #######\n",
      "\tElapsed time:  363  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  157\n",
      "\tNumber of words swapped:  28\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.05507918447256088  :  0.5226946473121643 \n",
      "\n",
      "####### Data Item:  103  #######\n",
      "\tElapsed time:  665  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  137\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.023794081062078483  :  0.5841048359870911 \n",
      "\n",
      "####### Data Item:  104  #######\n",
      "\tElapsed time:  426  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  204\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.9585625529289246  :  0.4602329432964325 \n",
      "\n",
      "####### Data Item:  105  #######\n",
      "\tElapsed time:  558  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  111\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.9856597185134888  :  0.4752047657966614 \n",
      "\n",
      "####### Data Item:  106  #######\n",
      "\tElapsed time:  509  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  177\n",
      "\tNumber of words swapped:  41\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.9905942678451538  :  0.4983263313770294 \n",
      "\n",
      "####### Data Item:  107  #######\n",
      "\tElapsed time:  634  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  123\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.992256760597229  :  0.4746260941028595 \n",
      "\n",
      "####### Data Item:  108  #######\n",
      "\tElapsed time:  360  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  826\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.9547419548034668  :  0.4235372245311737 \n",
      "\n",
      "####### Data Item:  109  #######\n",
      "\tElapsed time:  562  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  321\n",
      "\tNumber of words swapped:  41\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.9448337554931641  :  0.4599788188934326 \n",
      "\n",
      "####### Data Item:  110  #######\n",
      "\tElapsed time:  152  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  316\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.28355872631073  :  0.605351984500885 \n",
      "\n",
      "####### Data Item:  111  #######\n",
      "\tElapsed time:  160  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  332\n",
      "\tNumber of words swapped:  14\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.7615659832954407  :  0.388773649930954 \n",
      "\n",
      "####### Data Item:  112  #######\n",
      "\tElapsed time:  474  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  259\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.9888637065887452  :  0.48114997148513794 \n",
      "\n",
      "####### Data Item:  113  #######\n",
      "\tElapsed time:  50  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  191\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.8001740574836731  :  0.37062370777130127 \n",
      "\n",
      "####### Data Item:  114  #######\n",
      "\tElapsed time:  311  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  99\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.00836523063480854  :  0.5140477418899536 \n",
      "\n",
      "####### Data Item:  115  #######\n",
      "\tElapsed time:  411  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  196\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.06918337196111679  :  0.5251522660255432 \n",
      "\n",
      "####### Data Item:  116  #######\n",
      "\tElapsed time:  422  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  261\n",
      "\tNumber of words swapped:  33\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.99250727891922  :  0.4238601326942444 \n",
      "\n",
      "####### Data Item:  117  #######\n",
      "\tElapsed time:  49  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  562\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  1.0 %\n",
      "\tProb. before and after:  0.4598589837551117  :  0.5306345224380493 \n",
      "\n",
      "####### Data Item:  118  #######\n",
      "\tElapsed time:  308  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  142\n",
      "\tNumber of words swapped:  24\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.928932011127472  :  0.34221622347831726 \n",
      "\n",
      "####### Data Item:  119  #######\n",
      "\tElapsed time:  874  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  229\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.0017068140441551805  :  0.687366247177124 \n",
      "\n",
      "####### Data Item:  120  #######\n",
      "\tElapsed time:  607  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  121\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.005792516749352218  :  0.6937915086746216 \n",
      "\n",
      "####### Data Item:  121  #######\n",
      "\tElapsed time:  634  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.935371994972229  :  0.4752308130264282 \n",
      "\n",
      "####### Data Item:  122  #######\n",
      "\tElapsed time:  661  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  149\n",
      "\tNumber of words swapped:  37\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9913209080696106  :  0.46973276138305664 \n",
      "\n",
      "####### Data Item:  123  #######\n",
      "\tElapsed time:  469  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  97\n",
      "\tNumber of words swapped:  24\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.976813554763794  :  0.44493797421455383 \n",
      "\n",
      "####### Data Item:  124  #######\n",
      "\tElapsed time:  97  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  324\n",
      "\tNumber of words swapped:  12\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.6543893814086914  :  0.3267940282821655 \n",
      "\n",
      "####### Data Item:  125  #######\n",
      "\tElapsed time:  1098  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  106\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  38.0 %\n",
      "\tProb. before and after:  0.9972960352897644  :  0.7978855967521667 \n",
      "\n",
      "####### Data Item:  126  #######\n",
      "\tElapsed time:  299  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  304\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.9735913276672364  :  0.4791559875011444 \n",
      "\n",
      "####### Data Item:  127  #######\n",
      "\tElapsed time:  503  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  404\n",
      "\tNumber of words swapped:  38\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.8822804689407349  :  0.4612709879875183 \n",
      "\n",
      "####### Data Item:  128  #######\n",
      "\tElapsed time:  1087  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  165\n",
      "\tNumber of words swapped:  53\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.005431387573480606  :  0.33944499492645264 \n",
      "\n",
      "####### Data Item:  129  #######\n",
      "\tElapsed time:  1100  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  144\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.000731474079657346  :  0.08302264660596848 \n",
      "\n",
      "####### Data Item:  130  #######\n",
      "\tElapsed time:  1102  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  66\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.0889994278550148  :  0.4387107789516449 \n",
      "\n",
      "####### Data Item:  131  #######\n",
      "\tElapsed time:  358  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  179\n",
      "\tNumber of words swapped:  27\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.0011062261182814837  :  0.6215675473213196 \n",
      "\n",
      "####### Data Item:  132  #######\n",
      "\tElapsed time:  1068  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  220\n",
      "\tNumber of words swapped:  55\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.0010911750141531233  :  0.016052795574069023 \n",
      "\n",
      "####### Data Item:  133  #######\n",
      "\tElapsed time:  789  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  172\n",
      "\tNumber of words swapped:  45\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.9985150694847108  :  0.36772894859313965 \n",
      "\n",
      "####### Data Item:  134  #######\n",
      "\tElapsed time:  1064  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  135\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.0005692648119293152  :  0.4334089756011963 \n",
      "\n",
      "####### Data Item:  135  #######\n",
      "\tElapsed time:  1075  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  213\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.9925791621208192  :  0.7899084091186523 \n",
      "\n",
      "####### Data Item:  136  #######\n",
      "\tElapsed time:  1112  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  256\n",
      "\tNumber of words swapped:  65\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9914566874504088  :  0.6134633421897888 \n",
      "\n",
      "####### Data Item:  137  #######\n",
      "\tElapsed time:  1084  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  194\n",
      "\tNumber of words swapped:  53\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.990801990032196  :  0.5721525549888611 \n",
      "\n",
      "####### Data Item:  138  #######\n",
      "\tElapsed time:  509  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  530\n",
      "\tNumber of words swapped:  45\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.010741854086518288  :  0.5482475161552429 \n",
      "\n",
      "####### Data Item:  139  #######\n",
      "\tElapsed time:  473  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  194\n",
      "\tNumber of words swapped:  39\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.021374771371483803  :  0.5035995841026306 \n",
      "\n",
      "####### Data Item:  140  #######\n",
      "\tElapsed time:  630  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  122\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.9896507263183594  :  0.47236213088035583 \n",
      "\n",
      "####### Data Item:  141  #######\n",
      "\tElapsed time:  155  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  171\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.8301581144332886  :  0.2523011267185211 \n",
      "\n",
      "####### Data Item:  142  #######\n",
      "\tElapsed time:  988  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  171\n",
      "\tNumber of words swapped:  48\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.0011989995837211607  :  0.18866084516048431 \n",
      "\n",
      "####### Data Item:  143  #######\n",
      "\tElapsed time:  205  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  133\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.9361863732337952  :  0.41147223114967346 \n",
      "\n",
      "####### Data Item:  144  #######\n",
      "\tElapsed time:  299  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  202\n",
      "\tNumber of words swapped:  25\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.025705661624670032  :  0.6854689121246338 \n",
      "\n",
      "####### Data Item:  145  #######\n",
      "\tElapsed time:  247  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  443\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.8480010628700256  :  0.4467240869998932 \n",
      "\n",
      "####### Data Item:  146  #######\n",
      "\tElapsed time:  103  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  126\n",
      "\tNumber of words swapped:  12\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.816534698009491  :  0.4761819541454315 \n",
      "\n",
      "####### Data Item:  147  #######\n",
      "\tElapsed time:  374  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  325\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.9167178273200988  :  0.41087719798088074 \n",
      "\n",
      "####### Data Item:  148  #######\n",
      "\tElapsed time:  898  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  757\n",
      "\tNumber of words swapped:  74\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.033219043165445335  :  0.5097914934158325 \n",
      "\n",
      "####### Data Item:  149  #######\n",
      "\tElapsed time:  49  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  220\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.34020566940307617  :  0.7607871890068054 \n",
      "\n",
      "####### Data Item:  150  #######\n",
      "\tElapsed time:  642  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  446\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.9810459613800048  :  0.45821040868759155 \n",
      "\n",
      "####### Data Item:  151  #######\n",
      "\tElapsed time:  104  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  93\n",
      "\tNumber of words swapped:  9\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.7935424447059631  :  0.34698745608329773 \n",
      "\n",
      "####### Data Item:  152  #######\n",
      "\tElapsed time:  468  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  227\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.9868696331977844  :  0.37078845500946045 \n",
      "\n",
      "####### Data Item:  153  #######\n",
      "\tElapsed time:  149  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  116\n",
      "\tNumber of words swapped:  11\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.9166395664215088  :  0.43692970275878906 \n",
      "\n",
      "####### Data Item:  154  #######\n",
      "\tElapsed time:  103  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  325\n",
      "\tNumber of words swapped:  11\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.6516497135162354  :  0.4879838228225708 \n",
      "\n",
      "####### Data Item:  155  #######\n",
      "\tElapsed time:  925  seconds\n",
      "\tGeneration:  19\n",
      "\tNumber of words in review:  183\n",
      "\tNumber of words swapped:  48\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.9929409027099608  :  0.4962279796600342 \n",
      "\n",
      "####### Data Item:  156  #######\n",
      "\tElapsed time:  1031  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  335\n",
      "\tNumber of words swapped:  80\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.9921011328697203  :  0.8312060832977295 \n",
      "\n",
      "####### Data Item:  157  #######\n",
      "\tElapsed time:  799  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  260\n",
      "\tNumber of words swapped:  64\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.0024854051880538464  :  0.5968705415725708 \n",
      "\n",
      "####### Data Item:  158  #######\n",
      "\tElapsed time:  1015  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  122\n",
      "\tNumber of words swapped:  42\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9757640957832336  :  0.6813408732414246 \n",
      "\n",
      "####### Data Item:  159  #######\n",
      "\tElapsed time:  1094  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  152\n",
      "\tNumber of words swapped:  47\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.005235942546278238  :  0.026844622567296028 \n",
      "\n",
      "####### Data Item:  160  #######\n",
      "\tElapsed time:  567  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  138\n",
      "\tNumber of words swapped:  34\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9783052206039428  :  0.40588119626045227 \n",
      "\n",
      "####### Data Item:  161  #######\n",
      "\tElapsed time:  907  seconds\n",
      "\tGeneration:  19\n",
      "\tNumber of words in review:  108\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.010194080881774429  :  0.6949477791786194 \n",
      "\n",
      "####### Data Item:  162  #######\n",
      "\tElapsed time:  354  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  145\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.9741655588150024  :  0.4930709898471832 \n",
      "\n",
      "####### Data Item:  163  #######\n",
      "\tElapsed time:  209  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  64\n",
      "\tNumber of words swapped:  12\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.09153492748737337  :  0.6271497011184692 \n",
      "\n",
      "####### Data Item:  164  #######\n",
      "\tElapsed time:  1059  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  176\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.004954002331942321  :  0.25235235691070557 \n",
      "\n",
      "####### Data Item:  165  #######\n",
      "\tElapsed time:  1110  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  234\n",
      "\tNumber of words swapped:  66\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.0008698350866325201  :  0.02296774834394455 \n",
      "\n",
      "####### Data Item:  166  #######\n",
      "\tElapsed time:  364  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  238\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.948372483253479  :  0.48740270733833313 \n",
      "\n",
      "####### Data Item:  167  #######\n",
      "\tElapsed time:  1019  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  457\n",
      "\tNumber of words swapped:  72\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.00541384844109416  :  0.3897932469844818 \n",
      "\n",
      "####### Data Item:  168  #######\n",
      "\tElapsed time:  599  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  324\n",
      "\tNumber of words swapped:  49\n",
      "\tPercentage modified:  15.0 %\n",
      "\tProb. before and after:  0.99073725938797  :  0.478085458278656 \n",
      "\n",
      "####### Data Item:  169  #######\n",
      "\tElapsed time:  101  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  102\n",
      "\tNumber of words swapped:  9\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.918948531150818  :  0.4534982740879059 \n",
      "\n",
      "####### Data Item:  170  #######\n",
      "\tElapsed time:  700  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  285\n",
      "\tNumber of words swapped:  55\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.013829906471073627  :  0.6003627777099609 \n",
      "\n",
      "####### Data Item:  171  #######\n",
      "\tElapsed time:  454  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  173\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.04737640544772148  :  0.5191294550895691 \n",
      "\n",
      "####### Data Item:  172  #######\n",
      "\tElapsed time:  1049  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  268\n",
      "\tNumber of words swapped:  61\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.001096509164199233  :  0.05991993099451065 \n",
      "\n",
      "####### Data Item:  173  #######\n",
      "\tElapsed time:  839  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  163\n",
      "\tNumber of words swapped:  52\n",
      "\tPercentage modified:  32.0 %\n",
      "\tProb. before and after:  0.9986020922660828  :  0.4587118327617645 \n",
      "\n",
      "####### Data Item:  174  #######\n",
      "\tElapsed time:  470  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  393\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.006881974637508392  :  0.6360123753547668 \n",
      "\n",
      "####### Data Item:  175  #######\n",
      "\tElapsed time:  1131  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  70\n",
      "\tNumber of words swapped:  28\n",
      "\tPercentage modified:  40.0 %\n",
      "\tProb. before and after:  0.98346346616745  :  0.7671481966972351 \n",
      "\n",
      "####### Data Item:  176  #######\n",
      "\tElapsed time:  252  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  126\n",
      "\tNumber of words swapped:  23\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.01415192149579525  :  0.6263458728790283 \n",
      "\n",
      "####### Data Item:  177  #######\n",
      "\tElapsed time:  1026  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  113\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  35.0 %\n",
      "\tProb. before and after:  0.0009926462080329657  :  0.15512476861476898 \n",
      "\n",
      "####### Data Item:  178  #######\n",
      "\tElapsed time:  557  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  67\n",
      "\tNumber of words swapped:  23\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9852319955825806  :  0.4584012031555176 \n",
      "\n",
      "####### Data Item:  179  #######\n",
      "\tElapsed time:  306  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  120\n",
      "\tNumber of words swapped:  26\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.009670399129390715  :  0.6041370630264282 \n",
      "\n",
      "####### Data Item:  180  #######\n",
      "\tElapsed time:  160  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  151\n",
      "\tNumber of words swapped:  16\n",
      "\tPercentage modified:  11.0 %\n",
      "\tProb. before and after:  0.7802270650863647  :  0.3072684407234192 \n",
      "\n",
      "####### Data Item:  181  #######\n",
      "\tElapsed time:  1077  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  204\n",
      "\tNumber of words swapped:  60\n",
      "\tPercentage modified:  28.999999999999996 %\n",
      "\tProb. before and after:  0.0002795076288748532  :  0.07258658111095428 \n",
      "\n",
      "####### Data Item:  182  #######\n",
      "\tElapsed time:  1018  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  119\n",
      "\tNumber of words swapped:  49\n",
      "\tPercentage modified:  41.0 %\n",
      "\tProb. before and after:  0.0002845024864654988  :  0.40842801332473755 \n",
      "\n",
      "####### Data Item:  183  #######\n",
      "\tElapsed time:  256  seconds\n",
      "\tGeneration:  5\n",
      "\tNumber of words in review:  245\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.9246161580085754  :  0.43364232778549194 \n",
      "\n",
      "####### Data Item:  184  #######\n",
      "\tElapsed time:  50  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  259\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.23791271448135376  :  0.7250516414642334 \n",
      "\n",
      "####### Data Item:  185  #######\n",
      "\tElapsed time:  302  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  134\n",
      "\tNumber of words swapped:  25\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.9729138016700744  :  0.30494052171707153 \n",
      "\n",
      "####### Data Item:  186  #######\n",
      "\tElapsed time:  104  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  177\n",
      "\tNumber of words swapped:  9\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.8034048676490784  :  0.3790397644042969 \n",
      "\n",
      "####### Data Item:  187  #######\n",
      "\tElapsed time:  182  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  156\n",
      "\tNumber of words swapped:  14\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.11145492643117903  :  0.5309423804283142 \n",
      "\n",
      "####### Data Item:  188  #######\n",
      "\tElapsed time:  477  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  142\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.9762117266654968  :  0.44208747148513794 \n",
      "\n",
      "####### Data Item:  189  #######\n",
      "\tElapsed time:  1100  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  295\n",
      "\tNumber of words swapped:  71\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.982030987739563  :  0.7174155712127686 \n",
      "\n",
      "####### Data Item:  190  #######\n",
      "\tElapsed time:  208  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  92\n",
      "\tNumber of words swapped:  17\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.025438996031880386  :  0.7821828126907349 \n",
      "\n",
      "####### Data Item:  191  #######\n",
      "\tElapsed time:  714  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  551\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.03772751241922378  :  0.5045791864395142 \n",
      "\n",
      "####### Data Item:  192  #######\n",
      "\tElapsed time:  689  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  225\n",
      "\tNumber of words swapped:  55\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.9945266842842102  :  0.39304041862487793 \n",
      "\n",
      "####### Data Item:  193  #######\n",
      "\tElapsed time:  214  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  945\n",
      "\tNumber of words swapped:  21\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.7576837539672852  :  0.49648332595825195 \n",
      "\n",
      "####### Data Item:  194  #######\n",
      "\tElapsed time:  387  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  203\n",
      "\tNumber of words swapped:  28\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9374794363975524  :  0.46229249238967896 \n",
      "\n",
      "####### Data Item:  195  #######\n",
      "\tElapsed time:  205  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  111\n",
      "\tNumber of words swapped:  15\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9194779396057128  :  0.3944927155971527 \n",
      "\n",
      "####### Data Item:  196  #######\n",
      "\tElapsed time:  1108  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  148\n",
      "\tNumber of words swapped:  46\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.005119005683809519  :  0.18115389347076416 \n",
      "\n",
      "####### Data Item:  197  #######\n",
      "\tElapsed time:  702  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  317\n",
      "\tNumber of words swapped:  52\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.06624512374401093  :  0.5254958271980286 \n",
      "\n",
      "####### Data Item:  198  #######\n",
      "\tElapsed time:  298  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  223\n",
      "\tNumber of words swapped:  27\n",
      "\tPercentage modified:  12.0 %\n",
      "\tProb. before and after:  0.9334509372711182  :  0.4551438093185425 \n",
      "\n",
      "####### Data Item:  199  #######\n",
      "\tElapsed time:  1105  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  759\n",
      "\tNumber of words swapped:  77\n",
      "\tPercentage modified:  10.0 %\n",
      "\tProb. before and after:  0.0036931056529283524  :  0.06679636240005493 \n",
      "\n",
      "####### Data Item:  200  #######\n",
      "\tElapsed time:  528  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  655\n",
      "\tNumber of words swapped:  45\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.05449338629841805  :  0.526179313659668 \n",
      "\n",
      "####### Data Item:  201  #######\n",
      "\tElapsed time:  52  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  113\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.7229152321815491  :  0.2254236936569214 \n",
      "\n",
      "####### Data Item:  202  #######\n",
      "\tElapsed time:  151  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  350\n",
      "\tNumber of words swapped:  14\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.031811024993658066  :  0.6224957704544067 \n",
      "\n",
      "####### Data Item:  203  #######\n",
      "\tElapsed time:  408  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  217\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.9794110059738159  :  0.41353508830070496 \n",
      "\n",
      "####### Data Item:  204  #######\n",
      "\tElapsed time:  202  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  362\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.017631467431783676  :  0.576104998588562 \n",
      "\n",
      "####### Data Item:  205  #######\n",
      "\tElapsed time:  908  seconds\n",
      "\tGeneration:  18\n",
      "\tNumber of words in review:  118\n",
      "\tNumber of words swapped:  42\n",
      "\tPercentage modified:  36.0 %\n",
      "\tProb. before and after:  0.9956334233283995  :  0.38690385222435 \n",
      "\n",
      "####### Data Item:  206  #######\n",
      "\tElapsed time:  57  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  189\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.6575444936752319  :  0.4798612594604492 \n",
      "\n",
      "####### Data Item:  207  #######\n",
      "\tElapsed time:  481  seconds\n",
      "\tGeneration:  9\n",
      "\tNumber of words in review:  182\n",
      "\tNumber of words swapped:  25\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9340029954910278  :  0.4515630006790161 \n",
      "\n",
      "####### Data Item:  208  #######\n",
      "\tElapsed time:  1057  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  109\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  28.999999999999996 %\n",
      "\tProb. before and after:  0.005644870456308126  :  0.3551397919654846 \n",
      "\n",
      "####### Data Item:  209  #######\n",
      "\tElapsed time:  711  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  143\n",
      "\tNumber of words swapped:  45\n",
      "\tPercentage modified:  31.0 %\n",
      "\tProb. before and after:  0.975735068321228  :  0.4515239894390106 \n",
      "\n",
      "####### Data Item:  210  #######\n",
      "\tElapsed time:  549  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  130\n",
      "\tNumber of words swapped:  31\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.013704564422369005  :  0.8523426651954651 \n",
      "\n",
      "####### Data Item:  211  #######\n",
      "\tElapsed time:  1057  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  172\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  34.0 %\n",
      "\tProb. before and after:  0.9907717704772948  :  0.792716920375824 \n",
      "\n",
      "####### Data Item:  212  #######\n",
      "\tElapsed time:  372  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  166\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.0255155898630619  :  0.5618605017662048 \n",
      "\n",
      "####### Data Item:  213  #######\n",
      "\tElapsed time:  616  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  178\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.02079210989177227  :  0.5480394959449768 \n",
      "\n",
      "####### Data Item:  214  #######\n",
      "\tElapsed time:  738  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  197\n",
      "\tNumber of words swapped:  47\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.00329505349509418  :  0.5388823747634888 \n",
      "\n",
      "####### Data Item:  215  #######\n",
      "\tElapsed time:  152  seconds\n",
      "\tGeneration:  3\n",
      "\tNumber of words in review:  134\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  7.000000000000001 %\n",
      "\tProb. before and after:  0.7569559216499329  :  0.40092095732688904 \n",
      "\n",
      "####### Data Item:  216  #######\n",
      "\tElapsed time:  50  seconds\n",
      "\tGeneration:  1\n",
      "\tNumber of words in review:  137\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  4.0 %\n",
      "\tProb. before and after:  0.8352363705635071  :  0.3710249364376068 \n",
      "\n",
      "####### Data Item:  217  #######\n",
      "\tElapsed time:  559  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  173\n",
      "\tNumber of words swapped:  38\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.02155059576034546  :  0.5017931461334229 \n",
      "\n",
      "####### Data Item:  218  #######\n",
      "\tElapsed time:  214  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  126\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9452228546142578  :  0.29955893754959106 \n",
      "\n",
      "####### Data Item:  219  #######\n",
      "\tElapsed time:  564  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  246\n",
      "\tNumber of words swapped:  45\n",
      "\tPercentage modified:  18.0 %\n",
      "\tProb. before and after:  0.964992105960846  :  0.45537126064300537 \n",
      "\n",
      "####### Data Item:  220  #######\n",
      "\tElapsed time:  314  seconds\n",
      "\tGeneration:  6\n",
      "\tNumber of words in review:  158\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.019006146118044853  :  0.5875032544136047 \n",
      "\n",
      "####### Data Item:  221  #######\n",
      "\tElapsed time:  944  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  132\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  30.0 %\n",
      "\tProb. before and after:  0.001771317911334336  :  0.21634982526302338 \n",
      "\n",
      "####### Data Item:  222  #######\n",
      "\tElapsed time:  340  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  644\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  5.0 %\n",
      "\tProb. before and after:  0.10629912465810776  :  0.6358018517494202 \n",
      "\n",
      "####### Data Item:  223  #######\n",
      "\tElapsed time:  1051  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  192\n",
      "\tNumber of words swapped:  53\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.0047585139982402325  :  0.03406044468283653 \n",
      "\n",
      "####### Data Item:  224  #######\n",
      "\tElapsed time:  701  seconds\n",
      "\tGeneration:  14\n",
      "\tNumber of words in review:  313\n",
      "\tNumber of words swapped:  50\n",
      "\tPercentage modified:  16.0 %\n",
      "\tProb. before and after:  0.9807415604591372  :  0.47440582513809204 \n",
      "\n",
      "####### Data Item:  225  #######\n",
      "\tElapsed time:  94  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  37\n",
      "\tNumber of words swapped:  10\n",
      "\tPercentage modified:  27.0 %\n",
      "\tProb. before and after:  0.9073255062103271  :  0.421915739774704 \n",
      "\n",
      "####### Data Item:  226  #######\n",
      "\tElapsed time:  836  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  608\n",
      "\tNumber of words swapped:  87\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.009364640340209007  :  0.5608532428741455 \n",
      "\n",
      "####### Data Item:  227  #######\n",
      "\tElapsed time:  415  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  103\n",
      "\tNumber of words swapped:  29\n",
      "\tPercentage modified:  28.000000000000004 %\n",
      "\tProb. before and after:  0.9773009419441224  :  0.42724791169166565 \n",
      "\n",
      "####### Data Item:  228  #######\n",
      "\tElapsed time:  196  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  159\n",
      "\tNumber of words swapped:  22\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.9839230179786682  :  0.1882351040840149 \n",
      "\n",
      "####### Data Item:  229  #######\n",
      "\tElapsed time:  620  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  179\n",
      "\tNumber of words swapped:  43\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.0037854234687984  :  0.5427635908126831 \n",
      "\n",
      "####### Data Item:  230  #######\n",
      "\tElapsed time:  1034  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  242\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.004237036220729351  :  0.23607170581817627 \n",
      "\n",
      "####### Data Item:  231  #######\n",
      "\tElapsed time:  1018  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  59\n",
      "\tNumber of words swapped:  24\n",
      "\tPercentage modified:  41.0 %\n",
      "\tProb. before and after:  0.005112130660563707  :  0.24653923511505127 \n",
      "\n",
      "####### Data Item:  232  #######\n",
      "\tElapsed time:  503  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  211\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  19.0 %\n",
      "\tProb. before and after:  0.02152545377612114  :  0.5279065370559692 \n",
      "\n",
      "####### Data Item:  233  #######\n",
      "\tElapsed time:  547  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  202\n",
      "\tNumber of words swapped:  44\n",
      "\tPercentage modified:  22.0 %\n",
      "\tProb. before and after:  0.989238977432251  :  0.2727798819541931 \n",
      "\n",
      "####### Data Item:  234  #######\n",
      "\tElapsed time:  800  seconds\n",
      "\tGeneration:  16\n",
      "\tNumber of words in review:  399\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  14.000000000000002 %\n",
      "\tProb. before and after:  0.010014123283326626  :  0.5579285621643066 \n",
      "\n",
      "####### Data Item:  235  #######\n",
      "\tElapsed time:  1093  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  277\n",
      "\tNumber of words swapped:  73\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.003259977325797081  :  0.4102332592010498 \n",
      "\n",
      "####### Data Item:  236  #######\n",
      "\tElapsed time:  351  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  145\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  21.0 %\n",
      "\tProb. before and after:  0.035000376403331764  :  0.5972567796707153 \n",
      "\n",
      "####### Data Item:  237  #######\n",
      "\tElapsed time:  358  seconds\n",
      "\tGeneration:  7\n",
      "\tNumber of words in review:  113\n",
      "\tNumber of words swapped:  27\n",
      "\tPercentage modified:  24.0 %\n",
      "\tProb. before and after:  0.975425899028778  :  0.3247845470905304 \n",
      "\n",
      "####### Data Item:  238  #######\n",
      "\tElapsed time:  981  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  310\n",
      "\tNumber of words swapped:  79\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9960579872131348  :  0.8021161556243896 \n",
      "\n",
      "####### Data Item:  239  #######\n",
      "\tElapsed time:  988  seconds\n",
      "\tGeneration:  20\n",
      "\tNumber of words in review:  257\n",
      "\tNumber of words swapped:  59\n",
      "\tPercentage modified:  23.0 %\n",
      "\tProb. before and after:  0.9934070110321044  :  0.4866747260093689 \n",
      "\n",
      "####### Data Item:  240  #######\n",
      "\tElapsed time:  202  seconds\n",
      "\tGeneration:  4\n",
      "\tNumber of words in review:  73\n",
      "\tNumber of words swapped:  18\n",
      "\tPercentage modified:  25.0 %\n",
      "\tProb. before and after:  0.9526079893112184  :  0.32743027806282043 \n",
      "\n",
      "####### Data Item:  241  #######\n",
      "\tElapsed time:  404  seconds\n",
      "\tGeneration:  8\n",
      "\tNumber of words in review:  429\n",
      "\tNumber of words swapped:  35\n",
      "\tPercentage modified:  8.0 %\n",
      "\tProb. before and after:  0.8771718144416809  :  0.48800286650657654 \n",
      "\n",
      "####### Data Item:  242  #######\n",
      "\tElapsed time:  861  seconds\n",
      "\tGeneration:  17\n",
      "\tNumber of words in review:  329\n",
      "\tNumber of words swapped:  57\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.0042128064669668674  :  0.6093482971191406 \n",
      "\n",
      "####### Data Item:  243  #######\n",
      "\tElapsed time:  623  seconds\n",
      "\tGeneration:  12\n",
      "\tNumber of words in review:  253\n",
      "\tNumber of words swapped:  43\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.00509487045928836  :  0.5368659496307373 \n",
      "\n",
      "####### Data Item:  244  #######\n",
      "\tElapsed time:  673  seconds\n",
      "\tGeneration:  13\n",
      "\tNumber of words in review:  236\n",
      "\tNumber of words swapped:  40\n",
      "\tPercentage modified:  17.0 %\n",
      "\tProb. before and after:  0.003745395923033357  :  0.5757519602775574 \n",
      "\n",
      "####### Data Item:  245  #######\n",
      "\tElapsed time:  922  seconds\n",
      "\tGeneration:  18\n",
      "\tNumber of words in review:  555\n",
      "\tNumber of words swapped:  71\n",
      "\tPercentage modified:  13.0 %\n",
      "\tProb. before and after:  0.005970606580376625  :  0.5083473324775696 \n",
      "\n",
      "####### Data Item:  246  #######\n",
      "\tElapsed time:  547  seconds\n",
      "\tGeneration:  11\n",
      "\tNumber of words in review:  152\n",
      "\tNumber of words swapped:  30\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.0035710416268557314  :  0.6250085830688477 \n",
      "\n",
      "####### Data Item:  247  #######\n",
      "\tElapsed time:  102  seconds\n",
      "\tGeneration:  2\n",
      "\tNumber of words in review:  134\n",
      "\tNumber of words swapped:  12\n",
      "\tPercentage modified:  9.0 %\n",
      "\tProb. before and after:  0.8751592636108398  :  0.2106870710849762 \n",
      "\n",
      "####### Data Item:  248  #######\n",
      "\tElapsed time:  1008  seconds\n",
      "\tGeneration:  0\n",
      "\tNumber of words in review:  109\n",
      "\tNumber of words swapped:  36\n",
      "\tPercentage modified:  33.0 %\n",
      "\tProb. before and after:  0.005607310216873884  :  0.016075246036052704 \n",
      "\n",
      "####### Data Item:  249  #######\n",
      "\tElapsed time:  786  seconds\n",
      "\tGeneration:  15\n",
      "\tNumber of words in review:  300\n",
      "\tNumber of words swapped:  61\n",
      "\tPercentage modified:  20.0 %\n",
      "\tProb. before and after:  0.9941260814666748  :  0.4574572741985321 \n",
      "\n",
      "####### Data Item:  250  #######\n",
      "\tElapsed time:  511  seconds\n",
      "\tGeneration:  10\n",
      "\tNumber of words in review:  121\n",
      "\tNumber of words swapped:  32\n",
      "\tPercentage modified:  26.0 %\n",
      "\tProb. before and after:  0.961735188961029  :  0.49651336669921875 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>1</td>\n",
       "      <td>well i must say this is probably the meanest f...</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not saying somethings newer when i say tha...</td>\n",
       "      <td>0.426562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>1</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.637239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>caught this flick as anyone of a fivefor addre...</td>\n",
       "      <td>0.294813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             1  well i must say this is probably the meanest f...  0.768538   \n",
       "1             0  if christopher nolan had made memento before f...  0.395876   \n",
       "2             0  i'm not saying somethings newer when i say tha...  0.426562   \n",
       "3             1  i have been watching lost with my family since...  0.637239   \n",
       "4             0  caught this flick as anyone of a fivefor addre...  0.294813   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0               0           0.0                    N                0.0   \n",
       "1               0           0.0                    N                0.0   \n",
       "2               0           0.0                    N                0.0   \n",
       "3               0           0.0                    N                0.0   \n",
       "4               0           0.0                    N                0.0   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               5            264  \n",
       "1               2            107  \n",
       "2               2            106  \n",
       "3               1             53  \n",
       "4               5            256  "
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' TYPE_OF_ATTACK:\n",
    "    1 == BERT i.e. using context to generate replacement words\n",
    "    2 == Distance Matrix i.e. using only nearest neighbour - synonyms to generate replacement words\n",
    "    3 == Distance Matric and BERT sentence score i.e. find synonyms and then use sentence structure to score partial sentence for each replacement word\n",
    "'''\n",
    "data_sample = load_ga_attack_results('sample_stats_set_250.csv')\n",
    "\n",
    "TYPE_OF_ATTACK = 3\n",
    "\n",
    "GA_Attack_Stats(data_sample, type_of_attack=TYPE_OF_ATTACK)\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save stats sample data_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to:\t ga_attack_results/ga_results_DISTANCE_BERT_STATS.csv\n",
      "loaded file:  ga_results_DISTANCE_BERT.csv\n"
     ]
    }
   ],
   "source": [
    "ga_stats_results_BERT = \"ga_results_BERT_STATS.csv\"\n",
    "ga_stats_results_DISTANCE = \"ga_results_DISTANCE_STATS.csv\"\n",
    "ga_stats_results_DISTANCE_BERT = \"ga_results_DISTANCE_BERT_STATS.csv\"\n",
    "\n",
    "if TYPE_OF_ATTACK == 1:        \n",
    "    save_ga_attack_results(data_sample, ga_stats_results_BERT)\n",
    "if TYPE_OF_ATTACK == 2:\n",
    "    save_ga_attack_results(data_sample, ga_stats_results_DISTANCE)\n",
    "if TYPE_OF_ATTACK == 3:\n",
    "    save_ga_attack_results(data_sample, ga_stats_results_DISTANCE_BERT)\n",
    "    \n",
    "if TYPE_OF_ATTACK == 1:\n",
    "    data_sample = load_ga_attack_results(ga_stats_results_BERT)\n",
    "    print(\"loaded file: \", ga_stats_results_BERT)\n",
    "if TYPE_OF_ATTACK == 2:\n",
    "    data_sample = load_ga_attack_results(ga_stats_results_DISTANCE)\n",
    "    print(\"loaded file: \", ga_stats_results_DISTANCE)\n",
    "if TYPE_OF_ATTACK == 3:\n",
    "    data_sample = load_ga_attack_results(ga_stats_results_DISTANCE_BERT)\n",
    "    print(\"loaded file: \",ga_results_DISTANCE_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>1</td>\n",
       "      <td>well i must say this is probably the meanest f...</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>20</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>10</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not saying somethings newer when i say tha...</td>\n",
       "      <td>0.426562</td>\n",
       "      <td>10</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>1</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.637239</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978884</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>caught this flick as anyone of a fivefor addre...</td>\n",
       "      <td>0.294813</td>\n",
       "      <td>29</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             1  well i must say this is probably the meanest f...  0.768538   \n",
       "1             0  if christopher nolan had made memento before f...  0.395876   \n",
       "2             0  i'm not saying somethings newer when i say tha...  0.426562   \n",
       "3             1  i have been watching lost with my family since...  0.637239   \n",
       "4             0  caught this flick as anyone of a fivefor addre...  0.294813   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0              20      0.895522                    Y               0.13   \n",
       "1              10      0.966031                    Y               0.03   \n",
       "2              10      0.958418                    Y               0.05   \n",
       "3               5      0.978884                    Y               0.01   \n",
       "4              29      0.836257                    Y               0.19   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               5            264  \n",
       "1               2            107  \n",
       "2               2            106  \n",
       "3               1             53  \n",
       "4               5            256  "
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded DISTANCE MATRIX results file:  ga_results_DISTANCE_BERT.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright sidney bruhl michael caine has had ...</td>\n",
       "      <td>0.978781</td>\n",
       "      <td>0</td>\n",
       "      <td>playwright sidney bruhl micheal channeling has...</td>\n",
       "      <td>0.475356</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yikes did this movie blow  the characters were...</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>1</td>\n",
       "      <td>yikes did this filmmaking detonate the hallmar...</td>\n",
       "      <td>0.517380</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this would have to be one of the worst  if not...</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>1</td>\n",
       "      <td>this would have to be anyone of the hardest if...</td>\n",
       "      <td>0.538070</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock holmes adventure starring...</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock harrison escapades starri...</td>\n",
       "      <td>0.591156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first  i must say that i don't speak spanish a...</td>\n",
       "      <td>0.906772</td>\n",
       "      <td>0</td>\n",
       "      <td>first i must said that i don't speak spaniard ...</td>\n",
       "      <td>0.375404</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          1  playwright sidney bruhl michael caine has had ...  0.978781   \n",
       "1          0  yikes did this movie blow  the characters were...  0.007015   \n",
       "2          0  this would have to be one of the worst  if not...  0.016789   \n",
       "3          1  a brilliant sherlock holmes adventure starring...  0.999339   \n",
       "4          1  first  i must say that i don't speak spanish a...  0.906772   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             0  playwright sidney bruhl micheal channeling has...  0.475356   \n",
       "1             1  yikes did this filmmaking detonate the hallmar...  0.517380   \n",
       "2             1  this would have to be anyone of the hardest if...  0.538070   \n",
       "3             1  a brilliant sherlock harrison escapades starri...  0.591156   \n",
       "4             0  first i must said that i don't speak spaniard ...  0.375404   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \n",
       "0               0           0.0                    N                0.0  \n",
       "1               0           0.0                    N                0.0  \n",
       "2               0           0.0                    N                0.0  \n",
       "3               0           0.0                    N                0.0  \n",
       "4               0           0.0                    N                0.0  "
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_results_datafile():\n",
    "    if TYPE_OF_ATTACK == 1:\n",
    "        data_sample = load_ga_attack_results(ga_results_BERT)\n",
    "        print(\"loaded BERT results file\")\n",
    "    if TYPE_OF_ATTACK == 2:\n",
    "        data_sample = load_ga_attack_results(ga_results_DISTANCE)\n",
    "        print(\"loaded DISTANCE MATRIX results file\")\n",
    "    if TYPE_OF_ATTACK == 3:\n",
    "        data_sample = load_ga_attack_results(ga_results_DISTANCE_BERT)\n",
    "        print(\"loaded DISTANCE MATRIX results file: \",ga_results_DISTANCE_BERT)\n",
    "    \n",
    "    return data_sample\n",
    "\n",
    "# data_sample = load_results_datafile()\n",
    "# data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06)  Calculate stats after the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>1</td>\n",
       "      <td>well i must say this is probably the meanest f...</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>N</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>N</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not saying somethings newer when i say tha...</td>\n",
       "      <td>0.426562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>N</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>1</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.637239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978884</td>\n",
       "      <td>N</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>caught this flick as anyone of a fivefor addre...</td>\n",
       "      <td>0.294813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>N</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>this movie is sad  according to my fellow imdb...</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>1</td>\n",
       "      <td>this movie is unfortunate conformity to my cow...</td>\n",
       "      <td>0.582794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786872</td>\n",
       "      <td>N</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>20</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>as an animated film from this is pretty goodge...</td>\n",
       "      <td>0.981019</td>\n",
       "      <td>0</td>\n",
       "      <td>as an animated film from this is pretty goodge...</td>\n",
       "      <td>0.470409</td>\n",
       "      <td>0</td>\n",
       "      <td>0.919526</td>\n",
       "      <td>N</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>10</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>it must say something about the state of our n...</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>1</td>\n",
       "      <td>it must say somethin about the state of our na...</td>\n",
       "      <td>0.505280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935367</td>\n",
       "      <td>N</td>\n",
       "      <td>0.079755</td>\n",
       "      <td>9</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>whoa  this is one of the worst movies i have e...</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>1</td>\n",
       "      <td>whoa this is one of the roughest movies i have...</td>\n",
       "      <td>0.584806</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781027</td>\n",
       "      <td>N</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>12</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>not a film of entertainment  but of real lives...</td>\n",
       "      <td>0.858666</td>\n",
       "      <td>0</td>\n",
       "      <td>not a film of recreation but of veritable inha...</td>\n",
       "      <td>0.177477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>N</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>goo technomusic accompanying medieval swordpla...</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>1</td>\n",
       "      <td>goo technomusic accompanying medieval swordpla...</td>\n",
       "      <td>0.532833</td>\n",
       "      <td>0</td>\n",
       "      <td>0.784341</td>\n",
       "      <td>N</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>11</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>i know that chill wills usually played lovable...</td>\n",
       "      <td>0.531444</td>\n",
       "      <td>0</td>\n",
       "      <td>i know that chill wills usually played lovable...</td>\n",
       "      <td>0.394874</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974971</td>\n",
       "      <td>N</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>i love most movies and i'm a big fan of sean b...</td>\n",
       "      <td>0.112102</td>\n",
       "      <td>1</td>\n",
       "      <td>i love most movie and i'm a grand fan of are b...</td>\n",
       "      <td>0.567966</td>\n",
       "      <td>0</td>\n",
       "      <td>0.898643</td>\n",
       "      <td>N</td>\n",
       "      <td>0.155738</td>\n",
       "      <td>4</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>i've just had the evidence that confirmed my s...</td>\n",
       "      <td>0.380431</td>\n",
       "      <td>1</td>\n",
       "      <td>i've just had the evidence that confirmed my s...</td>\n",
       "      <td>0.506512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971849</td>\n",
       "      <td>N</td>\n",
       "      <td>0.023179</td>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>think 'charmed' with testosterone instead of e...</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0</td>\n",
       "      <td>think 'charmed' with hormones alternately of e...</td>\n",
       "      <td>0.111627</td>\n",
       "      <td>0</td>\n",
       "      <td>0.798669</td>\n",
       "      <td>N</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>i initially bought this dvd because it had srk...</td>\n",
       "      <td>0.920890</td>\n",
       "      <td>0</td>\n",
       "      <td>i initially bought this vcd because it had srk...</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944668</td>\n",
       "      <td>N</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>4</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>this is another film i missed out on italian t...</td>\n",
       "      <td>0.072082</td>\n",
       "      <td>1</td>\n",
       "      <td>this is another films i missed out on italian ...</td>\n",
       "      <td>0.515770</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937682</td>\n",
       "      <td>N</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>5</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>he was my hero for all time until he went alon...</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>1</td>\n",
       "      <td>he was my hero for all times until he went alo...</td>\n",
       "      <td>0.501515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900595</td>\n",
       "      <td>N</td>\n",
       "      <td>0.148256</td>\n",
       "      <td>11</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>nightscream is a tv movie so it's bound to be ...</td>\n",
       "      <td>0.394440</td>\n",
       "      <td>1</td>\n",
       "      <td>nightscream is a tv films so it's bound to be ...</td>\n",
       "      <td>0.645848</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982565</td>\n",
       "      <td>N</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>beast wars is a show that is overhyped  overpr...</td>\n",
       "      <td>0.015007</td>\n",
       "      <td>1</td>\n",
       "      <td>beast wars is a show that is overhyped overpra...</td>\n",
       "      <td>0.509039</td>\n",
       "      <td>0</td>\n",
       "      <td>0.839018</td>\n",
       "      <td>N</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>11</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>the story for the firstaired television instal...</td>\n",
       "      <td>0.966562</td>\n",
       "      <td>0</td>\n",
       "      <td>the story for the firstaired television instal...</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0</td>\n",
       "      <td>0.858592</td>\n",
       "      <td>N</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>7</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>chokher bali  a passion play based on rabindr...</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>1</td>\n",
       "      <td>chokher bali  a fervor playing based on rabin...</td>\n",
       "      <td>0.668990</td>\n",
       "      <td>0</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>N</td>\n",
       "      <td>0.199438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>ever since i remember  i have loved airplanes ...</td>\n",
       "      <td>0.926001</td>\n",
       "      <td>0</td>\n",
       "      <td>ever since i remember i have loved airplanes a...</td>\n",
       "      <td>0.278428</td>\n",
       "      <td>0</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>N</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>3</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>i thought this movie was highly underrated  th...</td>\n",
       "      <td>0.970309</td>\n",
       "      <td>0</td>\n",
       "      <td>i thought this films was supremely undervalued...</td>\n",
       "      <td>0.492154</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>N</td>\n",
       "      <td>0.201439</td>\n",
       "      <td>7</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>people tend to complain about the number of fi...</td>\n",
       "      <td>0.879474</td>\n",
       "      <td>0</td>\n",
       "      <td>people tend to complain about the number of fi...</td>\n",
       "      <td>0.498254</td>\n",
       "      <td>0</td>\n",
       "      <td>0.913851</td>\n",
       "      <td>N</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>from the weeks and weeks of promotion for this...</td>\n",
       "      <td>0.013749</td>\n",
       "      <td>0</td>\n",
       "      <td>from the weeks and week of promotion for this ...</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910766</td>\n",
       "      <td>N</td>\n",
       "      <td>0.126939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>now don't get me wrong  i do enjoy christmas m...</td>\n",
       "      <td>0.105219</td>\n",
       "      <td>1</td>\n",
       "      <td>now don't get me wrong i do enjoy navidad movi...</td>\n",
       "      <td>0.594158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944715</td>\n",
       "      <td>N</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>you know  i really have a problem with movie l...</td>\n",
       "      <td>0.035415</td>\n",
       "      <td>1</td>\n",
       "      <td>you know i truly have a problem with movie lis...</td>\n",
       "      <td>0.514445</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933906</td>\n",
       "      <td>N</td>\n",
       "      <td>0.084058</td>\n",
       "      <td>15</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>i've never been huge on imax films  they're co...</td>\n",
       "      <td>0.910961</td>\n",
       "      <td>0</td>\n",
       "      <td>i've never been huge on imax films they're coo...</td>\n",
       "      <td>0.335131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.929907</td>\n",
       "      <td>N</td>\n",
       "      <td>0.096234</td>\n",
       "      <td>4</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film in the worst possible circumst...</td>\n",
       "      <td>0.575034</td>\n",
       "      <td>0</td>\n",
       "      <td>i saw this film in the worse possible circumst...</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>N</td>\n",
       "      <td>0.037313</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>i didn't see this movie until it appeared on t...</td>\n",
       "      <td>0.847940</td>\n",
       "      <td>0</td>\n",
       "      <td>i didn't see this movie until it appeared on t...</td>\n",
       "      <td>0.193286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937971</td>\n",
       "      <td>N</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>wow  a truly fantastic 'trip' movie that has t...</td>\n",
       "      <td>0.976365</td>\n",
       "      <td>0</td>\n",
       "      <td>wow a truly magnificent 'trip' movie that has ...</td>\n",
       "      <td>0.442371</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774922</td>\n",
       "      <td>N</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>17</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>the night listener held my attention  with rob...</td>\n",
       "      <td>0.986968</td>\n",
       "      <td>0</td>\n",
       "      <td>the night listener organised my beware with re...</td>\n",
       "      <td>0.390291</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840313</td>\n",
       "      <td>N</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>11</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>the idea that anyone could of concocted such a...</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0</td>\n",
       "      <td>the idea that anybody would of concoction such...</td>\n",
       "      <td>0.401566</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765101</td>\n",
       "      <td>N</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>one of the best  as being a fan of the civil w...</td>\n",
       "      <td>0.993268</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the better as being a groupie of the ci...</td>\n",
       "      <td>0.612480</td>\n",
       "      <td>0</td>\n",
       "      <td>0.757112</td>\n",
       "      <td>N</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not a big musical fan  but this is one of ...</td>\n",
       "      <td>0.965051</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not a big songwriter fan but this is eden ...</td>\n",
       "      <td>0.348525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.783172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>i watched the first few episodes a short while...</td>\n",
       "      <td>0.087308</td>\n",
       "      <td>1</td>\n",
       "      <td>i watched the first few episodes a succinctly ...</td>\n",
       "      <td>0.513440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949507</td>\n",
       "      <td>N</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>i was lucky enough to see a test screening of ...</td>\n",
       "      <td>0.985280</td>\n",
       "      <td>0</td>\n",
       "      <td>i was lucky suffice to watching a test screeni...</td>\n",
       "      <td>0.449680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844957</td>\n",
       "      <td>N</td>\n",
       "      <td>0.201258</td>\n",
       "      <td>11</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>this is a really bad waste of your time  i wou...</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>1</td>\n",
       "      <td>this is a genuinely naughty detritus of your t...</td>\n",
       "      <td>0.721323</td>\n",
       "      <td>0</td>\n",
       "      <td>0.742741</td>\n",
       "      <td>N</td>\n",
       "      <td>0.348148</td>\n",
       "      <td>17</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>there is an awful lot wrong with this picture ...</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0</td>\n",
       "      <td>there is an awful parcel wrong with this pictu...</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762530</td>\n",
       "      <td>N</td>\n",
       "      <td>0.337931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>the reason why this movie sucks  have these pe...</td>\n",
       "      <td>0.182403</td>\n",
       "      <td>1</td>\n",
       "      <td>the reason why this movie fears have these peo...</td>\n",
       "      <td>0.594817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931311</td>\n",
       "      <td>N</td>\n",
       "      <td>0.101227</td>\n",
       "      <td>7</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>if vampire tales are your cup of blood  then t...</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>1</td>\n",
       "      <td>if vampire tales are your cups of baptism then...</td>\n",
       "      <td>0.555748</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>N</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>flavia the heretic is a strange entry in the n...</td>\n",
       "      <td>0.838611</td>\n",
       "      <td>0</td>\n",
       "      <td>flavia the heretic is a strange entrance in th...</td>\n",
       "      <td>0.374277</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965637</td>\n",
       "      <td>N</td>\n",
       "      <td>0.049261</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>this sorry excuse for a film reminded me a gre...</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>1</td>\n",
       "      <td>this sorry excuse for a film reminded me a gre...</td>\n",
       "      <td>0.527324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.883455</td>\n",
       "      <td>N</td>\n",
       "      <td>0.162896</td>\n",
       "      <td>18</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>if this movie was written directed and produce...</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>1</td>\n",
       "      <td>if this movie was written directed and produce...</td>\n",
       "      <td>0.702419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.841039</td>\n",
       "      <td>N</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>7</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>i am a pretty much a sucker for those ghost hu...</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>1</td>\n",
       "      <td>i am a pretty much a sucker for those specter ...</td>\n",
       "      <td>0.523679</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933969</td>\n",
       "      <td>N</td>\n",
       "      <td>0.085060</td>\n",
       "      <td>16</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm writing this years after the final episode...</td>\n",
       "      <td>0.973567</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm writing this years after the final inciden...</td>\n",
       "      <td>0.368506</td>\n",
       "      <td>0</td>\n",
       "      <td>0.926004</td>\n",
       "      <td>N</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>5</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>brilliant actors and brilliant picture  i love...</td>\n",
       "      <td>0.974310</td>\n",
       "      <td>0</td>\n",
       "      <td>brilliant actors and resplendent picture i lov...</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857332</td>\n",
       "      <td>N</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>9</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>i can find very little thats good to say about...</td>\n",
       "      <td>0.276049</td>\n",
       "      <td>1</td>\n",
       "      <td>i can find very little thats good to say about...</td>\n",
       "      <td>0.627074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950928</td>\n",
       "      <td>N</td>\n",
       "      <td>0.048951</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>after reading the book  which had a lot of mea...</td>\n",
       "      <td>0.076604</td>\n",
       "      <td>1</td>\n",
       "      <td>after reading the book which had a lot of sens...</td>\n",
       "      <td>0.555887</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910443</td>\n",
       "      <td>N</td>\n",
       "      <td>0.113269</td>\n",
       "      <td>8</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text     probs  \\\n",
       "0           0  well i must say this is probably the worst fil...  0.025771   \n",
       "1           1  if christopher nolan had made memento before f...  0.769358   \n",
       "2           1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3           0  i have been watching lost with my family since...  0.476023   \n",
       "4           1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "5           0  this movie is sad  according to my fellow imdb...  0.007910   \n",
       "6           1  as an animated film from this is pretty goodge...  0.981019   \n",
       "7           0  it must say something about the state of our n...  0.050363   \n",
       "8           0  whoa  this is one of the worst movies i have e...  0.002803   \n",
       "9           1  not a film of entertainment  but of real lives...  0.858666   \n",
       "10          0  goo technomusic accompanying medieval swordpla...  0.005910   \n",
       "11          1  i know that chill wills usually played lovable...  0.531444   \n",
       "12          0  i love most movies and i'm a big fan of sean b...  0.112102   \n",
       "13          0  i've just had the evidence that confirmed my s...  0.380431   \n",
       "14          0  think 'charmed' with testosterone instead of e...  0.006076   \n",
       "15          1  i initially bought this dvd because it had srk...  0.920890   \n",
       "16          0  this is another film i missed out on italian t...  0.072082   \n",
       "17          0  he was my hero for all time until he went alon...  0.008051   \n",
       "18          0  nightscream is a tv movie so it's bound to be ...  0.394440   \n",
       "19          0  beast wars is a show that is overhyped  overpr...  0.015007   \n",
       "20          1  the story for the firstaired television instal...  0.966562   \n",
       "21          1  chokher bali  a passion play based on rabindr...  0.999370   \n",
       "22          1  ever since i remember  i have loved airplanes ...  0.926001   \n",
       "23          1  i thought this movie was highly underrated  th...  0.970309   \n",
       "24          1  people tend to complain about the number of fi...  0.879474   \n",
       "25          0  from the weeks and weeks of promotion for this...  0.013749   \n",
       "26          0  now don't get me wrong  i do enjoy christmas m...  0.105219   \n",
       "27          0  you know  i really have a problem with movie l...  0.035415   \n",
       "28          1  i've never been huge on imax films  they're co...  0.910961   \n",
       "29          1  i saw this film in the worst possible circumst...  0.575034   \n",
       "30          1  i didn't see this movie until it appeared on t...  0.847940   \n",
       "31          1  wow  a truly fantastic 'trip' movie that has t...  0.976365   \n",
       "32          1  the night listener held my attention  with rob...  0.986968   \n",
       "33          0  the idea that anyone could of concocted such a...  0.003948   \n",
       "34          1  one of the best  as being a fan of the civil w...  0.993268   \n",
       "35          1  i'm not a big musical fan  but this is one of ...  0.965051   \n",
       "36          0  i watched the first few episodes a short while...  0.087308   \n",
       "37          1  i was lucky enough to see a test screening of ...  0.985280   \n",
       "38          0  this is a really bad waste of your time  i wou...  0.000874   \n",
       "39          0  there is an awful lot wrong with this picture ...  0.001900   \n",
       "40          0  the reason why this movie sucks  have these pe...  0.182403   \n",
       "41          1  if vampire tales are your cup of blood  then t...  0.973070   \n",
       "42          1  flavia the heretic is a strange entry in the n...  0.838611   \n",
       "43          0  this sorry excuse for a film reminded me a gre...  0.004494   \n",
       "44          0  if this movie was written directed and produce...  0.004485   \n",
       "45          0  i am a pretty much a sucker for those ghost hu...  0.018088   \n",
       "46          1  i'm writing this years after the final episode...  0.973567   \n",
       "47          1  brilliant actors and brilliant picture  i love...  0.974310   \n",
       "48          0  i can find very little thats good to say about...  0.276049   \n",
       "49          0  after reading the book  which had a lot of mea...  0.076604   \n",
       "\n",
       "    ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0              1  well i must say this is probably the meanest f...  0.768538   \n",
       "1              0  if christopher nolan had made memento before f...  0.395876   \n",
       "2              0  i'm not saying somethings newer when i say tha...  0.426562   \n",
       "3              1  i have been watching lost with my family since...  0.637239   \n",
       "4              0  caught this flick as anyone of a fivefor addre...  0.294813   \n",
       "5              1  this movie is unfortunate conformity to my cow...  0.582794   \n",
       "6              0  as an animated film from this is pretty goodge...  0.470409   \n",
       "7              1  it must say somethin about the state of our na...  0.505280   \n",
       "8              1  whoa this is one of the roughest movies i have...  0.584806   \n",
       "9              0  not a film of recreation but of veritable inha...  0.177477   \n",
       "10             1  goo technomusic accompanying medieval swordpla...  0.532833   \n",
       "11             0  i know that chill wills usually played lovable...  0.394874   \n",
       "12             1  i love most movie and i'm a grand fan of are b...  0.567966   \n",
       "13             1  i've just had the evidence that confirmed my s...  0.506512   \n",
       "14             0  think 'charmed' with hormones alternately of e...  0.111627   \n",
       "15             0  i initially bought this vcd because it had srk...  0.443320   \n",
       "16             1  this is another films i missed out on italian ...  0.515770   \n",
       "17             1  he was my hero for all times until he went alo...  0.501515   \n",
       "18             1  nightscream is a tv films so it's bound to be ...  0.645848   \n",
       "19             1  beast wars is a show that is overhyped overpra...  0.509039   \n",
       "20             0  the story for the firstaired television instal...  0.401613   \n",
       "21             1  chokher bali  a fervor playing based on rabin...  0.668990   \n",
       "22             0  ever since i remember i have loved airplanes a...  0.278428   \n",
       "23             0  i thought this films was supremely undervalued...  0.492154   \n",
       "24             0  people tend to complain about the number of fi...  0.498254   \n",
       "25             0  from the weeks and week of promotion for this ...  0.307925   \n",
       "26             1  now don't get me wrong i do enjoy navidad movi...  0.594158   \n",
       "27             1  you know i truly have a problem with movie lis...  0.514445   \n",
       "28             0  i've never been huge on imax films they're coo...  0.335131   \n",
       "29             0  i saw this film in the worse possible circumst...  0.347165   \n",
       "30             0  i didn't see this movie until it appeared on t...  0.193286   \n",
       "31             0  wow a truly magnificent 'trip' movie that has ...  0.442371   \n",
       "32             0  the night listener organised my beware with re...  0.390291   \n",
       "33             0  the idea that anybody would of concoction such...  0.401566   \n",
       "34             1  one of the better as being a groupie of the ci...  0.612480   \n",
       "35             0  i'm not a big songwriter fan but this is eden ...  0.348525   \n",
       "36             1  i watched the first few episodes a succinctly ...  0.513440   \n",
       "37             0  i was lucky suffice to watching a test screeni...  0.449680   \n",
       "38             1  this is a genuinely naughty detritus of your t...  0.721323   \n",
       "39             0  there is an awful parcel wrong with this pictu...  0.147876   \n",
       "40             1  the reason why this movie fears have these peo...  0.594817   \n",
       "41             1  if vampire tales are your cups of baptism then...  0.555748   \n",
       "42             0  flavia the heretic is a strange entrance in th...  0.374277   \n",
       "43             1  this sorry excuse for a film reminded me a gre...  0.527324   \n",
       "44             1  if this movie was written directed and produce...  0.702419   \n",
       "45             1  i am a pretty much a sucker for those specter ...  0.523679   \n",
       "46             0  i'm writing this years after the final inciden...  0.368506   \n",
       "47             0  brilliant actors and resplendent picture i lov...  0.457697   \n",
       "48             1  i can find very little thats good to say about...  0.627074   \n",
       "49             1  after reading the book which had a lot of sens...  0.555887   \n",
       "\n",
       "    ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0                0      0.895522                    N           0.129032   \n",
       "1                0      0.966031                    N           0.032895   \n",
       "2                0      0.958418                    N           0.053476   \n",
       "3                0      0.978884                    N           0.013514   \n",
       "4                0      0.836257                    N           0.189542   \n",
       "5                0      0.786872                    N           0.295455   \n",
       "6                0      0.919526                    N           0.105651   \n",
       "7                0      0.935367                    N           0.079755   \n",
       "8                0      0.781027                    N           0.292857   \n",
       "9                0      0.804545                    N           0.275000   \n",
       "10               0      0.784341                    N           0.280000   \n",
       "11               0      0.974971                    N           0.021552   \n",
       "12               0      0.898643                    N           0.155738   \n",
       "13               0      0.971849                    N           0.023179   \n",
       "14               0      0.798669                    N           0.283951   \n",
       "15               0      0.944668                    N           0.087912   \n",
       "16               0      0.937682                    N           0.086792   \n",
       "17               0      0.900595                    N           0.148256   \n",
       "18               0      0.982565                    N           0.033113   \n",
       "19               0      0.839018                    N           0.231343   \n",
       "20               0      0.858592                    N           0.209150   \n",
       "21               0      0.872483                    N           0.199438   \n",
       "22               0      0.897297                    N           0.119048   \n",
       "23               0      0.846715                    N           0.201439   \n",
       "24               0      0.913851                    N           0.089109   \n",
       "25               0      0.910766                    N           0.126939   \n",
       "26               0      0.944715                    N           0.087719   \n",
       "27               0      0.933906                    N           0.084058   \n",
       "28               0      0.929907                    N           0.096234   \n",
       "29               0      0.967742                    N           0.037313   \n",
       "30               0      0.937971                    N           0.061728   \n",
       "31               0      0.774922                    N           0.347826   \n",
       "32               0      0.840313                    N           0.218750   \n",
       "33               0      0.765101                    N           0.343750   \n",
       "34               0      0.757112                    N           0.355422   \n",
       "35               0      0.783172                    N           0.265823   \n",
       "36               0      0.949507                    N           0.070513   \n",
       "37               0      0.844957                    N           0.201258   \n",
       "38               0      0.742741                    N           0.348148   \n",
       "39               0      0.762530                    N           0.337931   \n",
       "40               0      0.931311                    N           0.101227   \n",
       "41               0      0.937500                    N           0.073171   \n",
       "42               0      0.965637                    N           0.049261   \n",
       "43               0      0.883455                    N           0.162896   \n",
       "44               0      0.841039                    N           0.223022   \n",
       "45               0      0.933969                    N           0.085060   \n",
       "46               0      0.926004                    N           0.096386   \n",
       "47               0      0.857332                    N           0.202899   \n",
       "48               0      0.950928                    N           0.048951   \n",
       "49               0      0.910443                    N           0.113269   \n",
       "\n",
       "    ga_generations  ga_time_taken  \n",
       "0                5            264  \n",
       "1                2            107  \n",
       "2                2            106  \n",
       "3                1             53  \n",
       "4                5            256  \n",
       "5               20           1073  \n",
       "6               10            532  \n",
       "7                9            462  \n",
       "8               12            641  \n",
       "9                2             97  \n",
       "10              11            557  \n",
       "11               1             54  \n",
       "12               4            215  \n",
       "13               2            113  \n",
       "14               0              0  \n",
       "15               4            205  \n",
       "16               5            242  \n",
       "17              11            599  \n",
       "18               1             54  \n",
       "19              11            565  \n",
       "20               7            376  \n",
       "21               0              0  \n",
       "22               3            156  \n",
       "23               7            376  \n",
       "24               2            100  \n",
       "25               0              0  \n",
       "26               3            161  \n",
       "27              15            784  \n",
       "28               4            212  \n",
       "29               1             50  \n",
       "30               2            106  \n",
       "31              17            912  \n",
       "32              11            558  \n",
       "33               0              0  \n",
       "34               0              0  \n",
       "35               5            266  \n",
       "36               3            153  \n",
       "37              11            558  \n",
       "38              17            919  \n",
       "39               0              0  \n",
       "40               7            388  \n",
       "41               0              0  \n",
       "42               2            108  \n",
       "43              18            927  \n",
       "44               7            379  \n",
       "45              16            843  \n",
       "46               5            255  \n",
       "47               9            485  \n",
       "48               2            105  \n",
       "49               8            414  "
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = calculate_levenshtein_ratios(data_sample)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of test data items processed during GA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items processed in GA Attack:  250\n",
      "No words were removed from original text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data items processed in GA Attack: \", len(data_sample))\n",
    "if deleted_words_from_text(data_sample):\n",
    "    print(\"Some words were removed from original text!\")\n",
    "else:\n",
    "    print(\"No words were removed from original text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for which reviews we successfully flipped the sentiment and set ```ga_flipped_sentiment``` == 'Y' if we were successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of reviews where sentiment was changed after attack: 53.2 % changed sentiment. i.e. 133  out of  250\n",
      "Percentage of reviews failed to change sentiment:  46.8 % did not change, i.e. 117  out of  250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>1</td>\n",
       "      <td>well i must say this is probably the meanest f...</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not saying somethings newer when i say tha...</td>\n",
       "      <td>0.426562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>1</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.637239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978884</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>caught this flick as anyone of a fivefor addre...</td>\n",
       "      <td>0.294813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             1  well i must say this is probably the meanest f...  0.768538   \n",
       "1             0  if christopher nolan had made memento before f...  0.395876   \n",
       "2             0  i'm not saying somethings newer when i say tha...  0.426562   \n",
       "3             1  i have been watching lost with my family since...  0.637239   \n",
       "4             0  caught this flick as anyone of a fivefor addre...  0.294813   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0               0      0.895522                    Y           0.129032   \n",
       "1               0      0.966031                    Y           0.032895   \n",
       "2               0      0.958418                    Y           0.053476   \n",
       "3               0      0.978884                    Y           0.013514   \n",
       "4               0      0.836257                    Y           0.189542   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               5            264  \n",
       "1               2            107  \n",
       "2               2            106  \n",
       "3               1             53  \n",
       "4               5            256  "
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = set_labels_changed(data_sample)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count how many words were changed for each review, average number of word changed and percentage of change to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. num of words changed changes made:  35\n",
      "Avg. percentage modified:  18.0 %\n"
     ]
    }
   ],
   "source": [
    "data_sample = set_percentage_modified(data_sample)\n",
    "\n",
    "#save_all_results(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "      <th>ga_generations</th>\n",
       "      <th>ga_time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>well i must say this is probably the worst fil...</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>1</td>\n",
       "      <td>well i must say this is probably the meanest f...</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>20</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>0</td>\n",
       "      <td>if christopher nolan had made memento before f...</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>10</td>\n",
       "      <td>0.966031</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm not saying anything new when i say that ra...</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not saying somethings newer when i say tha...</td>\n",
       "      <td>0.426562</td>\n",
       "      <td>10</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>1</td>\n",
       "      <td>i have been watching lost with my family since...</td>\n",
       "      <td>0.637239</td>\n",
       "      <td>5</td>\n",
       "      <td>0.978884</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>caught this flick as one of a fivefor deal fro...</td>\n",
       "      <td>0.942861</td>\n",
       "      <td>0</td>\n",
       "      <td>caught this flick as anyone of a fivefor addre...</td>\n",
       "      <td>0.294813</td>\n",
       "      <td>29</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          0  well i must say this is probably the worst fil...  0.025771   \n",
       "1          1  if christopher nolan had made memento before f...  0.769358   \n",
       "2          1  i'm not saying anything new when i say that ra...  0.807952   \n",
       "3          0  i have been watching lost with my family since...  0.476023   \n",
       "4          1  caught this flick as one of a fivefor deal fro...  0.942861   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             1  well i must say this is probably the meanest f...  0.768538   \n",
       "1             0  if christopher nolan had made memento before f...  0.395876   \n",
       "2             0  i'm not saying somethings newer when i say tha...  0.426562   \n",
       "3             1  i have been watching lost with my family since...  0.637239   \n",
       "4             0  caught this flick as anyone of a fivefor addre...  0.294813   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \\\n",
       "0              20      0.895522                    Y               0.13   \n",
       "1              10      0.966031                    Y               0.03   \n",
       "2              10      0.958418                    Y               0.05   \n",
       "3               5      0.978884                    Y               0.01   \n",
       "4              29      0.836257                    Y               0.19   \n",
       "\n",
       "   ga_generations  ga_time_taken  \n",
       "0               5            264  \n",
       "1               2            107  \n",
       "2               2            106  \n",
       "3               1             53  \n",
       "4               5            256  "
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the original raw data-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_filename = \"raw_data.csv\"\n",
    "raw_data_sample, normalized_dataset = load_raw_dataset(raw_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a delightful movie that is so over-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>After, I watched the films... I thought, \"Why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This short deals with a severely critical writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Just Cause is one of those films that at first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>For reasons I cannot begin to fathom, Dr. Lore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  This is a delightful movie that is so over-the...\n",
       "1          1  After, I watched the films... I thought, \"Why ...\n",
       "2          1  This short deals with a severely critical writ...\n",
       "3          0  Just Cause is one of those films that at first...\n",
       "4          0  For reasons I cannot begin to fathom, Dr. Lore..."
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check normalised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this is a delightful movie that is so overthet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>after  i watched the films  i thought  why the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this short deals with a severely critical writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>just cause is one of those films that at first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>for reasons i cannot begin to fathom  dr  lore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  this is a delightful movie that is so overthet...\n",
       "1          1  after  i watched the films  i thought  why the...\n",
       "2          1  this short deals with a severely critical writ...\n",
       "3          0  just cause is one of those films that at first...\n",
       "4          0  for reasons i cannot begin to fathom  dr  lore..."
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add to a list all the data_sample indexes that match against the raw datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_values_rawdata = matching_indexes(data_sample,normalized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "[7518, 5469, 24681, 19657, 3451, 12683, 42440, 48593, 22262, 38540, 23569, 38071, 39020, 17336, 26976, 2971, 41471, 10867, 25520, 27092, 20222, 23402, 35235, 35568, 49588, 43431, 41119, 11393, 47316, 26972, 42200, 10901, 47556, 3759, 32752, 27922, 46113, 38250, 520, 11872, 40810, 45353, 23513, 47064, 30419, 10751, 4925, 41951, 49741, 49471, 31442, 38159, 16625, 35122, 44736, 34784, 5538, 15156, 18507, 11760, 37612, 1725, 15972, 25224, 27037, 48103, 40122, 31445, 10905, 22306, 21492, 20522, 317, 1183, 45265, 38722, 45978, 3894, 44302, 12837, 23286, 4329, 13434, 10156, 30498, 5220, 26432, 36268, 13892, 39916, 6016, 2705, 49736, 9521, 21482, 1809, 20671, 34878, 40580, 33552, 43724, 28638, 4914, 24925, 38793, 29080, 32453, 32355, 8575, 2114, 36654, 21815, 10882, 35686, 46175, 34015, 6007, 25764, 22690, 45688, 5107, 37101, 41444, 19133, 19064, 42743, 42887, 3590, 30281, 46248, 16450, 27991, 47403, 4369, 6883, 43642, 47768, 42016, 16418, 13705, 47218, 31850, 18699, 42063, 25124, 29525, 44184, 5422, 17666, 44660, 3572, 14314, 13028, 48914, 9784, 4307, 48401, 20460, 2629, 21769, 26337, 10547, 35272, 30543, 37916, 32756, 3811, 4802, 29653, 18525, 15962, 21456, 37625, 7411, 46436, 14928, 5060, 36128, 38864, 4262, 24741, 30129, 22722, 25916, 22851, 16740, 37731, 11203, 11142, 5, 11052, 47621, 10151, 27878, 45796, 41538, 45271, 30755, 10961, 309, 20519]\n"
     ]
    }
   ],
   "source": [
    "print(len(index_values_rawdata))\n",
    "print(list(index_values_rawdata[:201])) # load the first 200 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yikes did this movie blow. The characters were weak, the plot weaker. I figured this couldn't be too bad because it has Christoper Walken, oops. He must have done this because he was bored and needed the money. The characters were supposed to be Irish but noone had an Irish accent. I am desperately trying to find something nice about this, I can't except Walken did a fine job with a wooden character. Find something to read, or watch discovery, don't ever see this movie.\n",
      "\n",
      "\n",
      "yikes did this movie blow  the characters were weak  the plot weaker  i figured this couldn't be too bad because it has christoper walken  oops  he must have done this because he was bored and needed the money  the characters were supposed to be irish but noone had an irish accent  i am desperately trying to find something nice about this  i can't except walken did a fine job with a wooden character  find something to read  or watch discovery  don't ever see this movie \n",
      "\n",
      "\n",
      "yikes did this filmmaking detonate the hallmarks were frail the plot lowest i evokes this couldn't be too amiss because it has christoper pesci oops he should have effected this because he was bored and required the moneys the nature were suspected to be northerners but someone had an dublin concentrate i am cruelly try to uncovered something nice about this i can't exclude pesci did a splendid job with a timber character found something to readings or observing uncovering don't ever see this movie\n",
      "\n",
      "BEFORE:\t\t 0.0070145814679563046\n",
      "AFTER:\t\t 0.5173801779747009\n",
      "PERCENT CHANGE:\t 34.0 %\n",
      "WORDS CHANGED:\t 29\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_sample.at[5469,'text'])\n",
    "print(\"\\n\")\n",
    "print(normalized_dataset.at[5469,'text'])\n",
    "print(\"\\n\")\n",
    "print(data_sample.at[1,'ga_text'])\n",
    "\n",
    "\n",
    "print(\"\\nBEFORE:\\t\\t\",data_sample.at[1,'probs'])\n",
    "print(\"AFTER:\\t\\t\",data_sample.at[1,'ga_probs'])\n",
    "print(\"PERCENT CHANGE:\\t\",round(data_sample.at[1,'ga_percent_change'] * 100), \"%\")\n",
    "print(\"WORDS CHANGED:\\t\",data_sample.at[1,'ga_num_changes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_sample.at[2,'text'].split())\n",
    "len(data_sample_split_words)\n",
    "len(raw_split_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  worst  AFTER:  best\n",
      "BEFORE:  worst  AFTER:  best\n",
      "BEFORE:  watch  AFTER:  remember\n",
      "BEFORE:  gratuitous  AFTER:  more\n",
      "BEFORE:  story  AFTER:  last\n",
      "BEFORE:  line  AFTER:  bit\n",
      "BEFORE:  lacklustre  AFTER:  stupid\n",
      "BEFORE:  love  AFTER:  passion\n"
     ]
    }
   ],
   "source": [
    "raw_split_words = normalized_dataset.at[24681,'text'].split()\n",
    "data_sample_split_words = data_sample.at[2,'ga_text'].split()\n",
    "\n",
    "len_range = len(data_sample_split_words)\n",
    "\n",
    "for i in range(len_range):\n",
    "    if raw_split_words[i] != data_sample_split_words[i]:\n",
    "        print(\"BEFORE: \", raw_split_words[i], \" AFTER: \",data_sample_split_words[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best', 'better', 'worst', 'greatest', 'strongest']"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentence = \"be one of the [MASK]  if not the worst \"\n",
    "word_list = return_masked_words(mask_word_pipeline,masked_sentence)\n",
    "\n",
    "masked_words = []\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i]['token_str'].isalpha():\n",
    "        masked_words.append(word_list[i]['token_str'])\n",
    "masked_words\n",
    "# word_list[0]['token_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ['best', 'worst', 'better', 'greatest', 'first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 234\n"
     ]
    }
   ],
   "source": [
    "t = data_sample.at[0,'text'].split()\n",
    "indx_val = randrange(2,(len(t) - 3))\n",
    "print(indx_val,len(t))\n",
    "join_list = [t[indx_val  - 2],t[indx_val  - 1],'[MASK]',t[indx_val  + 1], t[indx_val  + 2] ]\n",
    "o_join_list = [t[indx_val  - 2],t[indx_val  - 1],t[indx_val],t[indx_val  + 1], t[indx_val  + 2] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working full force caine is\n",
      "working full [MASK] caine is\n"
     ]
    }
   ],
   "source": [
    "masked_word_to_check = ' '.join(join_list )\n",
    "o_sent = ' '.join(o_join_list )\n",
    "print(o_sent)\n",
    "print(masked_word_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': '[CLS] working full time caine is [SEP]', 'score': 0.9468444585800171, 'token': 2051, 'token_str': 'time'}, {'sequence': '[CLS] working full on caine is [SEP]', 'score': 0.013376843184232712, 'token': 2006, 'token_str': 'on'}, {'sequence': '[CLS] working full, caine is [SEP]', 'score': 0.003743428271263838, 'token': 1010, 'token_str': ','}, {'sequence': '[CLS] working full circle caine is [SEP]', 'score': 0.0018968889489769936, 'token': 4418, 'token_str': 'circle'}, {'sequence': '[CLS] working full as caine is [SEP]', 'score': 0.0017040419625118375, 'token': 2004, 'token_str': 'as'}]\n"
     ]
    }
   ],
   "source": [
    "possible_substitutions = return_masked_words(mask_word_pipeline, masked_word_to_check)\n",
    "print(possible_substitutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] working full time caine is [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(possible_substitutions[0]['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
