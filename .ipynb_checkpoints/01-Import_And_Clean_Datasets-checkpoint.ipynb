{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset is stored in the ```dataset``` folder \n",
    "downloaded from ```https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz```\n",
    "### The GloVe vector embeddings are in the ```glove``` folder \n",
    "downloaded from ```https://github.com/stanfordnlp/GloVe```\n",
    "### The Counter-Fitted vectors are in the ```counter-fitted``` folder\n",
    "download from ```https://github.com/nmrksic/counter-fitting/blob/master/word_vectors/counter-fitted-vectors.txt.zip```\n",
    "(see referenced paper [20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependecies \n",
    "### Start by importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python 3.8.5\n",
    "Tensorflow 2.3.1\n",
    "Keras 2.4.3\n",
    "conda 4.9.0\n",
    "wordcloud 1.8.0\n",
    "\"\"\"\n",
    "import configparser\n",
    "import math\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, Embedding\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import string # use this to remove punctuation from tweets e.g. string.punctuation\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Which dataset to use:\n",
    "1 = IMDB\n",
    "2 = Sentiment140\n",
    "\"\"\"\n",
    "\n",
    "DATASET = 2\n",
    "\n",
    "\n",
    "\n",
    "############ SENTIMENT MODEL TRAINING PARAMETERS ################################\n",
    "DIMENSION = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "############ DATA SPLIT PERCENTAGE ##############################################\n",
    "TRAINING_SPLIT = 0.80\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "############ DATAFILE IS DOWNLOADED (for fresh download set this to False) ######\n",
    "DATASET_DOWNLOADED = False\n",
    "LOAD_FROM_FILE = True\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "############ NEED TO LIMIT THIS TO 50000 TO ENABLE BUILDING OF DISTANCE MATRIX ##\n",
    "USE_MAX_VOCABULARY_SIZE = False \n",
    "MAX_VOCABULARY_SIZE = 50000\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and set global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config/main.conf')\n",
    "\n",
    "MODEL_VERSION =  \"0001\"\n",
    "\n",
    "if DATASET == 1:\n",
    "    set_dataset = \"imdb\"\n",
    "if DATASET == 2:\n",
    "    set_dataset = \"s140\"\n",
    "\n",
    "DATASET_URL = (config[set_dataset]['DATASET_URL'])\n",
    "\n",
    "DATASET_FOLDER = config[set_dataset]['DATASET_FOLDER']\n",
    "DATASET_TAR_FILE_NAME = config[set_dataset]['DATASET_TAR_FILE_NAME']\n",
    "DATASET_NAME = config[set_dataset]['DATASET_NAME']\n",
    "\n",
    "MODEL_NAME = config[set_dataset]['MODEL_NAME']\n",
    "\n",
    "CLEAN_DATA_FILE = os.path.join(DATASET_FOLDER,\"normalized_dataset.csv\")\n",
    "TAR_FILE_PATH = os.path.join(DATASET_FOLDER,DATASET_TAR_FILE_NAME)\n",
    "DATA_SET_LOCATION = os.path.join(DATASET_FOLDER,DATASET_NAME)\n",
    "\n",
    "MODEL_SAVE_DIRECTORY = config[set_dataset]['MODEL_SAVE_DIRECTORY']\n",
    "\n",
    "GLOVE_EMBEDDINGS = config[set_dataset]['GLOVE_EMBEDDINGS']\n",
    "\n",
    "GLOVE_EMBEDDINGS_MATRIX = config[set_dataset]['GLOVE_EMBEDDINGS_MATRIX']\n",
    "COUNTER_FITTED_EMBEDDINGS_MATRIX = config[set_dataset]['COUNTER_FITTED_EMBEDDINGS_MATRIX']\n",
    "\n",
    "\n",
    "####### files required to reconstruct the final trained model ##############################\n",
    "MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "ASSESTS_FOLDER = os.path.join(MODEL_PATH,\"assets\")\n",
    "MODEL_ASSETS_VOCABULARY_FILE = os.path.join(ASSESTS_FOLDER,\"vocab\")\n",
    "MODEL_ASSETS_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"imdb_glove_embeddings_matrix\")\n",
    "MODEL_BEST_WEIGHTS = os.path.join(ASSESTS_FOLDER, \"cp.ckpt\")\n",
    "MODEL_TRAINING_HISORTY_FILE = os.path.join(ASSESTS_FOLDER, \"training_history.csv\")\n",
    "#print(MODEL_BEST_WEIGHTS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_number = []\n",
    "\n",
    "if os.path.exists(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):\n",
    "    \n",
    "    for entry in os.listdir(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):\n",
    "        version_number.append(entry)\n",
    "        \n",
    "    # increment version number if we have a previous model already saved\n",
    "    if len(version_number):\n",
    "        MODEL_VERSION = ((str)(int(version_number[-1]) + 1)).zfill(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model save directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(MODEL_SAVE_DIRECTORY):\n",
    "    os.makedirs(MODEL_SAVE_DIRECTORY)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset and extract contents to correct directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to download a fresh copy of the data set,\n",
      "make sure the variable DATASET_DOWNLOADED is set to False\n"
     ]
    }
   ],
   "source": [
    "from manny_train import file_download_extraction as fde\n",
    "\n",
    "\n",
    "from_folder =\"\"\n",
    "to_folder = DATA_SET_LOCATION \n",
    "\n",
    "if DATASET_DOWNLOADED:\n",
    "    print(\"If you want to download a fresh copy of the data set,\")\n",
    "    print(\"make sure the variable DATASET_DOWNLOADED is set to False\")\n",
    "elif DATASET == 1:\n",
    "    DATASET_DOWNLOADED = True\n",
    "    fde.download(DATASET_URL, dest_folder=DATASET_FOLDER)\n",
    "    fde.extract_tar_file(TAR_FILE_PATH, DATASET_FOLDER)\n",
    "    directories_in_dataset_folder = fde.get_directory_name(DATASET_FOLDER)\n",
    "\n",
    "    if len(directories_in_dataset_folder) == 1:\n",
    "        from_folder=DATASET_FOLDER+\"/\"+directories_in_dataset_folder[0]\n",
    "        fde.rename_folder(from_folder, to_folder)\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"ERROR!\")\n",
    "        print(DATASET_FOLDER,\" folder has too many sub directories!\")\n",
    "        print(\"Make sure \", DATASET_FOLDER, \" is empty before downloading fresh dataset file\")\n",
    "elif DATASET == 2:\n",
    "    DATASET_DOWNLOADED = True\n",
    "    fde.download(DATASET_URL, dest_folder=DATASET_FOLDER)\n",
    "    fde.extract_zip_file(TAR_FILE_PATH, DATASET_FOLDER)\n",
    "    fde.rename_s140_file(DATASET_FOLDER)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load dataset:\n",
    "## We have 2 options: \n",
    "#### 1) Load data from a fresh download, or\n",
    "#### 2) Load a previously saved dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load data from a fresh download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset, please wait ...\n",
      "Normalizing dataset, please wait...\n",
      "Dataset created!\n",
      "\n",
      "Saved dataset to:  s140_dataset/normalized_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1427435</th>\n",
       "      <td>1</td>\n",
       "      <td>i wish  or i would be buying or lol  you'd ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101080</th>\n",
       "      <td>1</td>\n",
       "      <td>i hate you right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085676</th>\n",
       "      <td>1</td>\n",
       "      <td>up to no good as always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025165</th>\n",
       "      <td>1</td>\n",
       "      <td>means so much chris  thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760122</th>\n",
       "      <td>0</td>\n",
       "      <td>winter classes early empire  pop culture  peop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "1427435          1   i wish  or i would be buying or lol  you'd ma...\n",
       "1101080          1                               i hate you right now\n",
       "1085676          1                            up to no good as always\n",
       "1025165          1                       means so much chris  thanks \n",
       "760122           0  winter classes early empire  pop culture  peop..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from manny_train import data_loading as process\n",
    "\n",
    "data_frame = pd.DataFrame\n",
    "if DATASET == 1:\n",
    "    data_frame  = process.process_dataset_IMDB(DATA_SET_LOCATION)\n",
    "elif DATASET == 2:\n",
    "    data_frame = process.process_dataset_Sentiment140(os.path.join(DATASET_FOLDER,'train.csv'))\n",
    "else:\n",
    "    print(\"Please specify which dataset you want to work with...\")\n",
    "\n",
    "# save the cleaned dataset\n",
    "\n",
    "data_frame.to_csv(CLEAN_DATA_FILE, index = False)\n",
    "print(\"Saved dataset to: \", CLEAN_DATA_FILE)\n",
    "    \n",
    "# randomise dataframe\n",
    "data_frame = data_frame.sample(frac = 1, random_state = 7) \n",
    "\n",
    "# show the first few rows of the datasaet\n",
    "data_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load a previously saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the clean dataset from .csv file\n",
    "dtypes = {'sentiment': 'int', 'text': 'str'}\n",
    "data_frame = pd.read_csv(CLEAN_DATA_FILE,dtype=dtypes) \n",
    "\n",
    "#drop and Nan rows\n",
    "data_frame = data_frame.dropna() \n",
    "\n",
    "# randomise dataframe\n",
    "data_frame = data_frame.sample(frac = 1, random_state = 7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test, then convert each to a Keras Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split into test and train datsets\n",
    "train_data_raw, test_data_raw = train_test_split(data_frame, test_size= (1 - TRAINING_SPLIT), random_state = 7)\n",
    "\n",
    "\n",
    "# convert to keras Dataset (TensorSliceDataset) \n",
    "train_data = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(train_data_raw['text'].values, tf.string),\n",
    "            tf.cast(train_data_raw['sentiment'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(test_data_raw['text'].values, tf.string),\n",
    "            tf.cast(test_data_raw['sentiment'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# for creating vocabulary, we just need the string, so discard the sentiment value \n",
    "text_dataset =  train_data.map(lambda string_text, sentiment: string_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "#### We see that we have 2 types of sentiments, ```0 = negative``` and ```1 = positive```. We have 25,000 data points for each type of sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sentiment Distribution')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_count = data_frame.sentiment.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(val_count.index, val_count.values)\n",
    "plt.xticks(val_count.index)\n",
    "plt.title(\"Sentiment Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset: this is the pre-processing callback function used by ```TextVectorization```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud of ```positive``` sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add aditional stopwords\n",
    "STOPWORDS.add(\"film\")\n",
    "STOPWORDS.add(\"movie\")\n",
    "STOPWORDS.add(\"one\")\n",
    "STOPWORDS.add(\"see\")\n",
    "STOPWORDS.add(\"make\")\n",
    "STOPWORDS.add(\"time\")\n",
    "STOPWORDS.add(\"even\")\n",
    "STOPWORDS.add(\"really\")\n",
    "STOPWORDS.add(\"character\")\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(stopwords = STOPWORDS, width = 1600 , height = 800).generate(\" \".join(data_frame[data_frame.sentiment == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud of ```negative``` sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(stopwords = STOPWORDS, width = 1600 , height = 800).generate(\" \".join(data_frame[data_frame.sentiment == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='normlize_data')\n",
    "def normlize_data(text):\n",
    "    \n",
    "    # define regex to use for replacements\n",
    "    html_regex = '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});' # remove all html tags\n",
    "    url_regex = 'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*'\n",
    "    twitter_username_regex = \"@\\w+\" # twitter username\n",
    "    html_amp_start_regex = \"&\\w+\"  # remove html entities starting with a &\n",
    "    smiley_regex = \"[8:=;]['`\\-]?[)dD]+|[)dD]+['`\\-]?[8:=;]\"\n",
    "    lol_face_regex = \"[8:=;]['`\\-]?p+\"\n",
    "    sad_face_regex = \"[8:=;]['`\\-]?\\(+|\\)+['`\\-]?[8:=;]\"\n",
    "    neutral_face_regex = \"[8:=;]['`\\-]?[\\/|l*]\"\n",
    "    forward_slash_regex = \"/\" # add space around forward slash\n",
    "    heart_regex = \"<3\" # heart emoji\n",
    "    numbers_regex = \"[-+]?[.\\d]*[\\d]+[:,.\\d]*\" # remove numbers with signs e.g. -2, +3 etc..\n",
    "    hash_tag_regex = \"#\\w+\"\n",
    "    repeated_punctuation_regex = \"([!?.]){2,}\" # e.g. replace ?????? to ?\n",
    "    remove_numbers_regex = '[0-9]+' # remove all numbers\n",
    " \n",
    "    # remove punctuation\n",
    "    punctuation = string.punctuation\n",
    "    punctuation = punctuation.translate({ord(i):None for i in \"'\"}) # keep the apostrophe, but remove all other punctuation\n",
    "    remove_punctuation = f'[{re.escape(punctuation)}]'\n",
    "    \n",
    "    result = tf.strings.lower(text)\n",
    "    result = tf.strings.strip(result) # remove leading and trailing spaces\n",
    "     \n",
    "    # make sure to remove html tags second\n",
    "    result = tf.strings.regex_replace(result, html_regex, '')\n",
    "     \n",
    "    # remove any URLs\n",
    "    result = tf.strings.regex_replace(result, url_regex, ' ') # url \n",
    "    result = tf.strings.regex_replace(result, twitter_username_regex, ' ') # twitetr user names \n",
    "    result = tf.strings.regex_replace(result, html_amp_start_regex, ' ') # any html entity that starts with an &\n",
    "    result = tf.strings.regex_replace(result, smiley_regex, ' ') # remove any smilies/emojis in the text\n",
    "    result = tf.strings.regex_replace(result, lol_face_regex, ' ') # lolface emoji\n",
    "    result = tf.strings.regex_replace(result, sad_face_regex, ' ') # sad face emoji\n",
    "    result = tf.strings.regex_replace(result, neutral_face_regex, ' ') # face emoji\n",
    "    result = tf.strings.regex_replace(result, forward_slash_regex, r' / ') # add space around forward slash\n",
    "    result = tf.strings.regex_replace(result, heart_regex, ' ') # remove heart emoji\n",
    "    result = tf.strings.regex_replace(result, numbers_regex, ' ') # remove numbers e.g. -3, 2, 8, +8 etc..\n",
    "    result = tf.strings.regex_replace(result, hash_tag_regex, ' ') # remove hashtags\n",
    "    result = tf.strings.regex_replace(result, repeated_punctuation_regex, r'\\1 ') # replace any repeated puctuation with single occurance\n",
    "    result = tf.strings.regex_replace(result, remove_numbers_regex, ' ') # finally remove all numbers\n",
    "    \n",
    "    # then remove punctuation at the very end\n",
    "    result = tf.strings.regex_replace(result, remove_punctuation, '')\n",
    "    \n",
    "    #result = tf.strings.strip(result)\n",
    "    \n",
    "    # print(\"Done!\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\t 40000 \t\t% datapoints in train dataset:\t 80 %\n",
      "Test Data:\t 10000 \t\t% datapoints in test dataset:\t 20 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data:\\t\",len(train_data), \"\\t\\t% datapoints in train dataset:\\t\", math.ceil(TRAINING_SPLIT * 100),\"%\")\n",
    "print(\"Test Data:\\t\",len(test_data), \"\\t\\t% datapoints in test dataset:\\t\", math.ceil((1 - TRAINING_SPLIT) * 100) ,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the training and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points:  40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=string, numpy=b\"although the recent retelling of part of homer's epic troy with brad pitt was entertaining once  iphigenia with the incandescent irene pappas is breathtaking  unfolding in a natural setting with greek actors speaking their own language lends such authenticity  a chance encounter with this film on one of directv's many movie channels kept me interested in spite of my concentration problems  there is no glitter or bling in this movie  just a fabulously rich story impeccably told by actors so real one feels they are eavesdropping on a real family in turmoil  i think even homer  if he really existed  would be proud of this telling jlh\">,\n",
       "  <tf.Tensor: shape=(), dtype=int64, numpy=1>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of data points: \",len(train_data))\n",
    "[t for t in train_data.take(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points:  10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=string, numpy=b\"this is a gem of a movie not just for people who like fun and quirky premises  but who love the history and traditions of scifi and classic hollywood movies  each alien of the martian crew is the embodiment of a classic scifi character or member of hollywood royalty and it's pure pleasure watching them bounce of each other and the residents of big bean \">,\n",
       "  <tf.Tensor: shape=(), dtype=int64, numpy=1>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of data points: \",len(test_data))\n",
    "[t for t in test_data.take(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of training data\n",
    "- We will use the new ```TextVectorization``` Keras class to tokenize our training data\n",
    "- ```max_tokens``` i.e. limit vocabulary to this size \n",
    "- ```output_sequence_length``` i.e. limit length of text sequence\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tokenization-manning.png\">\n",
    "</center>\n",
    "  \n",
    "<center><strong>Francois Chollet. 2017. Deep Learning with Python (1st. ed.). Manning Publications Co., USA.</strong></center>\n",
    "    \n",
    "\n",
    "\n",
    "#### We will use ```TextVectorization``` to index the vocabulary found in the dataset. Later, we'll use the same layer instance to vectorize the samples.\n",
    "\n",
    "#### 1) Our layer will only consider the top $ 50,000 $ words\n",
    "#### 2) we will truncate or pad sequences to be actually $ 300 $ tokens long\n",
    "#### 3) we will pre-process our text data by defining a callback function: ```normalize_data```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    standardize=normlize_data, \n",
    "    max_tokens=50000, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=300)\n",
    "\n",
    "# build vocabulary, will also run the normalize_data() function \n",
    "vectorizer.adapt(text_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary, mapping words to their indices\n",
    "#### Store vocabulary in a list ```vocab```, and reverse lookup in a dictionary ```word_index```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab: 50000, length of word_index:  50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "print(\"length of vocab: %d, length of word_index:  %d\" % (len(vocab),len(word_index)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup word using the vocabulary list and word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of index_of_word is:  17\n",
      "Word in vocab at index: 17  is: movie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 1241, 1699, 20, 2, 11603]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_of_word = word_index['movie']\n",
    "print(\"value of index_of_word is: \",index_of_word)\n",
    "\n",
    "word_in_vocab = vocab[index_of_word]\n",
    "print(\"Word in vocab at index:\",index_of_word, \" is:\", word_in_vocab)\n",
    "\n",
    "\n",
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the top 5 most frquent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5] # index 0 reserved for padding token and index 1 is reserved for out of vocabulary (UNK) words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example vectorized sentence:\n",
    "#### the word ```ghjghj``` is out of vocaabulary (oov) and hence coded as 1 i.e. 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 6132, 4934,  127,    2, 1815,    1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer([['the cow jumped over the moon ghjghj']]).numpy()[0, :7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained word GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dict mapping words (strings) to their NumPy vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings model to dictionary, please wait...\n",
      "Finished!\n",
      "Found 2196007 word vectors in model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index= {}\n",
    "print(\"Loading embeddings model to dictionary, please wait...\")\n",
    "with open(GLOVE_EMBEDDINGS, 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Finished!\")\n",
    "print('Found %s word vectors in model.' %len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now prepare a corresponding embedding matrix that we can use in a Keras ```Embedding``` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 41834 words (8166 not found)\n",
      "Embedding matrix shape:  (50001, 300)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "words_found = 0\n",
    "words_not_found = 0\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "count = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word) \n",
    "    if embedding_vector is not None:\n",
    "        \n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        words_found += 1\n",
    "    else:\n",
    "        words_not_found += 1\n",
    "print(\"Converted %d words (%d not found)\" % (words_found, words_not_found))\n",
    "print(\"Embedding matrix shape: \",  (embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our model\n",
    "#### Load our pre-trained GloVe embeddings into an ```Embedding``` layer (set ```training=False``` so to keep the embeddings fixed i.e. we do NOT want them to update during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "def create_model(vectorizer, embedding_matrix, vocab):\n",
    "    \n",
    "    num_tokens = len(vocab) + 1\n",
    "    embedding_dim = 300\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # activation changed from tanh to relu\n",
    "    # make sure to add the shape parameter (the length of the sequences), otherwise the loss and val_loss wont be calculated and will show as NaN\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorizer(inputs)\n",
    "    embedding_sequences = embedding_layer(x)\n",
    "\n",
    "    x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    x = Bidirectional(LSTM(128, dropout=0.2))(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)  # set dropout rate to 0.5 to prevent over-fitting\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model =  keras.Model(inputs, predictions)\n",
    "\n",
    "    # default learning rate is 0.001 for Adam optimizer\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=Adam(LEARNING_RATE), metrics=[\"acc\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_model():\n",
    "    \n",
    "#     model = tf.keras.models.Sequential()\n",
    "#     model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "#     model.add(vectorizer) # use vovaculary as layer in model\n",
    "#     model.add(tf.keras.layers.Embedding(num_tokens,\n",
    "#                                         embedding_dim,\n",
    "#                                         embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#                                         trainable=False, # we don't want weights to update during training\n",
    "#                                         mask_zero=True))\n",
    "    \n",
    "#     model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "#     model.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Bidirectional(LSTM(128, dropout=0.2)))\n",
    "#     model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dropout(0.5)) # set dropout rate to 0.5 to prevent over-fitting\n",
    "#     model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "#     # classification layer\n",
    "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "   \n",
    "\n",
    "#     # default learning rate is 0.001 for Adam optimizer, reduced to 1e-4 here\n",
    "#     model.compile(\n",
    "#         loss=\"binary_crossentropy\", optimizer=Adam(LEARNING_RATE), metrics=[\"acc\"]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model, then show a summary of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          15000300  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 296, 64)           96064     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 15,688,749\n",
      "Trainable params: 688,449\n",
      "Non-trainable params: 15,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vectorizer, embedding_matrix, vocab)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert our list of strings data to a NumPy arrays of integer indices (arrays are right padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use ```binary_crossentropy``` since we have only two possible classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n",
      "Epoch 1/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.5487 - acc: 0.7172\n",
      "Epoch 00001: val_acc improved from -inf to 0.82770, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 66ms/step - loss: 0.5487 - acc: 0.7172 - val_loss: 0.4119 - val_acc: 0.8277\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4335 - acc: 0.8077\n",
      "Epoch 00002: val_acc improved from 0.82770 to 0.83920, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.4335 - acc: 0.8077 - val_loss: 0.3641 - val_acc: 0.8392\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3923 - acc: 0.8290\n",
      "Epoch 00003: val_acc improved from 0.83920 to 0.85880, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.3923 - acc: 0.8290 - val_loss: 0.3344 - val_acc: 0.8588\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3688 - acc: 0.8413\n",
      "Epoch 00004: val_acc improved from 0.85880 to 0.86570, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.3688 - acc: 0.8413 - val_loss: 0.3258 - val_acc: 0.8657\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3486 - acc: 0.8514\n",
      "Epoch 00005: val_acc improved from 0.86570 to 0.87320, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.3486 - acc: 0.8514 - val_loss: 0.3084 - val_acc: 0.8732\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3287 - acc: 0.8606\n",
      "Epoch 00006: val_acc improved from 0.87320 to 0.87730, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.3287 - acc: 0.8606 - val_loss: 0.3007 - val_acc: 0.8773\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3143 - acc: 0.8691\n",
      "Epoch 00007: val_acc improved from 0.87730 to 0.88410, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.3143 - acc: 0.8691 - val_loss: 0.2885 - val_acc: 0.8841\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3018 - acc: 0.8749\n",
      "Epoch 00008: val_acc improved from 0.88410 to 0.88710, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.3018 - acc: 0.8749 - val_loss: 0.2822 - val_acc: 0.8871\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2859 - acc: 0.8809\n",
      "Epoch 00009: val_acc did not improve from 0.88710\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2859 - acc: 0.8809 - val_loss: 0.2771 - val_acc: 0.8859\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2801 - acc: 0.8842\n",
      "Epoch 00010: val_acc improved from 0.88710 to 0.88860, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2801 - acc: 0.8842 - val_loss: 0.2704 - val_acc: 0.8886\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2713 - acc: 0.8885\n",
      "Epoch 00011: val_acc improved from 0.88860 to 0.89080, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2713 - acc: 0.8885 - val_loss: 0.2660 - val_acc: 0.8908\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2659 - acc: 0.8913\n",
      "Epoch 00012: val_acc improved from 0.89080 to 0.89230, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.2659 - acc: 0.8913 - val_loss: 0.2676 - val_acc: 0.8923\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2570 - acc: 0.8951\n",
      "Epoch 00013: val_acc did not improve from 0.89230\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2570 - acc: 0.8951 - val_loss: 0.2773 - val_acc: 0.8856\n",
      "Epoch 14/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2495 - acc: 0.8990\n",
      "Epoch 00014: val_acc improved from 0.89230 to 0.89440, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.2495 - acc: 0.8990 - val_loss: 0.2617 - val_acc: 0.8944\n",
      "Epoch 15/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2429 - acc: 0.9035\n",
      "Epoch 00015: val_acc improved from 0.89440 to 0.89460, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.2429 - acc: 0.9035 - val_loss: 0.2573 - val_acc: 0.8946\n",
      "Epoch 16/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2387 - acc: 0.9054\n",
      "Epoch 00016: val_acc improved from 0.89460 to 0.89620, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.2387 - acc: 0.9054 - val_loss: 0.2550 - val_acc: 0.8962\n",
      "Epoch 17/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.9085\n",
      "Epoch 00017: val_acc did not improve from 0.89620\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2300 - acc: 0.9085 - val_loss: 0.2622 - val_acc: 0.8938\n",
      "Epoch 18/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2272 - acc: 0.9093\n",
      "Epoch 00018: val_acc improved from 0.89620 to 0.89740, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 65ms/step - loss: 0.2272 - acc: 0.9093 - val_loss: 0.2547 - val_acc: 0.8974\n",
      "Epoch 19/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2182 - acc: 0.9126\n",
      "Epoch 00019: val_acc did not improve from 0.89740\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2182 - acc: 0.9126 - val_loss: 0.2561 - val_acc: 0.8965\n",
      "Epoch 20/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2133 - acc: 0.9166\n",
      "Epoch 00020: val_acc did not improve from 0.89740\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2133 - acc: 0.9166 - val_loss: 0.2540 - val_acc: 0.8973\n",
      "Epoch 21/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2106 - acc: 0.9162\n",
      "Epoch 00021: val_acc did not improve from 0.89740\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2106 - acc: 0.9162 - val_loss: 0.2556 - val_acc: 0.8944\n",
      "Epoch 22/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2084 - acc: 0.9181\n",
      "Epoch 00022: val_acc improved from 0.89740 to 0.89770, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.2084 - acc: 0.9181 - val_loss: 0.2541 - val_acc: 0.8977\n",
      "Epoch 23/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2004 - acc: 0.9222\n",
      "Epoch 00023: val_acc improved from 0.89770 to 0.89780, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.2004 - acc: 0.9222 - val_loss: 0.2540 - val_acc: 0.8978\n",
      "Epoch 24/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1948 - acc: 0.9233\n",
      "Epoch 00024: val_acc improved from 0.89780 to 0.89930, saving model to ../models/imdb/imdb_sentiment/0001/assets/cp.ckpt\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1948 - acc: 0.9233 - val_loss: 0.2530 - val_acc: 0.8993\n",
      "Epoch 25/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1907 - acc: 0.9258\n",
      "Epoch 00025: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1907 - acc: 0.9258 - val_loss: 0.2693 - val_acc: 0.8911\n",
      "Epoch 26/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1881 - acc: 0.9263\n",
      "Epoch 00026: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 0.1881 - acc: 0.9263 - val_loss: 0.2685 - val_acc: 0.8918\n",
      "Epoch 27/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1837 - acc: 0.9281\n",
      "Epoch 00027: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1837 - acc: 0.9281 - val_loss: 0.2577 - val_acc: 0.8957\n",
      "Epoch 28/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1795 - acc: 0.9309\n",
      "Epoch 00028: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1795 - acc: 0.9309 - val_loss: 0.2767 - val_acc: 0.8855\n",
      "Epoch 29/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1716 - acc: 0.9334\n",
      "Epoch 00029: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1716 - acc: 0.9334 - val_loss: 0.2537 - val_acc: 0.8973\n",
      "Epoch 30/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1706 - acc: 0.9345\n",
      "Epoch 00030: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1706 - acc: 0.9345 - val_loss: 0.2589 - val_acc: 0.8972\n",
      "Epoch 31/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1635 - acc: 0.9363\n",
      "Epoch 00031: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1635 - acc: 0.9363 - val_loss: 0.2688 - val_acc: 0.8917\n",
      "Epoch 32/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1636 - acc: 0.9369\n",
      "Epoch 00032: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1636 - acc: 0.9369 - val_loss: 0.2753 - val_acc: 0.8908\n",
      "Epoch 33/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1551 - acc: 0.9415\n",
      "Epoch 00033: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1551 - acc: 0.9415 - val_loss: 0.2876 - val_acc: 0.8904\n",
      "Epoch 34/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1563 - acc: 0.9386\n",
      "Epoch 00034: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 0.1563 - acc: 0.9386 - val_loss: 0.2802 - val_acc: 0.8938\n",
      "Epoch 35/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1540 - acc: 0.9412\n",
      "Epoch 00035: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1540 - acc: 0.9412 - val_loss: 0.2893 - val_acc: 0.8943\n",
      "Epoch 36/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1520 - acc: 0.9408\n",
      "Epoch 00036: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1520 - acc: 0.9408 - val_loss: 0.2987 - val_acc: 0.8807\n",
      "Epoch 37/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1497 - acc: 0.9413\n",
      "Epoch 00037: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1497 - acc: 0.9413 - val_loss: 0.3299 - val_acc: 0.8693\n",
      "Epoch 38/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1470 - acc: 0.9447\n",
      "Epoch 00038: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.1470 - acc: 0.9447 - val_loss: 0.2726 - val_acc: 0.8934\n",
      "Epoch 39/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1426 - acc: 0.9457\n",
      "Epoch 00039: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1426 - acc: 0.9457 - val_loss: 0.2870 - val_acc: 0.8899\n",
      "Epoch 40/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1443 - acc: 0.9451\n",
      "Epoch 00040: val_acc did not improve from 0.89930\n",
      "625/625 [==============================] - 40s 63ms/step - loss: 0.1443 - acc: 0.9451 - val_loss: 0.3286 - val_acc: 0.8849\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40  # for quick testing\n",
    "\n",
    "# callbacks\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                         factor=0.1, \n",
    "                                         patience=7, \n",
    "                                         verbose=1, \n",
    "                                         min_lr = 0.0001, \n",
    "                                         mode='min')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_BEST_WEIGHTS,\n",
    "                                                         save_best_only=True,\n",
    "                                                         monitor='val_acc',\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "\n",
    "# make sure we have a GPU\n",
    "print(\"Training on GPU...\") if tf.config.list_physical_devices('GPU') else print(\"Training on CPU...\")\n",
    "\n",
    "model_history = model.fit(train_data.batch(BATCH_SIZE), epochs=EPOCHS, validation_data=test_data.batch(BATCH_SIZE),callbacks=[reduce_lr_on_plateau,checkpoint_callback] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model using unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3286 - acc: 0.8849\n",
      "Accuracy: 88.49%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores_evaluate = model.evaluate(test_data.batch(BATCH_SIZE),verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_evaluate[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88      4993\n",
      "           1       0.84      0.95      0.89      5007\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.89      0.88      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix',cmap=plt.cm.Greys, normalize=True):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=16)\n",
    "    plt.yticks(tick_marks, classes, fontsize=16)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Actual', fontsize=22)\n",
    "    plt.xlabel('Predicted', fontsize=17)\n",
    "    \n",
    "    \n",
    "def decode_sentiment(score):\n",
    "    return 1 if score>0.5 else 0\n",
    "\n",
    "scores_predict = model.predict(test_data.batch(BATCH_SIZE),verbose=0)\n",
    "y_predictions = [decode_sentiment(score) for score in scores_predict]\n",
    "\n",
    "# add text predictions to list \n",
    "text_dataset_predictions = []\n",
    "[text_dataset_predictions.append(item[1].numpy()) for item in test_data]\n",
    "\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(text_dataset_predictions, y_predictions)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(set(text_dataset_predictions)), title=\"Confusion matrix\")\n",
    "plt.savefig('images/imdb-confusion-matrix.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# show the summary report\n",
    "print(\"\\n\\n\",classification_report(text_dataset_predictions, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability this review is positive:  [[0.1174846]]\n",
      "Probability this review is positive:  [[0.83070904]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p = [[\"Totally overrated serie, the storyline just doesn't add up. Especially the action scenes are chaotic, have no common sense. The only positive thing is the acting of the Mandalorian. The little gremlin (baby Yoda) is only cute, but it uses his (or her?) powers on strange moments. Overall not worth to pay for Disney+ only for this serie.\"]]\n",
    "prob_positive = model.predict(p)\n",
    "print(\"Probability this review is positive: \",prob_positive)\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"This is the Star Wars atmosphere and feeling I've been waiting for. No dumb humor, cool characters, and a story I can take seriously. I'm a big fan of what they've done with this series so far and current Star Wars filmmakers need to take notes. The cinematography is amazing. You can tell they use practicality as much as possible and CGI is used only for the obvious like spaceships and creatures etc. It's a truly remarkable balance of old and new.\"]]\n",
    "prob_positive = model.predict(p)\n",
    "print(\"Probability this review is positive: \",prob_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best weights generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2530 - acc: 0.8993\n",
      "Accuracy: 89.93%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights(MODEL_BEST_WEIGHTS) # loads best weights saved during training\n",
    "\n",
    "scores_evaluate = model.evaluate(test_data.batch(BATCH_SIZE),verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_evaluate[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model again after loading the best weights found for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability this review is positive:  [[0.13463074]]\n",
      "Probability this review is positive:  [[0.96699154]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p_2 = [[\"Totally overrated serie, the storyline just doesn't add up. Especially the action scenes are chaotic, have no common sense. The only positive thing is the acting of the Mandalorian. The little gremlin (baby Yoda) is only cute, but it uses his (or her?) powers on strange moments. Overall not worth to pay for Disney+ only for this serie.\"]]\n",
    "prob_positive = model.predict(p_2)\n",
    "print(\"Probability this review is positive: \",prob_positive)\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"This is the Star Wars atmosphere and feeling I've been waiting for. No dumb humor, cool characters, and a story I can take seriously. I'm a big fan of what they've done with this series so far and current Star Wars filmmakers need to take notes. The cinematography is amazing. You can tell they use practicality as much as possible and CGI is used only for the obvious like spaceships and creatures etc. It's a truly remarkable balance of old and new.\"]]\n",
    "prob_positive = model.predict(p)\n",
    "print(\"Probability this review is positive: \",prob_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save our final model, along with the embedding_matrix and vocab file, so we can recreate it exactly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/imdb/imdb_sentiment/0001/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# save the model first, so that assets folder is created and available\n",
    "model.save(MODEL_PATH)  # creates a .pd file\n",
    "\n",
    "# save history as a dictionary in case we need to recreate the plots again later\n",
    "with open(MODEL_TRAINING_HISORTY_FILE, 'wb') as file_pickle:\n",
    "        pickle.dump(model_history.history, file_pickle)\n",
    "\n",
    "\n",
    "# save the vocabulary file\n",
    "with open(MODEL_ASSETS_VOCABULARY_FILE, 'wb') as file_pickle:\n",
    "        pickle.dump(vocab, file_pickle)\n",
    "\n",
    "\n",
    "# save the embeddings matrix\n",
    "with open(MODEL_ASSETS_EMBEDDINGS_FILE, 'wb') as file_pickle:\n",
    "        pickle.dump(embedding_matrix, file_pickle)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f14e28fb6a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # in Tensorflow 2.1 and above\n",
    "import pickle \n",
    "\n",
    "v = pickle.load(open(MODEL_ASSETS_VOCABULARY_FILE, 'rb'))\n",
    "em = pickle.load(open(MODEL_ASSETS_EMBEDDINGS_FILE, 'rb'))\n",
    "\n",
    "\n",
    "vectorizer_layer = TextVectorization(\n",
    "    standardize=normlize_data, \n",
    "    max_tokens=50000, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=300)\n",
    "\n",
    "# build vocabulary, will also run the normalize_data() \n",
    "vectorizer_layer.set_vocabulary(v)\n",
    "\n",
    "\n",
    "saved_model = create_model(vectorizer_layer,em,v)\n",
    "\n",
    "# load the weights\n",
    "saved_model.load_weights(MODEL_BEST_WEIGHTS) # loads best weights saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions for the above input is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability this review is positive:  [[0.13463074]]\n",
      "Probability this review is positive:  [[0.96699154]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p_2 = [[\"Totally overrated serie, the storyline just doesn't add up. Especially the action scenes are chaotic, have no common sense. The only positive thing is the acting of the Mandalorian. The little gremlin (baby Yoda) is only cute, but it uses his (or her?) powers on strange moments. Overall not worth to pay for Disney+ only for this serie.\"]]\n",
    "prob_positive = saved_model.predict(p_2)\n",
    "print(\"Probability this review is positive: \",prob_positive)\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"This is the Star Wars atmosphere and feeling I've been waiting for. No dumb humor, cool characters, and a story I can take seriously. I'm a big fan of what they've done with this series so far and current Star Wars filmmakers need to take notes. The cinematography is amazing. You can tell they use practicality as much as possible and CGI is used only for the obvious like spaceships and creatures etc. It's a truly remarkable balance of old and new.\"]]\n",
    "prob_positive = saved_model.predict(p)\n",
    "print(\"Probability this review is positive: \",prob_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create attack embeddings matrix for our vocabulary\n",
    "#### (This will be used when generating attacks against the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter_embeddings_matrix = get_embeddings_matrix(COUNTER_FITTED_VECTORS, data_dictionary, DIMENSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of our vocabulary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(counter_embeddings_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our vocabulary embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(epochs, loss,'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('images/S140-training-and-validation-loss.png',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('images/S140-training-and-validation-accuracy.png',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_embeddings_matrix  = get_embeddings_matrix(COUNTER_FITTED_VECTORS, data_dictionary, DIMENSION)\n",
    "#counter_dict = counter_embeddings_dictionary(COUNTER_FITTED_VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counter_embeddings_matrix = np.load((\"embeddings/imdb_counter_fitted_embeddings_%d.npy\" % MAX_VOCABULARY_SIZE))\n",
    "#print(find_closest_embeddings(data_dictionary[\"movies\"])[1:10])\n",
    "\n",
    "\n",
    "print(type(counter_embeddings_matrix))\n",
    "print(counter_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save word distance matrix\n",
    "#### l2 distance: $(x-y)^2$ expand this out and we get: $x^2 + y^2 - 2xy$ using this to calculate euclidian (l2) distance between vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(matrix):\n",
    "    dists = -2 * np.dot(matrix, matrix.T) + np.sum(matrix**2,    axis=1) + np.sum(matrix**2, axis=1)[:, np.newaxis]\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THIS IS WHY WE HAVE TO REDUCE THE VOCABULARY SIZE - WE NEED TO ALLOCATE ENOUGH SPACE FOR THIS MATRIX\n",
    "IF WE USE THE FULL VOCABULARY SIZE - WE WILL NOT BE ABLE TO ALLOCATE ENOUGH MEMORY FOR THE SIZE OF THE MATRIX\n",
    "THAT WILL BE GENERATED\n",
    "\"\"\"\n",
    "square_matrix = -2 * np.dot(counter_embeddings_matrix, counter_embeddings_matrix.T)\n",
    "print(len(square_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(np.square(counter_embeddings_matrix), axis=1).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = a + b + square_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(('embeddings/distance_counterfitted_embeddings_%d.npy' % MAX_VOCABULARY_SIZE), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amin(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closest words in disrtance matrix, and their distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_train import build_embeddings as bemb\n",
    "\n",
    "for r in range(100,105):\n",
    "    source_word = r\n",
    "    \n",
    "    nearest_neighbour, distance_to_neighbour = bemb.closest_neighbours(source_word, dist, ret_count=5)\n",
    "    \n",
    "    print(\"Nearest Neighbours to `%s` are:\" %inv_data_dictionary[source_word])\n",
    "    \n",
    "    for word_index, distance in zip(nearest_neighbour,distance_to_neighbour):\n",
    "        print(\"\\t--> \", inv_data_dictionary[word_index], \"     \", distance)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_order = np.argsort(dist[src_word, :])[1:8]\n",
    "print(dist_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB reviews dataset using tfds. This is the raw data.\n",
    "imdb_reviews = tfds.load('imdb_reviews')\n",
    "\n",
    "# The IMDB dataset contains a train split and a test split; we create a separate\n",
    "# handle for each.\n",
    "train_raw = imdb_reviews['train']\n",
    "test_raw = imdb_reviews['test']\n",
    "\n",
    "\n",
    "# Once we have our handles, we format the datasets in a Keras-fit compatible\n",
    "# format: a tuple of the form (text_data, label).\n",
    "def format_dataset(input_data):\n",
    "    return (input_data['text'], input_data['label'])\n",
    "\n",
    "train_dataset = train_raw.map(format_dataset)\n",
    "test_dataset = test_raw.map(format_dataset)\n",
    "\n",
    "# We also create a dataset with only the textual data in it. This will be used\n",
    "# to build our vocabulary later on.\n",
    "\n",
    "text_dataset_1 = train_raw.map(lambda data: data['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, i in train_dataset.take(1):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
