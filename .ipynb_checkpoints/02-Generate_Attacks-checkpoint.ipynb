{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to latest model version number\n",
    "\n",
    "def set_model_version_number():\n",
    "    version_number = []\n",
    "    global MODEL_VERSION\n",
    "    global MODEL_PATH\n",
    "\n",
    "    if os.path.exists(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):   \n",
    "        for entry in os.listdir(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):\n",
    "            version_number.append(entry)       \n",
    "        MODEL_VERSION = version_number[-1]\n",
    "        MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config/main.conf')\n",
    "\n",
    "DATASET = 1\n",
    "MODEL_VERSION =  \"0001\"\n",
    "DOWNLOAD_GOOGLE_LM = False\n",
    "\n",
    "if DATASET == 1:\n",
    "    set_dataset = \"imdb\"\n",
    "if DATASET == 2:\n",
    "    set_dataset = \"s140\"\n",
    "\n",
    "DATASET_URL = (config[set_dataset]['DATASET_URL'])\n",
    "\n",
    "DATASET_FOLDER = config[set_dataset]['DATASET_FOLDER']\n",
    "DATASET_TAR_FILE_NAME = config[set_dataset]['DATASET_TAR_FILE_NAME']\n",
    "DATASET_NAME = config[set_dataset]['DATASET_NAME']\n",
    "\n",
    "MODEL_NAME = config[set_dataset]['MODEL_NAME']\n",
    "\n",
    "CLEAN_DATA_FILE = os.path.join(DATASET_FOLDER,\"normalized_dataset.csv\")\n",
    "TAR_FILE_PATH = os.path.join(DATASET_FOLDER,DATASET_TAR_FILE_NAME)\n",
    "DATA_SET_LOCATION = os.path.join(DATASET_FOLDER,DATASET_NAME)\n",
    "\n",
    "MODEL_SAVE_DIRECTORY = config[set_dataset]['MODEL_SAVE_DIRECTORY']\n",
    "# Create the model save directory\n",
    "if not os.path.exists(MODEL_SAVE_DIRECTORY):\n",
    "    os.makedirs(MODEL_SAVE_DIRECTORY)\n",
    "    \n",
    "IMAGE_SAVE_FOLDER = config[set_dataset]['IMAGE_SAVE_FOLDER']\n",
    "    \n",
    "GLOVE_EMBEDDINGS = config[set_dataset]['GLOVE_EMBEDDINGS']\n",
    "COUNTER_FITTED_VECTORS = config[set_dataset]['COUNTER_FITTED_VECTORS']\n",
    "\n",
    "GLOVE_EMBEDDINGS_MATRIX = config[set_dataset]['GLOVE_EMBEDDINGS_MATRIX']\n",
    "COUNTER_FITTED_EMBEDDINGS_MATRIX = config[set_dataset]['COUNTER_FITTED_EMBEDDINGS_MATRIX']\n",
    "\n",
    "LM_URLS = config[set_dataset]['LM_URLS']\n",
    "LM_DIRECTORY = config[set_dataset]['LM_DIRECTORY']\n",
    "\n",
    "####### files required to reconstruct the final trained model ##############################\n",
    "MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "set_model_version_number()\n",
    "\n",
    "ASSESTS_FOLDER = os.path.join(MODEL_PATH,\"assets\")\n",
    "MODEL_ASSETS_VOCABULARY_FILE = os.path.join(ASSESTS_FOLDER,\"vocab\")\n",
    "MODEL_ASSETS_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"imdb_glove_embeddings_matrix\")\n",
    "MODEL_ASSETS_COUNTER_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"counter_embeddings_matrix\")\n",
    "MODEL_ASSETS_DISTANCE_MATRIX = os.path.join(ASSESTS_FOLDER,\"distance_matrix.npy\")\n",
    "MODEL_ASSETS_SAVE_BEST_WEIGHTS = os.path.join(ASSESTS_FOLDER, \"cp.ckpt\")\n",
    "MODEL_TRAINING_HISORTY_FILE = os.path.join(ASSESTS_FOLDER, \"training_history.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our pre trained sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5ea02c9c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # in Tensorflow 2.1 and above\n",
    "import pickle \n",
    "\n",
    "MAX_VOCABULARY_SIZE = 50000\n",
    "DIMENSION = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "from manny_modules import tf_normalize_data as tfnd\n",
    "from manny_modules import return_model as rmodel\n",
    "\n",
    "saved_vocab = pickle.load(open(MODEL_ASSETS_VOCABULARY_FILE, 'rb'))\n",
    "saved_word_index = dict(zip(saved_vocab, range(len(saved_vocab))))\n",
    "\n",
    "saved_embeddings_matric = pickle.load(open(MODEL_ASSETS_EMBEDDINGS_FILE, 'rb'))\n",
    "\n",
    "\n",
    "vectorizer_layer = TextVectorization(\n",
    "    standardize=tfnd.normlize_data, \n",
    "    max_tokens=MAX_VOCABULARY_SIZE, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=300)\n",
    "\n",
    "# build vocabulary, will also run the normalize_data() \n",
    "vectorizer_layer.set_vocabulary(saved_vocab)\n",
    "\n",
    "\n",
    "saved_model = rmodel.create_model(vectorizer_layer,\n",
    "                                  saved_embeddings_matric,\n",
    "                                  saved_vocab,\n",
    "                                  dimension=DIMENSION, \n",
    "                                  lrate=LEARNING_RATE)\n",
    "\n",
    "# load the weights\n",
    "saved_model.load_weights(MODEL_ASSETS_SAVE_BEST_WEIGHTS) # loads best weights saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model - check predictions for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive confidence:  [[0.01855881]]  Negative confidence:  [[0.9814412]]\n",
      "Positive confidence:  [[0.9553545]]  Negative confidence:  [[0.04464549]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p_2 = [[\"Seriously, don't bother if you're over 12. This looks like a kids show designed purely to sell merchandise, theme park rides, etc. No logic, holes all over the shop, no characters motivation and really crap acting to top it off... just rubbish, really\"]]\n",
    "prob_positive = saved_model.predict(p_2)\n",
    "\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"It's one thing to bring back elements, characters, settings and stories, and to flash them in front of the audience to cash in on the nostalgia and/or recognisable memorabilia but without using it to further the plot and other to do exactly the opposite. It was about time that Star Wars directives understood that it is too unique a product to be lend to corporate filmmakers. Star Wars needs to be understood and its uniqueness has to be acknowledged in order to make the new stories feel like they belong. This may sound too obvious but if you ever wondered why the new SW movies are so controversial this may be the reason.Like with 'Spider-Man: Into the Spider-verse (2018)' and their comicbook-industry experts participation, the creators behind The Mandalorian were experts of the industry, connoisseurs of the Star Wars Universe and even long time fans. So they were able to not only recapture the aesthetic of the grimy, battered Star Wars but also build upon it taking the most 'subtle' things into account. Things like the predominancy of puppets and practical effects over CGI, settings you can feel and touch over green screens and the abundancy of not only known elements previously seen in Star Wars, but a whole batch of new creatures, designs and overall plot elements that felt like they belong to this universe and had always been there. Exceeding expectations are not only the visual aspects but the narrative too. It might be too late for some story elements now, but it is of great importance that from now on you try to watch the unraveling of the story unspoiled. I was lucky to have seen the premiere of the show before the 'memefication' of a certain 'element' that went viral and became one of the biggest highlights of the show. But for me I saw the reveal of this element unspoiled and I was pleasantly shocked, a memory I'll always carry with me. The ability of these creators to generate such shock value and deep moments it's often baffling to me. This is proof that the creators behind the narrative are fully aware of the complexities of the universe they are tampering with and like an experienced surgeon, they are able to tweak, traverse and call back any Star Wars element as they please and with astonishing results.\"]]\n",
    "prob_positive = saved_model.predict(p)\n",
    "\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the distance matrix from disk (load this before running below tests)\n",
    "- This is a large file (~20GB), so will take time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_matrix = np.load(MODEL_ASSETS_DISTANCE_MATRIX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word =  saved_word_index['scotland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import nearest_neighbour as nn\n",
    "\n",
    "nearest_neighbour, distance_to_neighbour = nn.closest_neighbours(target_word, distance_matrix, number_of_words_to_return=5, max_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words closest to `scotland` are `['scot', 'scots', 'scottish', 'scotsman', 'scotch']` \n"
     ]
    }
   ],
   "source": [
    "closest_word = [saved_vocab[x] for x in nearest_neighbour]\n",
    "\n",
    "print(\"Words closest to `%s` are `%s` \" % (saved_vocab[target_word], closest_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtypes = {'sentiment': 'int', 'text': 'str'}\n",
    "data_frame = pd.read_csv(CLEAN_DATA_FILE,dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the saved sample dataset - use the same dataset for both BERT and distance matrix GA attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright sidney bruhl michael caine has had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yikes did this movie blow  the characters were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this would have to be one of the worst  if not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock holmes adventure starring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first  i must say that i don't speak spanish a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  playwright sidney bruhl michael caine has had ...\n",
       "1          0  yikes did this movie blow  the characters were...\n",
       "2          0  this would have to be one of the worst  if not...\n",
       "3          1  a brilliant sherlock holmes adventure starring...\n",
       "4          1  first  i must say that i don't speak spanish a..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "dtypes = {'sentiment': 'int', 'text': 'str'}\n",
    "\n",
    "# if True then load saved sample dataset, else create a sample dataset\n",
    "SAVED_DATASET = True\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "def load_sample_dataset(): \n",
    "    global SAMPLE_SIZE\n",
    "    if SAVED_DATASET:\n",
    "        # load the same data-sample used in original attack\n",
    "        data_sample = pd.read_csv('imdb_dataset/sample_dataset.csv',dtype=dtypes).dropna()\n",
    "        SAMPLE_SIZE = len(data_sample)\n",
    "    else:\n",
    "        #generate new sample from original data set\n",
    "        data_frame = pd.read_csv(CLEAN_DATA_FILE,dtype=dtypes)\n",
    "        data_sample = data_frame.sample(n = SAMPLE_SIZE).dropna()\n",
    "        SAMPLE_SIZE = len(data_sample)\n",
    "    return data_sample\n",
    "\n",
    "\n",
    "data_sample = load_sample_dataset()\n",
    "# save sample dataset with date stamp\n",
    "data_sample.to_csv('imdb_dataset/sample_dataset_'+ str(date.today()) + '_.csv', index = False)\n",
    "\n",
    "# show the first 5 randonly selected data items\n",
    "data_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items:  1006\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data items: \",len(data_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add a new column ```probs``` to our sample dataframe to store probability of text being positove (default values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright sidney bruhl michael caine has had ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yikes did this movie blow  the characters were...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this would have to be one of the worst  if not...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock holmes adventure starring...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first  i must say that i don't speak spanish a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  probs\n",
       "0          1  playwright sidney bruhl michael caine has had ...    0.0\n",
       "1          0  yikes did this movie blow  the characters were...    0.0\n",
       "2          0  this would have to be one of the worst  if not...    0.0\n",
       "3          1  a brilliant sherlock holmes adventure starring...    0.0\n",
       "4          1  first  i must say that i don't speak spanish a...    0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample['probs'] = 0\n",
    "data_sample['probs'] = data_sample['probs'].astype(float) # has to be of type float, to store probability values\n",
    "\n",
    "data_sample = data_sample.reset_index(drop=True) # reindex so we start from 0 in the sample data set\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now run each one against model and store probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright sidney bruhl michael caine has had ...</td>\n",
       "      <td>0.978781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yikes did this movie blow  the characters were...</td>\n",
       "      <td>0.007015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this would have to be one of the worst  if not...</td>\n",
       "      <td>0.016789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock holmes adventure starring...</td>\n",
       "      <td>0.999339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first  i must say that i don't speak spanish a...</td>\n",
       "      <td>0.906772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs\n",
       "0          1  playwright sidney bruhl michael caine has had ...  0.978781\n",
       "1          0  yikes did this movie blow  the characters were...  0.007015\n",
       "2          0  this would have to be one of the worst  if not...  0.016789\n",
       "3          1  a brilliant sherlock holmes adventure starring...  0.999339\n",
       "4          1  first  i must say that i don't speak spanish a...  0.906772"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in data_sample.index:\n",
    "    p = saved_model.predict([data_sample.iloc[i]['text']])\n",
    "    data_sample.at[i,'probs']= p\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new columns to hold results after genetic attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright sidney bruhl michael caine has had ...</td>\n",
       "      <td>0.978781</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.978781</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>yikes did this movie blow  the characters were...</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.007015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this would have to be one of the worst  if not...</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a brilliant sherlock holmes adventure starring...</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>first  i must say that i don't speak spanish a...</td>\n",
       "      <td>0.906772</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.906772</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          1  playwright sidney bruhl michael caine has had ...  0.978781   \n",
       "1          0  yikes did this movie blow  the characters were...  0.007015   \n",
       "2          0  this would have to be one of the worst  if not...  0.016789   \n",
       "3          1  a brilliant sherlock holmes adventure starring...  0.999339   \n",
       "4          1  first  i must say that i don't speak spanish a...  0.906772   \n",
       "\n",
       "   ga_sentiment ga_text  ga_probs  ga_num_changes  ga_lev_ratio  \\\n",
       "0             1          0.978781               0           0.0   \n",
       "1             0          0.007015               0           0.0   \n",
       "2             0          0.016789               0           0.0   \n",
       "3             1          0.999339               0           0.0   \n",
       "4             1          0.906772               0           0.0   \n",
       "\n",
       "  ga_flipped_sentiment  ga_percent_change  \n",
       "0                    N                0.0  \n",
       "1                    N                0.0  \n",
       "2                    N                0.0  \n",
       "3                    N                0.0  \n",
       "4                    N                0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy current sentiment values to new columns, we can then later go through and compare any values that have been changed during the GA attack\n",
    "data_sample['ga_sentiment'] = data_sample['sentiment']\n",
    "\n",
    "# create new ga_text to hold perturbed text\n",
    "data_sample['ga_text'] = \"\"\n",
    "\n",
    "# create ga_probs to hold new probability values after GA Attack, fill with current values\n",
    "data_sample['ga_probs'] = data_sample['probs']\n",
    "\n",
    "# create ga_num_changes to hold the number of words changed\n",
    "data_sample['ga_num_changes'] = 0\n",
    "data_sample['ga_num_changes'] = data_sample['ga_num_changes'].astype(int)\n",
    "\n",
    "# create ga_lev_ratio to hold the Levenshtein ratio\n",
    "data_sample['ga_lev_ratio'] = 0.0\n",
    "data_sample['ga_lev_ratio'] = data_sample['ga_lev_ratio'].astype(float)\n",
    "\n",
    "# add field to indicate if sentiment was flipped on review text\n",
    "data_sample['ga_flipped_sentiment'] = 'N'\n",
    "data_sample['ga_flipped_sentiment'] = data_sample['ga_flipped_sentiment'].astype(str)\n",
    "\n",
    "\n",
    "# percentage of words changed in sentence\n",
    "data_sample['ga_percent_change'] = 0.0\n",
    "data_sample['ga_percent_change'] = data_sample['ga_percent_change'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the predictions are correct, if not then drop row from data set\n",
    "### we only want to keep correctly classified data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indexes = []\n",
    "\n",
    "for i in data_sample.index:\n",
    "    if data_sample.iloc[i]['sentiment'] == 1 and data_sample.iloc[i]['probs'] > 0.5:\n",
    "        continue\n",
    "    if data_sample.iloc[i]['sentiment'] == 0 and data_sample.iloc[i]['probs'] <= 0.5:\n",
    "        continue\n",
    "    else:\n",
    "        drop_indexes.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items dropped from sample:  0\n",
      "Number of data items kept in sample:  1006\n"
     ]
    }
   ],
   "source": [
    "data_sample = data_sample.drop(drop_indexes)\n",
    "data_sample = data_sample.reset_index(drop=True) # reindex dataframe to start from 0\n",
    "\n",
    "# check how many rows we dropped due to incorrect classification\n",
    "print(\"Number of data items dropped from sample: \",(SAMPLE_SIZE - len(data_sample)))\n",
    "print(\"Number of data items kept in sample: \",(len(data_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import math\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/\n",
    "bertMaskedLM = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bertMaskedLM.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# https://huggingface.co/bert-base-cased?text=I+%5BMASK%5D+salad+for+lunch\n",
    "# create pipeline to process masked words\n",
    "mask_word_pipeline = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "def return_masked_words(p, masked_sentence):\n",
    "    '''return list of possible masked words'''\n",
    "    return p(masked_sentence)\n",
    "\n",
    "def get_score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions=bertMaskedLM(tensor_input)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(predictions.squeeze(),tensor_input.squeeze()).data\n",
    "    return math.exp(loss)\n",
    "\n",
    "\n",
    "def prediction_probability(model, text_review):\n",
    "    '''return probability of this string being a positive sentiment'''\n",
    "    return model.predict([text_review])\n",
    "\n",
    "\n",
    "def crossover(parent_one, parent_two):\n",
    "    p_one = parent_one.split()\n",
    "    p_two = parent_two.split()\n",
    "    \n",
    "    new_offspring = p_one.copy()\n",
    "    text_len = min(len(new_offspring), len(p_two))\n",
    "    # use random univform distribution to select which words to replace \n",
    "    # when creating the new offspring for our two parent strings\n",
    "    for i in range(text_len):\n",
    "        if np.random.uniform() < 0.5:\n",
    "            new_offspring[i] = p_two[i]\n",
    "    return ' '.join(new_offspring)\n",
    "    \n",
    "    \n",
    "def mutation(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):\n",
    "    '''returns the string after swapping max_perturbations nearest neighbours\n",
    "    of each string'''\n",
    "    \n",
    "    # keep track of list index of the which word we have already changed\n",
    "    selected_index = []\n",
    "    \n",
    "    #split string so we can iterate over each word\n",
    "    t_split = text_review.split()\n",
    "    \n",
    "    # select a random index value\n",
    "    indx = randrange(2, len(t_split) - 3)\n",
    "    \n",
    "    for i in range(max_perturbations):\n",
    "        # get a random index number for word list\n",
    "        # start at 3rd word and upto 3rd last word index\n",
    "        indx = randrange(2,len(t_split) - 3)\n",
    "        \n",
    "        found_in_vocab = False\n",
    "        # skip over all stop words and any indexes we have already selected\n",
    "        while t_split[indx] in stop_words or indx in selected_index  or not found_in_vocab:     \n",
    "            \n",
    "            indx = randrange(2,len(t_split) - 3)\n",
    "            # if word is not found in vocabulary, skip it and try next word\n",
    "            try:\n",
    "                target_word = saved_word_index[t_split[indx]]\n",
    "                found_in_vocab = True\n",
    "            except KeyError:\n",
    "                found_in_vocab = False\n",
    "        \n",
    "        # now we have a word that is not a stop word and has not already been selected\n",
    "        selected_index.append(indx) # add to our list\n",
    "        \n",
    "        \n",
    "        ### FITNESS TEST #########\n",
    "        # we want to now get a list of the closest max_neighbours synonyms using BERT - \n",
    "        # we use the two words before and 2 after so we have context for chosen word\n",
    "        # and then use BERT to return a list of possible substitutions for our masked word\n",
    "        join_masked = [t_split[indx  - 2],t_split[indx  - 1],'[MASK]',t_split[indx  + 1], t_split[indx  + 2]]\n",
    "        masked_word_string = ' '.join(join_masked) \n",
    "        \n",
    "        # now we need to pass this string to BERT and get a list of possible substitutions back\n",
    "        possible_substitutions = return_masked_words(mask_word_pipeline, masked_word_string)\n",
    "        \n",
    "        # if one of the words returned is the same as the original word - we want to remove it from our list\n",
    "        # also remove any non alpha returns\n",
    "        closest_word = []\n",
    "        for i in range(len(possible_substitutions)):\n",
    "            if possible_substitutions[i]['token_str'] == t_split[indx] or not possible_substitutions[i]['token_str'].isalpha():\n",
    "                continue\n",
    "            else:\n",
    "                closest_word.append(possible_substitutions[i]['token_str'])\n",
    "            \n",
    "        \n",
    "        original_word = t_split[indx]\n",
    "        word_prob_dict = dict()\n",
    "        for w in closest_word:\n",
    "            if not w: # if we have an empty string then do nothing, we don't want to remove a word from the string\n",
    "                continue\n",
    "            t_split[indx] = w\n",
    "            word_prob_dict[w] = model.predict([' '.join(t_split)])\n",
    "            \n",
    "        # didn't find any suitable words\n",
    "        if len(word_prob_dict) == 0:\n",
    "            continue\n",
    "        \n",
    "        # the word we decided to substitute is based on the probability returned by the model\n",
    "        # if we have target_label == 1 then we are trying to go from negative to positive sentiment\n",
    "        # therefore we want to keep the highest probability returned\n",
    "        # NB the probability returned by our model is the probability that the review is positive, higher values == more positive sentiment\n",
    "        if target_label == 1:\n",
    "            sub_word = max(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "            \n",
    "        # if our target_label == 0 i.e. negative, then we are going from positive to negative\n",
    "        # hence we want to keep only the lowest value\n",
    "        else:\n",
    "            sub_word = min(word_prob_dict, key=word_prob_dict.get)\n",
    "            t_split[indx] = sub_word\n",
    "        \n",
    "        ### END FITNESS TEST ##########\n",
    "        \n",
    "        # add selected index to selected_index list\n",
    "        selected_index.append(indx)\n",
    "\n",
    "    return ' '.join(t_split)\n",
    "    \n",
    "\n",
    "def generate_population(model, text_review, population_size, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab):\n",
    "    \n",
    "    '''return list of strings of size population_size'''\n",
    "    \n",
    "    population = []\n",
    "    \n",
    "    for i in range(population_size):\n",
    "        t = mutation(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab)\n",
    "        population.append(t)\n",
    "    return population\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Data Item:  1  #######\n",
      "\tNumber of words in review:  316\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  2.0 %\n",
      "\tProb. before and after:  0.0005437727668322623  :  0.0014025685377418995 \n",
      "\n",
      "####### Data Item:  2  #######\n",
      "\tNumber of words in review:  150\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  3.0 %\n",
      "\tProb. before and after:  0.001128381467424333  :  0.03041226603090763 \n",
      "\n",
      "####### Data Item:  3  #######\n",
      "\tNumber of words in review:  405\n",
      "\tNumber of words swapped:  5\n",
      "\tPercentage modified:  1.0 %\n",
      "\tProb. before and after:  0.6753912568092346  :  0.30428051948547363 \n",
      "\n",
      "####### Data Item:  4  #######\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-34c0e7b68929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcurrent_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOPULATION_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_PERTURBATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_NEIGHBOURS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mpopulation_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation_probs_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-757e2d6b47b5>\u001b[0m in \u001b[0;36mgenerate_population\u001b[0;34m(model, text_review, population_size, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_perturbations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_neighbours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mpopulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-757e2d6b47b5>\u001b[0m in \u001b[0;36mmutation\u001b[0;34m(model, text_review, current_prediction, target_label, max_perturbations, max_neighbours, saved_vocab)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mt_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mword_prob_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# didn't find any suitable words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         sample_weights, sample_weight_modes)\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m     self._internal_adapter = TensorLikeDataAdapter(\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1725\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \"\"\"\n\u001b[0;32m-> 1727\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4120\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4122\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4123\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m   4124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m-> 2938\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m       \u001b[0;31m# TensorArrays and `None`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       func_outputs = nest.map_structure(convert, func_outputs,\n\u001b[0m\u001b[1;32m    991\u001b[0m                                         expand_composites=True)\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    948\u001b[0m               (str(python_func), type(x)))\n\u001b[1;32m    949\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# depend on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3999\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   4002\u001b[0m         \"Identity\", input=input, name=name)\n\u001b[1;32m   4003\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3475\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3476\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3477\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m     \u001b[0;31m# List of _UserDevSpecs holding code location of device context manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/tensorflow/python/util/tf_stack.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(limit)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;31m# traversing the stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mthread_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_thread_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   return _tf_stack.extract_stack(\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0m_source_mapper_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "POPULATION_SIZE = 40 # max population size to create\n",
    "MAXIMUM_ITERATIONS = 100 # stopping condition for loop if we do not find an optimal solution\n",
    "MAX_PERTURBATIONS = 5 # maximum number of changes to make for each population member\n",
    "MAX_NEIGHBOURS = 4 # maximum number of neighbouring words to return and check against\n",
    "\n",
    "\n",
    "def population_probs_df(population_list, model):\n",
    "    # dataframe to store population text and probabilities\n",
    "    population_probs = pd.DataFrame(columns=['ga_text','ga_probs'])\n",
    "    \n",
    "\n",
    "    # make sure columns have the correct types\n",
    "    population_probs['ga_text'] = population_probs['ga_text'].astype(str)\n",
    "    population_probs['ga_probs'] = population_probs['ga_probs'].astype(float)\n",
    "    \n",
    "    for i in range(len(population_list)):\n",
    "        new_row = {'ga_text':population_list[i], 'ga_probs':model.predict([population_list[i]])}\n",
    "        #append row to the dataframe\n",
    "        population_probs = population_probs.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # sort the array by probabilities column, i.e column 2 \n",
    "    return  population_probs.sort_values('ga_probs') # return sorted df, sorted by probability lowest to highest\n",
    "    \n",
    "\n",
    "def found_solution(target_label, population_df):\n",
    "    \n",
    "    if target_label == 1:\n",
    "        if population_df.iloc[-1]['ga_probs'] > 0.5:\n",
    "            return True\n",
    "    \n",
    "    if target_label == 0:\n",
    "        if population_df.iloc[0]['ga_probs'] <= 0.5:\n",
    "            return True \n",
    "    return False\n",
    "\n",
    "\n",
    "def number_of_changes_made(review_before, review_after):\n",
    "    word_count = 0\n",
    "    r_before = review_before.split()\n",
    "    r_after = review_after.split()\n",
    "    for i in range(len(r_before)):\n",
    "        if r_before[i] == r_after[i]:\n",
    "            pass\n",
    "        else:\n",
    "            word_count += 1\n",
    "    return word_count\n",
    "\n",
    "#data_sample_len = len(data_sample)\n",
    "data_sample_len = len(data_sample)\n",
    "\n",
    "for i in range(data_sample_len):\n",
    "    \n",
    "    print(\"####### Data Item: \",i+1,\" #######\")\n",
    "    target_label = 0 if data_sample.iloc[i]['sentiment'] == 1 else 1\n",
    "    current_prediction = data_sample.iloc[i]['probs']\n",
    "    \n",
    "    p = generate_population(saved_model, data_sample.iloc[i]['text'], POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "    \n",
    "    population_dataframe = population_probs_df(p, saved_model)\n",
    "\n",
    "    \n",
    "    ## GA Attack START\n",
    "    # need to run through and do crossover and mutation, recheck the label and if it has flipped then we stop, other wise keep going\n",
    "    # also on each iteration keep updating the ga_text and ga_prob and if label has changed update the ga_sentiment column and stop\n",
    "    for j in range(MAXIMUM_ITERATIONS):\n",
    "        \n",
    "        \n",
    "        # first check if we have found a solution, if yes then we are done so save results and break\n",
    "        # and move onto next\n",
    "        if found_solution(target_label, population_dataframe):\n",
    "            #save solution\n",
    "            if target_label == 1:   \n",
    "                data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "                data_sample.at[i, \"ga_sentiment\"] = 1\n",
    "                break\n",
    "            if target_label == 0:\n",
    "                data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[0]['ga_text']\n",
    "                data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "                data_sample.at[i, \"ga_sentiment\"] = 0\n",
    "                break\n",
    "        \n",
    "        # limit max percentage change to 20% of each review\n",
    "        if round(1 - (lev.ratio(data_sample.at[i,'text'],data_sample.at[i,'ga_text']))) > 0.20:\n",
    "            data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "            data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "            break\n",
    "            \n",
    "        \n",
    "        if target_label == 1:\n",
    "            parent_one = population_dataframe.iloc[-1]['ga_text']\n",
    "            parent_two = population_dataframe.iloc[-2]['ga_text']\n",
    "        else: \n",
    "            parent_one = population_dataframe.iloc[0]['ga_text']\n",
    "            parent_two = population_dataframe.iloc[1]['ga_text']\n",
    "        \n",
    "        \n",
    "        if target_label == 1 and (data_sample.iloc[i]['ga_probs'] < population_dataframe.iloc[-1]['ga_probs']):\n",
    "            data_sample.at[i, \"ga_text\"] = population_dataframe.iloc[-1]['ga_text']\n",
    "            data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[-1]['ga_probs']\n",
    "            \n",
    "        if target_label == 0 and (data_sample.iloc[i]['ga_probs'] > population_dataframe.iloc[0]['ga_probs']):\n",
    "            data_sample.at[i, \"ga_text\"]  = population_dataframe.iloc[0]['ga_text']\n",
    "            data_sample.at[i, \"ga_probs\"] = population_dataframe.iloc[0]['ga_probs']\n",
    "        \n",
    "        # crossover \n",
    "        t = crossover(parent_one, parent_two)\n",
    "        \n",
    "        # we didn't find a solution yet, so we do crossover and generate a new population of possible solutions\n",
    "        p = generate_population(saved_model, t, POPULATION_SIZE, current_prediction, target_label, MAX_PERTURBATIONS, MAX_NEIGHBOURS, saved_vocab)\n",
    "        population_dataframe = population_probs_df(p, saved_model)\n",
    "        \n",
    "    num_words_changes = number_of_changes_made(data_sample.at[i, \"text\"], data_sample.at[i, \"ga_text\"])\n",
    "    total_words_in_review = len(data_sample.at[i, \"text\"].split())\n",
    "    print(\"\\tNumber of words in review: \",total_words_in_review )\n",
    "    print(\"\\tNumber of words swapped: \", num_words_changes)\n",
    "    print(\"\\tPercentage modified: \", round((num_words_changes / total_words_in_review ),2) * 100,\"%\")\n",
    "    print(\"\\tProb. before and after: \", data_sample.at[i, \"probs\"],\" : \", data_sample.at[i, \"ga_probs\"],\"\\n\")\n",
    "\n",
    "    \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the results after running GA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final set of results\n",
    "data_sample.to_csv('imdb_dataset/ga_results_BERT.csv', index = False)\n",
    "\n",
    "# load saved file, so we can remove any results that were not processed\n",
    "dtypes = {'sentiment': 'int', \n",
    "          'text': 'str', \n",
    "          'probs': 'float', \n",
    "          'ga_sentiment': 'int', \n",
    "          'ga_text': 'str',\n",
    "          'ga_probs': 'float', \n",
    "          'ga_num_changes': 'int', \n",
    "          'ga_lev_ratio': 'float', \n",
    "          'ga_flipped_sentiment': 'str',\n",
    "         'ga_percent_change': 'float'}\n",
    "\n",
    "data_sample = pd.read_csv('imdb_dataset/ga_results_BERT.csv', dtype=dtypes)\n",
    "\n",
    "# drop any rows with nan value - i.e. data items not processed due to Jupyter notebook crash\n",
    "# so we don't have to re-run the whole GA Attack again\n",
    "data_sample = data_sample.dropna()\n",
    "\n",
    "# save the final set of results\n",
    "data_sample.to_csv('imdb_dataset/ga_results_BERT.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of test data items processed during GA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items processed in GA Attack:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data items processed in GA Attack: \", len(data_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate Levenshtein ratio and percentage change after GA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "      <th>ga_sentiment</th>\n",
       "      <th>ga_text</th>\n",
       "      <th>ga_probs</th>\n",
       "      <th>ga_num_changes</th>\n",
       "      <th>ga_lev_ratio</th>\n",
       "      <th>ga_flipped_sentiment</th>\n",
       "      <th>ga_percent_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>it's a gentle  easygoing s comedy  kim novak b...</td>\n",
       "      <td>0.970925</td>\n",
       "      <td>0</td>\n",
       "      <td>it's a couple men s comedy kim novak belongs t...</td>\n",
       "      <td>0.393895</td>\n",
       "      <td>24</td>\n",
       "      <td>0.974993</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.021771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i first watched this film when i was a kid and...</td>\n",
       "      <td>0.976360</td>\n",
       "      <td>0</td>\n",
       "      <td>i first watched this film when i was a kid and...</td>\n",
       "      <td>0.216453</td>\n",
       "      <td>14</td>\n",
       "      <td>0.941591</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>it was pm last night at my friend's camping tr...</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>1</td>\n",
       "      <td>it was pm last night at my friend's camping tr...</td>\n",
       "      <td>0.531123</td>\n",
       "      <td>14</td>\n",
       "      <td>0.974615</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.015810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>fox's epic telling of one a america's greatest...</td>\n",
       "      <td>0.980742</td>\n",
       "      <td>0</td>\n",
       "      <td>fox's epic telling of one a america's greatest...</td>\n",
       "      <td>0.441447</td>\n",
       "      <td>25</td>\n",
       "      <td>0.946303</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.067093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm writing this years after the final episode...</td>\n",
       "      <td>0.973567</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm writing this book after the final edition ...</td>\n",
       "      <td>0.068129</td>\n",
       "      <td>16</td>\n",
       "      <td>0.918861</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.102410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs  \\\n",
       "0          1  it's a gentle  easygoing s comedy  kim novak b...  0.970925   \n",
       "1          1  i first watched this film when i was a kid and...  0.976360   \n",
       "2          0  it was pm last night at my friend's camping tr...  0.025126   \n",
       "3          1  fox's epic telling of one a america's greatest...  0.980742   \n",
       "4          1  i'm writing this years after the final episode...  0.973567   \n",
       "\n",
       "   ga_sentiment                                            ga_text  ga_probs  \\\n",
       "0             0  it's a couple men s comedy kim novak belongs t...  0.393895   \n",
       "1             0  i first watched this film when i was a kid and...  0.216453   \n",
       "2             1  it was pm last night at my friend's camping tr...  0.531123   \n",
       "3             0  fox's epic telling of one a america's greatest...  0.441447   \n",
       "4             0  i'm writing this book after the final edition ...  0.068129   \n",
       "\n",
       "   ga_num_changes  ga_lev_ratio ga_flipped_sentiment  ga_percent_change  \n",
       "0              24      0.974993                    Y           0.021771  \n",
       "1              14      0.941591                    Y           0.055000  \n",
       "2              14      0.974615                    Y           0.015810  \n",
       "3              25      0.946303                    Y           0.067093  \n",
       "4              16      0.918861                    Y           0.102410  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "# calculate Levenshtein ratio for text and ga_text\n",
    "# text == initial review text, ga_text == review text after GA Attack\n",
    "# value closer to 1.0 indicates more similarity, i.e. less changes made to original text\n",
    "for i in range (len(data_sample)):\n",
    "    data_sample.at[i,'ga_lev_ratio'] = lev.ratio(data_sample.at[i,'text'],data_sample.at[i,'ga_text'])\n",
    "    num_words_changes = number_of_changes_made(data_sample.at[i, \"text\"], data_sample.at[i, \"ga_text\"])\n",
    "    total_words_in_review = len(data_sample.at[i, \"text\"].split())\n",
    "    data_sample.at[i,'ga_percent_change'] = num_words_changes/total_words_in_review\n",
    "\n",
    "data_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for which reviews we successfully flipped the sentiment and set ```ga_flipped_sentiment``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of reviews where sentiment was changed after attack: 90.0 % changed sentiment. i.e. 9  out of  10\n",
      "Percentage of reviews failed to change sentiment:  10.0 % did not change, i.e. 1  out of  10\n"
     ]
    }
   ],
   "source": [
    "flipped_count = 0\n",
    "failed_flipped_count = 0\n",
    "for i in range(len(data_sample)):\n",
    "    if  data_sample.at[i,'sentiment'] != data_sample.at[i,'ga_sentiment'] and (data_sample.at[i,'ga_percent_change'] <= 0.2):\n",
    "        data_sample.at[i,'ga_flipped_sentiment'] = 'Y'\n",
    "        flipped_count += 1\n",
    "    else:\n",
    "        failed_flipped_count += 1\n",
    "        data_sample.at[i,'ga_flipped_sentiment'] = 'N'\n",
    "        \n",
    "print(\"Percentage of reviews where sentiment was changed after attack:\", round(flipped_count/len(data_sample) * 100, 2),\"%\", \"changed sentiment. i.e.\", flipped_count, \" out of \",len(data_sample))\n",
    "print(\"Percentage of reviews failed to change sentiment: \", round(failed_flipped_count/len(data_sample) * 100, 2), \"%\",\"did not change, i.e.\", failed_flipped_count, \" out of \",len(data_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure all reviews are of the same length before and after GA Attack\n",
    "i.e. make sure we didn't remove any words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "len_diff_count = 0\n",
    "# check to make sure no words were completely removed form reviews\n",
    "for i in range(len(data_sample)):\n",
    "    if  len(data_sample.at[i,'text'].split()) != len(data_sample.at[i,'ga_text'].split()):\n",
    "        len_diff_count += 1\n",
    "print(len_diff_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count how many words were changed for each review, average number of word changed and percentage of change to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. num of words changed changes made:  12\n",
      "Avg. percentage modified:  8.0 %\n"
     ]
    }
   ],
   "source": [
    "word_total_count = 0\n",
    "percent_modified_total = 0.0\n",
    "for i in range(len(data_sample)):\n",
    "    review_before = data_sample.at[i,'text'].split()\n",
    "    review_after = data_sample.at[i,'ga_text'].split()\n",
    "    word_count = 0\n",
    "    for j in range(len(review_before)):\n",
    "        if review_before[j] != review_after[j]:\n",
    "            word_count += 1\n",
    "    data_sample.at[i, 'ga_num_changes'] = word_count\n",
    "    percent_modified_total += (word_count / len(review_before))\n",
    "    data_sample.at[i, 'ga_percent_change'] = round(word_count / len(review_before), 2)\n",
    "    word_total_count += word_count\n",
    "            \n",
    "data_sample.head()\n",
    "\n",
    "print(\"Avg. num of words changed changes made: \", (int)(word_total_count / len(data_sample)))\n",
    "print(\"Avg. percentage modified: \", (round(percent_modified_total / len(data_sample), 2) * 100),\"%\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_not_changed = []\n",
    "for i in range(len(data_sample)):\n",
    "    if data_sample.at[i,'ga_flipped_sentiment'] != 'Y' :\n",
    "        index_not_changed.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the original raw data-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import normalize_dataset as nd\n",
    "# load raw dataset i.e. before we normalised it\n",
    "dtypes = {'sentiment': 'int', \n",
    "          'text': 'str' \n",
    "         }\n",
    "\n",
    "raw_data_sample = pd.read_csv('imdb_dataset/raw_data.csv', dtype=dtypes)\n",
    "rds = raw_data_sample.copy()\n",
    "\n",
    "# normalise the dataset\n",
    "normalized_dataset = nd.clean_and_return(rds, 'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this is a delightful movie that is so overthet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>after  i watched the films  i thought  why the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this short deals with a severely critical writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>just cause is one of those films that at first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>for reasons i cannot begin to fathom  dr  lore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  this is a delightful movie that is so overthet...\n",
       "1          1  after  i watched the films  i thought  why the...\n",
       "2          1  this short deals with a severely critical writ...\n",
       "3          0  just cause is one of those films that at first...\n",
       "4          0  for reasons i cannot begin to fathom  dr  lore..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add to a list all the data-sample indexes that match against the raw datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30660, 31678, 33563, 2347, 25740, 44905, 2629, 11260, 42336, 12774]\n"
     ]
    }
   ],
   "source": [
    "data_sample_original = pd.read_csv('imdb_dataset/ga_results-original-GA-Algo.csv', dtype=dtypes)\n",
    "\n",
    "index_values_rawdata = []\n",
    "for i in range(len(data_sample)):\n",
    "    for j in range(len(normalized_dataset)): \n",
    "        if normalized_dataset.at[j,'text'] == data_sample.at[i, 'text']:\n",
    "           # print(raw_data_sample.at[j,'text'],\"\\n\",data_sample.at[i, 'text'],\"\\n\\n\")\n",
    "            index_values_rawdata.append(j)\n",
    "            break\n",
    "print(index_values_rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249, 356, 981, 445, 395, 290, 158, 1004, 826, 648]\n",
      "it was evening last nocturnal at my friend's camping trailer and we were so amped to watch south park a new episode the aspect is in my country south park airs at afternoon and we decided to kill time by watching the show now airing father of the pride i'll start by saying that i have only watched to episodes the first time i watched it i found it witless and crude for nothing so i thought ''holy sht i have a football game early tomorrow so i have to halts watching stupid cartoons'' but yesterday i tried to give father of the pride a second chance i find that it's a complete ripoff of the simpsons only replacing yellow human traits by lions instead the second aspect is i wonder why it got it's tv rating i find the simpsons a lot more vulgar and the only real vulgarity in this show is a few homosexual unfunny jokes the squarepants is additionally a lot more violent halloween specials and crude i also heard that the creator of the series has also directed shrek well i've got news for him shrek was way better and i think he stayed too much in the family thematic however i must admit that father of the pride did make me smile even burst out laughing once three or four times all in all i don't mind vader of the pride i don't hates it but i don't like either i've seen way finest from ''the simpsons''\n",
      "0.06\n"
     ]
    }
   ],
   "source": [
    "print(index_values_rawdata)\n",
    "print(data_sample_original.at[981,'ga_text'])\n",
    "print(data_sample_original.at[981,'ga_percent_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was 9:30 PM last night at my friend's camping trailer and we were so hyped to watch South Park (a new episode). The thing is, in my country, South Park airs at 10:30 PM and we decided to kill time by watching the show now airing, Father of the Pride. I'll start by saying that I have only watched to episodes. The first time I watched it, I found it unfunny and crude for nothing, so I thought ''Holy sh*t, I have a football game early tomorrow, so I have to stop watching stupid cartoons''. But yesterday, I tried to give Father of the Pride a second chance. I find that it's a complete rip-off of The Simpsons, only replacing yellow human characters by lions instead.<br /><br />The second thing is I wonder why it got it's TV-14 rating. I find The Simpsons a lot more vulgar, and the only real vulgarity in this show is a few homosexual (unfunny) jokes. The Simpsons is also a lot more violent (Halloween specials) and crude. I also heard that the creator of the series has also directed Shrek 2, well I've got news for him: Shrek 2 was way better and I think he stayed too much in the family thematic. However, I must admit that Father of the Pride did make me smile (even burst out laughing once) three or four times.<br /><br />All in all, I don't mind Father of the Pride. I don't hate it, but I don't like either. I've seen way better from ''The Simpsons''.<br /><br />3.5/10\n",
      "\n",
      "\n",
      "it was pm last night at my friend's camping trailer and we were so hyped to watch south park a new episode the thing is in my country south park airs at pm and we decided to kill time by watching the show now airing father of the pride i'll start by saying that i have only watched to episodes the first time i watched it i found it hard and crude for nothing so i thought ''holy sht i have a football game early tomorrow so i have to stop watching stupid cartoons'' but yesterday i tried to give father of the pride a second chance i find that it's a complete history of the simpsons only replacing yellow human characters by lions instead the second thing is i wonder why it got it's tv rating i find the simpsons a lot more vulgar and the only real vulgarity in this show is a few homosexual love jokes the simpsons is also a lot more violent halloween specials and crude i also heard that the creator of the series has also directed shrek well i've got news for him shrek was way better and i think he stayed too much in the family thematic however i must admit that father of the pride did make me smile even burst out laughing once three or four times all in all i don't mind father of the pride i don't hate it but i don't like either was seen way better from ''the simpsons''\n",
      "0.02512647956609726\n",
      "0.5311226844787598\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_sample.at[33563,'text'])\n",
    "print(\"\\n\")\n",
    "print(data_sample.at[2,'ga_text'])\n",
    "print(data_sample.at[2,'probs'])\n",
    "print(data_sample.at[2,'ga_probs'])\n",
    "print(data_sample.at[2,'ga_percent_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "masked_sentence = \"flying [MASK] in the sky\"\n",
    "word_list = return_masked_words(mask_word_pipeline,masked_sentence)\n",
    "\n",
    "masked_words = []\n",
    "for i in range(len(word_list)):\n",
    "    if not word_list[i]['token_str'].isalpha():\n",
    "        masked_words.append(word_list[i]['token_str'])\n",
    "masked_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 234\n"
     ]
    }
   ],
   "source": [
    "t = data_sample.at[0,'text'].split()\n",
    "indx_val = randrange(2,(len(t) - 3))\n",
    "print(indx_val,len(t))\n",
    "join_list = [t[indx_val  - 2],t[indx_val  - 1],'[MASK]',t[indx_val  + 1], t[indx_val  + 2] ]\n",
    "o_join_list = [t[indx_val  - 2],t[indx_val  - 1],t[indx_val],t[indx_val  + 1], t[indx_val  + 2] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working full force caine is\n",
      "working full [MASK] caine is\n"
     ]
    }
   ],
   "source": [
    "masked_word_to_check = ' '.join(join_list )\n",
    "o_sent = ' '.join(o_join_list )\n",
    "print(o_sent)\n",
    "print(masked_word_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': '[CLS] working full time caine is [SEP]', 'score': 0.9468444585800171, 'token': 2051, 'token_str': 'time'}, {'sequence': '[CLS] working full on caine is [SEP]', 'score': 0.013376843184232712, 'token': 2006, 'token_str': 'on'}, {'sequence': '[CLS] working full, caine is [SEP]', 'score': 0.003743428271263838, 'token': 1010, 'token_str': ','}, {'sequence': '[CLS] working full circle caine is [SEP]', 'score': 0.0018968889489769936, 'token': 4418, 'token_str': 'circle'}, {'sequence': '[CLS] working full as caine is [SEP]', 'score': 0.0017040419625118375, 'token': 2004, 'token_str': 'as'}]\n"
     ]
    }
   ],
   "source": [
    "possible_substitutions = return_masked_words(mask_word_pipeline, masked_word_to_check)\n",
    "print(possible_substitutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] working full time caine is [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(possible_substitutions[0]['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
