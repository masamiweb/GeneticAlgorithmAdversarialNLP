{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to latest model version number\n",
    "\n",
    "def set_model_version_number():\n",
    "    version_number = []\n",
    "    global MODEL_VERSION\n",
    "    global MODEL_PATH\n",
    "\n",
    "    if os.path.exists(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):   \n",
    "        for entry in os.listdir(os.path.join(MODEL_SAVE_DIRECTORY,MODEL_NAME)):\n",
    "            version_number.append(entry)       \n",
    "        MODEL_VERSION = version_number[-1]\n",
    "        MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config/main.conf')\n",
    "\n",
    "DATASET = 1\n",
    "MODEL_VERSION =  \"0001\"\n",
    "DOWNLOAD_GOOGLE_LM = False\n",
    "\n",
    "if DATASET == 1:\n",
    "    set_dataset = \"imdb\"\n",
    "if DATASET == 2:\n",
    "    set_dataset = \"s140\"\n",
    "\n",
    "DATASET_URL = (config[set_dataset]['DATASET_URL'])\n",
    "\n",
    "DATASET_FOLDER = config[set_dataset]['DATASET_FOLDER']\n",
    "DATASET_TAR_FILE_NAME = config[set_dataset]['DATASET_TAR_FILE_NAME']\n",
    "DATASET_NAME = config[set_dataset]['DATASET_NAME']\n",
    "\n",
    "MODEL_NAME = config[set_dataset]['MODEL_NAME']\n",
    "\n",
    "CLEAN_DATA_FILE = os.path.join(DATASET_FOLDER,\"normalized_dataset.csv\")\n",
    "TAR_FILE_PATH = os.path.join(DATASET_FOLDER,DATASET_TAR_FILE_NAME)\n",
    "DATA_SET_LOCATION = os.path.join(DATASET_FOLDER,DATASET_NAME)\n",
    "\n",
    "MODEL_SAVE_DIRECTORY = config[set_dataset]['MODEL_SAVE_DIRECTORY']\n",
    "# Create the model save directory\n",
    "if not os.path.exists(MODEL_SAVE_DIRECTORY):\n",
    "    os.makedirs(MODEL_SAVE_DIRECTORY)\n",
    "    \n",
    "IMAGE_SAVE_FOLDER = config[set_dataset]['IMAGE_SAVE_FOLDER']\n",
    "    \n",
    "GLOVE_EMBEDDINGS = config[set_dataset]['GLOVE_EMBEDDINGS']\n",
    "COUNTER_FITTED_VECTORS = config[set_dataset]['COUNTER_FITTED_VECTORS']\n",
    "\n",
    "GLOVE_EMBEDDINGS_MATRIX = config[set_dataset]['GLOVE_EMBEDDINGS_MATRIX']\n",
    "COUNTER_FITTED_EMBEDDINGS_MATRIX = config[set_dataset]['COUNTER_FITTED_EMBEDDINGS_MATRIX']\n",
    "\n",
    "LM_URLS = config[set_dataset]['LM_URLS']\n",
    "LM_DIRECTORY = config[set_dataset]['LM_DIRECTORY']\n",
    "\n",
    "####### files required to reconstruct the final trained model ##############################\n",
    "MODEL_PATH = os.path.join(MODEL_SAVE_DIRECTORY, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "set_model_version_number()\n",
    "\n",
    "ASSESTS_FOLDER = os.path.join(MODEL_PATH,\"assets\")\n",
    "MODEL_ASSETS_VOCABULARY_FILE = os.path.join(ASSESTS_FOLDER,\"vocab\")\n",
    "MODEL_ASSETS_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"imdb_glove_embeddings_matrix\")\n",
    "MODEL_ASSETS_COUNTER_EMBEDDINGS_FILE = os.path.join(ASSESTS_FOLDER,\"counter_embeddings_matrix\")\n",
    "MODEL_ASSETS_DISTANCE_MATRIX = os.path.join(ASSESTS_FOLDER,\"distance_matrix.npy\")\n",
    "MODEL_ASSETS_SAVE_BEST_WEIGHTS = os.path.join(ASSESTS_FOLDER, \"cp.ckpt\")\n",
    "MODEL_TRAINING_HISORTY_FILE = os.path.join(ASSESTS_FOLDER, \"training_history.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our pre trained sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb24c21c160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # in Tensorflow 2.1 and above\n",
    "import pickle \n",
    "\n",
    "MAX_VOCABULARY_SIZE = 50000\n",
    "DIMENSION = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "from manny_modules import tf_normalize_data as tfnd\n",
    "from manny_modules import return_model as rmodel\n",
    "\n",
    "saved_vocab = pickle.load(open(MODEL_ASSETS_VOCABULARY_FILE, 'rb'))\n",
    "saved_word_index = dict(zip(saved_vocab, range(len(saved_vocab))))\n",
    "\n",
    "saved_embeddings_matric = pickle.load(open(MODEL_ASSETS_EMBEDDINGS_FILE, 'rb'))\n",
    "\n",
    "\n",
    "vectorizer_layer = TextVectorization(\n",
    "    standardize=tfnd.normlize_data, \n",
    "    max_tokens=MAX_VOCABULARY_SIZE, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=300)\n",
    "\n",
    "# build vocabulary, will also run the normalize_data() \n",
    "vectorizer_layer.set_vocabulary(saved_vocab)\n",
    "\n",
    "\n",
    "saved_model = rmodel.create_model(vectorizer_layer,\n",
    "                                  saved_embeddings_matric,\n",
    "                                  saved_vocab,\n",
    "                                  dimension=DIMENSION, \n",
    "                                  lrate=LEARNING_RATE)\n",
    "\n",
    "# load the weights\n",
    "saved_model.load_weights(MODEL_ASSETS_SAVE_BEST_WEIGHTS) # loads best weights saved during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model - check predictions for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive confidence:  [[0.01855881]]  Negative confidence:  [[0.9814412]]\n",
      "Positive confidence:  [[0.9553545]]  Negative confidence:  [[0.04464549]]\n"
     ]
    }
   ],
   "source": [
    "# check negative review\n",
    "p_2 = [[\"Seriously, don't bother if you're over 12. This looks like a kids show designed purely to sell merchandise, theme park rides, etc. No logic, holes all over the shop, no characters motivation and really crap acting to top it off... just rubbish, really\"]]\n",
    "prob_positive = saved_model.predict(p_2)\n",
    "\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))\n",
    "\n",
    "\n",
    "# check positive review\n",
    "p = [[\"It's one thing to bring back elements, characters, settings and stories, and to flash them in front of the audience to cash in on the nostalgia and/or recognisable memorabilia but without using it to further the plot and other to do exactly the opposite. It was about time that Star Wars directives understood that it is too unique a product to be lend to corporate filmmakers. Star Wars needs to be understood and its uniqueness has to be acknowledged in order to make the new stories feel like they belong. This may sound too obvious but if you ever wondered why the new SW movies are so controversial this may be the reason.Like with 'Spider-Man: Into the Spider-verse (2018)' and their comicbook-industry experts participation, the creators behind The Mandalorian were experts of the industry, connoisseurs of the Star Wars Universe and even long time fans. So they were able to not only recapture the aesthetic of the grimy, battered Star Wars but also build upon it taking the most 'subtle' things into account. Things like the predominancy of puppets and practical effects over CGI, settings you can feel and touch over green screens and the abundancy of not only known elements previously seen in Star Wars, but a whole batch of new creatures, designs and overall plot elements that felt like they belong to this universe and had always been there. Exceeding expectations are not only the visual aspects but the narrative too. It might be too late for some story elements now, but it is of great importance that from now on you try to watch the unraveling of the story unspoiled. I was lucky to have seen the premiere of the show before the 'memefication' of a certain 'element' that went viral and became one of the biggest highlights of the show. But for me I saw the reveal of this element unspoiled and I was pleasantly shocked, a memory I'll always carry with me. The ability of these creators to generate such shock value and deep moments it's often baffling to me. This is proof that the creators behind the narrative are fully aware of the complexities of the universe they are tampering with and like an experienced surgeon, they are able to tweak, traverse and call back any Star Wars element as they please and with astonishing results.\"]]\n",
    "prob_positive = saved_model.predict(p)\n",
    "\n",
    "print(\"Positive confidence: \",prob_positive, \" Negative confidence: \", (1 - prob_positive ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['some string']]\n"
     ]
    }
   ],
   "source": [
    "test_n = \"some string\"\n",
    "test_nn = [[test_n]]\n",
    "print(test_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the distance matrix from disk (load this before running below tests)\n",
    "- This is a large file (~20GB), so will take time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "impodistance_matrixumpy as np\n",
    "\n",
    "distance_matrix = np.load(MODEL_ASSETS_DISTANCE_MATRIX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word =  saved_word_index['england']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import nearest_neighbour as nn\n",
    "\n",
    "nearest_neighbour, distance_to_neighbour = nn.closest_neighbours(target_word, distance_matrix, number_of_words_to_return=5, max_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words closest to `england` are `['british', 'britain', 'brits', 'uk', 'britons']` \n"
     ]
    }
   ],
   "source": [
    "closest_word = [saved_vocab[x] for x in nearest_neighbour]\n",
    "\n",
    "print(\"Words closest to `%s` are `%s` \" % (saved_vocab[target_word], closest_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the genetic attack class and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manny_modules import nearest_neighbour as nn\n",
    "import itertools\n",
    "\n",
    "\n",
    "class GeneticAtack(object):\n",
    "    def __init__(self, model,\n",
    "                 saved_vocab,\n",
    "                 saved_word_index,\n",
    "                 dist_mat,\n",
    "                 pop_size=20, \n",
    "                 max_iters=100,\n",
    "                 n1=20,):\n",
    "        self.saved_vocab = saved_vocab\n",
    "        self.saved_word_index = saved_word_index\n",
    "        self.dist_mat = dist_mat\n",
    "        self.model = model\n",
    "        self.max_iters = max_iters\n",
    "        self.pop_size = pop_size\n",
    "        self.top_n = n1  # num of similar words to return\n",
    "        self.temp = 0.3\n",
    "\n",
    "    def do_replace(self, x_cur, pos, new_word):\n",
    "        x_new = x_cur.copy()\n",
    "        x_new[pos] = new_word\n",
    "        return x_new\n",
    "\n",
    "    def select_best_replacement(self, pos, x_cur, x_orig, target, replace_list):\n",
    "        \"\"\" Select the most effective replacement to word at pos (pos)\n",
    "        in (x_cur) between the words in replace_list \"\"\"\n",
    "\n",
    "        # now we have a list of neighbours\n",
    "        new_x_list = [self.do_replace(\n",
    "            x_cur, pos, w) if x_orig[pos] != w and w != 0 else x_cur for w in replace_list]\n",
    "        \n",
    "        \n",
    "        new_x_list_neighbours = []\n",
    "        for w_indx in new_x_list:\n",
    "            new_x_list_words = []\n",
    "            for w in w_indx:\n",
    "                new_x_list_words.append(self.saved_vocab[int(w)])\n",
    "            new_x_list_neighbours.append(' '.join(new_x_list_words))\n",
    "\n",
    "\n",
    "        new_x_preds = []\n",
    "        for n in new_x_list_neighbours: \n",
    "            new_x_preds.append(self.model.predict([[n]]))\n",
    "       \n",
    "        \n",
    "\n",
    "        # Keep only top_n neighbours\n",
    "        new_x_scores = new_x_preds.copy()\n",
    "        \n",
    "        \n",
    "        original_text = []\n",
    "        cur_text = []\n",
    "        for w in x_cur:\n",
    "            original_text.append(self.saved_vocab[int(w)])\n",
    "        cur_text.append(' '.join(original_text))\n",
    "        \n",
    "        orig_score = self.model.predict([cur_text])\n",
    "        \n",
    "        new_x_scores = new_x_scores - orig_score\n",
    "        # Eliminate words that are not close together\n",
    "        new_x_scores[self.top_n:] = -10000000\n",
    "        \n",
    "#         print(\"new_x_list\",new_x_list)\n",
    "#         Z = [x for _,x in sorted(zip(new_x_scores,new_x_list))]\n",
    "#         print(\"new_x_list sorted\",Z)\n",
    "        \n",
    "\n",
    "        if (np.max(new_x_scores) > 0):\n",
    "            return new_x_list\n",
    "        return [x_cur]\n",
    "  \n",
    "\n",
    "       \n",
    "    def perturb(self, x_cur, x_orig, neigbhours, neighbours_dist,  w_select_probs, target):\n",
    "        # Pick a word that is not modified and is not UNK\n",
    "      \n",
    "        x_len = len(w_select_probs)\n",
    "        \n",
    "        rand_idx = np.random.choice(x_len, 1, p=w_select_probs)[0]\n",
    "        \n",
    "        while x_cur[rand_idx] != x_orig[rand_idx] and np.sum(x_orig != x_cur) < np.sum(np.sign(w_select_probs)):\n",
    "            \n",
    "            # The condition above has a quick hack to prevent getting stuck in infinite loop while processing examples that are too short\n",
    "            # and all words `excluding articles` have been already replaced and still no-successful attack found.\n",
    "            # a more elegent way to handle this could be done in attack to abort early based on the status of all population members\n",
    "            # or to improve select_best_replacement by making it schocastic.\n",
    "            \n",
    "            rand_idx = np.random.choice(x_len, 1, p=w_select_probs)[0]\n",
    "            \n",
    "\n",
    "        # nearest neighbour list, words we'll use to replace words in original text\n",
    "        replace_list = neigbhours[rand_idx]\n",
    "        if len(replace_list) < self.top_n:\n",
    "            replace_list = np.concatenate(\n",
    "                (replace_list, np.zeros(self.top_n - replace_list.shape[0])))\n",
    "        return self.select_best_replacement(rand_idx, x_cur, x_orig, target, replace_list)\n",
    "\n",
    "    def generate_population(self, x_orig, neigbhours_list, neighbours_dist, w_select_probs, target, pop_size):\n",
    "        return [self.perturb(x_orig, x_orig, neigbhours_list, neighbours_dist, w_select_probs, target) for _ in range(pop_size)]\n",
    "\n",
    "    def crossover(self, x1, x2):\n",
    "        x_new = x1.copy()\n",
    "        for i in range(len(x1)):\n",
    "            if np.random.uniform() < 0.5:\n",
    "                x_new[i] = x2[i]\n",
    "        return x_new\n",
    "\n",
    "    def attack(self, x_orig, target, max_change=0.4):\n",
    "        \n",
    "        x_orig_list = x_orig.split()\n",
    "        x_orig_index = []\n",
    "        unknown_word = self.saved_vocab[1]\n",
    "        \n",
    "        for w in x_orig_list:\n",
    "            try:\n",
    "                x_orig_index.append(self.saved_word_index[w])\n",
    "            except KeyError:\n",
    "                x_orig_index.append(self.saved_word_index[unknown_word])\n",
    "               \n",
    "        \n",
    "        x_adv = x_orig_index.copy()\n",
    "        x_len = np.sum(np.sign(x_orig_index))\n",
    "        \n",
    "        # Neigbhours for every word.\n",
    "        tmp = [nn.closest_neighbours(\n",
    "            x_orig_index[i], self.dist_mat, 50, 0.5) for i in range(x_len)]\n",
    "        neigbhours_list = [x[0] for x in tmp]\n",
    "        neighbours_dist = [x[1] for x in tmp]\n",
    "        neighbours_len = [len(x) for x in neigbhours_list]\n",
    "        for i in range(x_len):\n",
    "            if (x_adv[i] < 27):\n",
    "                # To prevent replacement of words like 'the', 'a', 'of', etc.\n",
    "                neighbours_len[i] = 0\n",
    "                \n",
    "        w_select_probs = neighbours_len / np.sum(neighbours_len)\n",
    "        tmp = [nn.closest_neighbours( x_orig_index[i], self.dist_mat, self.top_n, 0.5) for i in range(x_len)]\n",
    "        neigbhours_list = [x[0] for x in tmp]\n",
    "        neighbours_dist = [x[1] for x in tmp]\n",
    "\n",
    "        \n",
    "        pop = self.generate_population(\n",
    "            x_orig_index, neigbhours_list, neighbours_dist, w_select_probs, target, self.pop_size)\n",
    " \n",
    "      \n",
    " \n",
    "        for i in range(self.max_iters):\n",
    "            \n",
    "            pop_after = [e for sl in pop for e in sl]\n",
    "\n",
    "            new_x_list_pop = []\n",
    "            for w_indx in pop_after:\n",
    "                new_x_list_words = []\n",
    "                for w in w_indx:\n",
    "                    new_x_list_words.append(self.saved_vocab[int(w)])\n",
    "                new_x_list_pop.append(' '.join(new_x_list_words))\n",
    "\n",
    "\n",
    "            new_x_preds = []\n",
    "            for n in new_x_list_pop: \n",
    "                new_x_preds.append(self.model.predict([[n]]))\n",
    "            \n",
    "            pop_preds = new_x_preds.copy()\n",
    "            \n",
    "\n",
    "            #pop_preds = self.model.predict(np.array(pop))\n",
    "            pop_scores = pop_preds[:]\n",
    "            pop_scores = np.array(pop_scores)\n",
    "         \n",
    "            \n",
    "            print('\\t\\t', i, ' -- ', np.max(pop_scores))\n",
    "            pop_ranks = np.argsort(pop_scores)[::-1]\n",
    "            \n",
    "            top_attack = pop_ranks[0]\n",
    "                 \n",
    "            \n",
    "            logits = np.exp(pop_scores / self.temp)\n",
    "            select_probs = logits / np.sum(logits)\n",
    "            \n",
    "            if np.argmax(pop_preds[top_attack, :]) == target:\n",
    "                return pop[top_attack]\n",
    "            \n",
    "            elite = [pop[top_attack]]  # elite\n",
    "            # print(select_probs.shape)\n",
    "            parent1_idx = np.random.choice(\n",
    "                self.pop_size, size=self.pop_size-1, p=select_probs)\n",
    "            parent2_idx = np.random.choice(\n",
    "                self.pop_size, size=self.pop_size-1, p=select_probs)\n",
    "\n",
    "            childs = [self.crossover(pop[parent1_idx[i]],\n",
    "                                     pop[parent2_idx[i]])\n",
    "                      for i in range(self.pop_size-1)]\n",
    "            childs = [self.perturb(\n",
    "                x, x_orig_index, neigbhours_list, neighbours_dist, w_select_probs, target) for x in childs]\n",
    "            pop = elite + childs\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "TRAINING_SPLIT = 0.80\n",
    "\n",
    "dtypes = {'sentiment': 'int', 'text': 'str'}\n",
    "data_frame = pd.read_csv(CLEAN_DATA_FILE,dtype=dtypes)\n",
    "\n",
    "# split the dataset\n",
    "#train_data_raw, test_data_raw = train_test_split(data_frame, test_size= (1 - TRAINING_SPLIT), random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a sample data set from dataframe of size ```SAMPLE_SIZE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26438</th>\n",
       "      <td>0</td>\n",
       "      <td>this is one of those movies you think that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39463</th>\n",
       "      <td>0</td>\n",
       "      <td>once again canadian tv outdoes itself and crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30397</th>\n",
       "      <td>0</td>\n",
       "      <td>for all the cast and crew who worked on this e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43248</th>\n",
       "      <td>1</td>\n",
       "      <td>beyond the clouds is a hauntingly beautiful  e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20235</th>\n",
       "      <td>0</td>\n",
       "      <td>being an unrelenting nonstop overthetop explos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                               text\n",
       "26438          0  this is one of those movies you think that the...\n",
       "39463          0  once again canadian tv outdoes itself and crea...\n",
       "30397          0  for all the cast and crew who worked on this e...\n",
       "43248          1  beyond the clouds is a hauntingly beautiful  e...\n",
       "20235          0  being an unrelenting nonstop overthetop explos..."
      ]
     },
     "execution_count": 1263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 1010\n",
    "\n",
    "data_sample = data_frame.sample(n = SAMPLE_SIZE) \n",
    "\n",
    "# show the first 5 randonly selected data items\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add a new column ```probs``` to our sample dataframe to store probability of text being positove (default values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this is one of those movies you think that the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>once again canadian tv outdoes itself and crea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>for all the cast and crew who worked on this e...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>beyond the clouds is a hauntingly beautiful  e...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>being an unrelenting nonstop overthetop explos...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  probs\n",
       "0          0  this is one of those movies you think that the...    0.0\n",
       "1          0  once again canadian tv outdoes itself and crea...    0.0\n",
       "2          0  for all the cast and crew who worked on this e...    0.0\n",
       "3          1  beyond the clouds is a hauntingly beautiful  e...    0.0\n",
       "4          0  being an unrelenting nonstop overthetop explos...    0.0"
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample['probs'] = 0\n",
    "data_sample['probs'] = data_sample['probs'].astype(float) # has to be of type float, to store probability values\n",
    "\n",
    "data_sample = data_sample.reset_index(drop=True) # reindex so we start from 0 in the sample data set\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now run each one against model and store probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this is one of those movies you think that the...</td>\n",
       "      <td>0.010627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>once again canadian tv outdoes itself and crea...</td>\n",
       "      <td>0.021806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>for all the cast and crew who worked on this e...</td>\n",
       "      <td>0.009686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>beyond the clouds is a hauntingly beautiful  e...</td>\n",
       "      <td>0.991471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>being an unrelenting nonstop overthetop explos...</td>\n",
       "      <td>0.153403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text     probs\n",
       "0          0  this is one of those movies you think that the...  0.010627\n",
       "1          0  once again canadian tv outdoes itself and crea...  0.021806\n",
       "2          0  for all the cast and crew who worked on this e...  0.009686\n",
       "3          1  beyond the clouds is a hauntingly beautiful  e...  0.991471\n",
       "4          0  being an unrelenting nonstop overthetop explos...  0.153403"
      ]
     },
     "execution_count": 1265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in data_sample.index:\n",
    "    p = saved_model.predict([data_sample.iloc[i]['text']])\n",
    "    data_sample.at[i,'probs']=p\n",
    "\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the predictions are correct, if not then drop row from data set\n",
    "### we only want to keep correctly classified data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indexes = []\n",
    "\n",
    "for i in data_sample.index:\n",
    "    if data_sample.iloc[i]['sentiment'] == 1 and data_sample.iloc[i]['probs'] > 0.5:\n",
    "        continue\n",
    "    if data_sample.iloc[i]['sentiment'] == 0 and data_sample.iloc[i]['probs'] <= 0.5:\n",
    "        continue\n",
    "    else:\n",
    "        drop_indexes.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data items dropped from sample:  57\n"
     ]
    }
   ],
   "source": [
    "data_sample = data_sample.drop(drop_indexes)\n",
    "data_sample = data_sample.reset_index(drop=True) # reindex dataframe to start from 0\n",
    "\n",
    "# check how many rows we dropped due to incorrect classification\n",
    "print(\"Number of data items dropped from sample: \",(SAMPLE_SIZE - len(data_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest review is 137 words\n"
     ]
    }
   ],
   "source": [
    "# what is the shortest review size\n",
    "text_length = []\n",
    "for i in data_sample.index:\n",
    "    text_length.append(len(data_sample.iloc[i]['text']))\n",
    "    \n",
    "print('Shortest review is %d words' %np.min(text_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate initial population of size ```POPULATION_SIZE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 10\n",
    "MAXIMUM_ITERATIONS = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 8\n",
    "\n",
    "ga_atttack = GeneticAtack(saved_model,saved_vocab, saved_word_index, distance_matrix, max_iters=30, pop_size=population_size, n1=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate initial population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sentence is 205 words\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 100\n",
    "TEST_SIZE = 10\n",
    "test_len = []\n",
    "\n",
    "test_idx = np.random.choice(len(data_frame), SAMPLE_SIZE, replace=False)\n",
    "\n",
    "\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    test_len.append(len(data_frame.iloc[test_idx[i]].get('text')))\n",
    "\n",
    "print('Shortest sentence is %d words' %np.min(test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******  1  ********\n",
      "SORTED SCORES BY INDEX: [[[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[0]]]\n",
      "\t\t 0  --  0.009571421\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1172-973dc9e68ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mga_atttack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0madv_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1168-8ca8e9eb8706>\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, x_orig, target, max_change)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mselect_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_attack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "orig_list = []\n",
    "orig_label_list = []\n",
    "adv_list = []\n",
    "dist_list = []\n",
    "\n",
    "\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    x_orig_list = []\n",
    "   \n",
    "    x_orig = test_data_raw.iloc[test_idx[i]].get('text')\n",
    "    orig_label = test_data_raw.iloc[test_idx[i]].get('sentiment')\n",
    "    \n",
    "    x_orig_list.append(x_orig)\n",
    "    orig_preds =  saved_model.predict(x_orig_list)\n",
    "    \n",
    "    # if the classification is not correct we just skip it\n",
    "    if int(round(orig_preds[0,0])) != orig_label:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    print('****** ', len(test_list) + 1, ' ********')\n",
    "    test_list.append(test_idx[i])\n",
    "    orig_list.append(x_orig) # save the original text, for comparision after changes\n",
    "    \n",
    "    # target label is the opposit of the original label\n",
    "    target_label = 1 if orig_label == 0 else 0\n",
    "    \n",
    "    # keep track of all the original labels for our text\n",
    "    orig_label_list.append(orig_label)\n",
    "    \n",
    "   \n",
    "    x_adv = ga_atttack.attack(x_orig, target_label)\n",
    "\n",
    "    adv_list.append(x_adv)\n",
    "    if x_adv is None:\n",
    "        print('%d failed' %(i+1))\n",
    "        dist_list.append(100000)\n",
    "    else:\n",
    "        num_changes = np.sum(x_orig != x_adv)\n",
    "        print('%d - %d changed.' %(i+1, num_changes))\n",
    "        dist_list.append(num_changes)\n",
    "        # display_utils.visualize_attack(sess, model, dataset, x_orig, x_adv)\n",
    "    print('--------------------------')\n",
    "    if (len(test_list)>= TEST_SIZE):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
