{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset is stored in the ```dataset``` folder \n",
    "downloaded from ```http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip```\n",
    "### The GloVe vector embeddings are in the ```glove``` folder \n",
    "downloaded from ```https://github.com/stanfordnlp/GloVe```\n",
    "### The Counter-Fitted vectors are in the ```counter-fitted``` folder\n",
    "download from ```https://github.com/nmrksic/counter-fitting/blob/master/word_vectors/counter-fitted-vectors.txt.zip```\n",
    "(see referenced paper [20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependecies \n",
    "Start by importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python 3.8.5\n",
    "Tensorflow 2.2.0\n",
    "Keras 2.4.3\n",
    "\n",
    "wordcloud 1.8.0\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import string # use this to remove punctuation from tweets e.g. string.punctuation\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "############ CONSTANTS ##################################\n",
    "\n",
    "# DATA FILE LOCATIONS\n",
    "DATA_SET = \"dataset/train.csv\"\n",
    "COUNTER_FITTED_vectors = \"counter-fitted/counter-fitted-vectors.txt\" \n",
    "GLOVE_EMBEDDINGS = \"glove/glove.840B.300d.txt\"\n",
    "\n",
    "# DATA SPLIT & VOCABULARY SIZE\n",
    "TRAINING_SPLIT = 0.8\n",
    "MAXIMUM_VOCABULARY = 50000\n",
    "MAX_TEXT_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he', \"they're\", 'com', 'themselves', 'as', 'same', 'myself', 'further', 'if', \"won't\", \"hasn't\", 'only', \"that's\", \"they've\", 'get', 'the', 'were', 'would', 'up', 'else', \"can't\", \"when's\", 'on', \"i'm\", \"you've\", \"didn't\", 'both', 'why', 'k', 'how', 'again', 'theirs', 'our', 'off', 'each', \"weren't\", 'in', \"i've\", \"they'll\", \"they'd\", 'they', 'ever', 'nor', \"we're\", 'it', \"we'll\", \"aren't\", 'at', 'we', 'r', \"he'll\", 'http', 'himself', 'otherwise', 'then', \"i'll\", 'are', 'their', 'me', 'any', 'own', 'below', 'for', \"mustn't\", 'this', 'about', 'more', 'than', 'when', 'do', \"haven't\", 'few', 'be', 'i', \"wasn't\", 'and', 'all', 'just', 'out', 'between', 'so', 'while', \"she's\", 'am', \"shan't\", 'been', 'over', \"you'd\", 'was', 'him', 'having', 'of', 'could', 'yourselves', 'from', 'but', 'itself', 'above', \"why's\", \"we'd\", \"who's\", \"shouldn't\", 'which', \"let's\", 'being', \"he's\", 'therefore', \"doesn't\", 'yours', 'those', 'where', 'an', 'should', 'there', 'since', 'does', 'she', 'into', 'once', 'its', 'what', 'before', 'other', \"isn't\", 'also', \"i'd\", 'ought', 'such', \"he'd\", 'www', 'not', \"she'd\", \"wouldn't\", 'my', \"there's\", \"where's\", \"couldn't\", 'through', \"we've\", 'doing', 'or', 'here', 'by', 'can', 'most', 'you', 'too', 'until', 'yourself', 'because', \"here's\", 'whom', 'has', 'shall', 'under', \"you'll\", 'against', 'some', 'your', 'with', 'these', \"it's\", 'ourselves', 'to', 'a', 'his', \"what's\", \"she'll\", 'very', 'is', 'that', 'down', 'after', 'had', 'who', 'have', \"how's\", 'cannot', \"hadn't\", 'no', 'however', 'them', 'herself', \"don't\", \"you're\", 'her', 'hers', 'like', 'hence', 'during', 'did', 'ours'}\n"
     ]
    }
   ],
   "source": [
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "### Read the dataset file and return a data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_Sentiment140(file_path: str ):\n",
    "    # load the data file into a data frame\n",
    "    df = pd.read_csv(file_path, encoding='latin-1', header=None) # changed encoding to 'latin-1'\n",
    "    \n",
    "    # Rename the columns so we can reference them later\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "    \n",
    "    # drop all the columns we don't need\n",
    "    df = df.drop(['id', 'date', 'query', 'user_id'], axis=1) \n",
    "    \n",
    "    # change all 4's to 1's (just for neatness)\n",
    "    df.loc[df['sentiment'] == 4, 'sentiment'] = 1\n",
    "    \n",
    "    # sort all the rows by the sentiment columns\n",
    "    df.sort_values(by=['sentiment'])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the dataset file and show the first 5 rows of the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = process_dataset_Sentiment140(DATA_SET)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "#### We see that we have 2 types of sentiments, ```0 = negative``` and ```1 = positive```. We have 800,000 data points for each type of sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sentiment Distribution')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_count = data_frame.sentiment.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(val_count.index, val_count.values)\n",
    "plt.title(\"Sentiment Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to remove hyperlinks from the text - define a regex here to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hashtag(text):\n",
    "#     text = text.group()\n",
    "#     hashtag_body = text[1:]\n",
    "#     if hashtag_body.isupper():\n",
    "#         result = \"<hashtag> {} <allcaps>\".format(hashtag_body.lower())\n",
    "#     else:\n",
    "#         result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "#     return result\n",
    "\n",
    "# def allcaps(text):\n",
    "#     text = text.group()\n",
    "#     return text.lower() + \" <allcaps> \" # amackcrane added trailing space\n",
    "\n",
    "\n",
    "# def tokenize(text):\n",
    "#     # Different regex parts for smiley faces\n",
    "#     eyes = r\"[8:=;]\"\n",
    "#     nose = r\"['`\\-]?\"\n",
    "\n",
    "#     # function so code less repetitive\n",
    "#     def re_sub(pattern, repl):\n",
    "#         return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "#     text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "#     text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "#     text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "#     text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "#     text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "#     text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "#     text = re_sub(r\"/\",\" / \")\n",
    "#     text = re_sub(r\"<3\",\"<heart>\")\n",
    "#     text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "#     text = re_sub(r\"#\\w+\", hashtag)  # amackcrane edit\n",
    "#     text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "#     text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    \n",
    "\n",
    "#     ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "#     # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "#     #text = re_sub(r\"([A-Z]){2,}\", allcaps)  # moved below -amackcrane\n",
    "\n",
    "#     # amackcrane additions\n",
    "#     text = re_sub(r\"([a-zA-Z<>()])([?!.:;,])\", r\"\\1 \\2\")\n",
    "#     text = re_sub(r\"\\(([a-zA-Z<>]+)\\)\", r\"( \\1 )\")\n",
    "#     text = re_sub(r\"  \", r\" \")\n",
    "#     text = re_sub(r\" ([A-Z]){2,} \", allcaps)\n",
    "    \n",
    "#     return text.lower()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "preprocess-twitter.py\n",
    "\n",
    "python preprocess-twitter.py \"Some random text with #hashtags, @mentions and http://t.co/kdjfkdjf (links). :)\"\n",
    "\n",
    "Script for preprocessing tweets by Romain Paulus\n",
    "with small modifications by Jeffrey Pennington\n",
    "with translation to Python by Motoki Wu (github.com/tokestermw)\n",
    "\n",
    "Translation of Ruby script to create features for GloVe vectors for Twitter data.\n",
    "http://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "\n",
    "this version from gist.github.com/ppope > preprocess_twitter.py\n",
    "\n",
    "light edits by amackcrane, mostly inspired by the test case given at bottom\n",
    "\"\"\"\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "amp_and_quot_text = []\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return  result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() # removed tag\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # remove extra spaces so we can then remove all amp and quot chars correctly, also removes trailing spaces\n",
    "    text = ' '.join(text.split()) \n",
    "    \n",
    "    \n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\") # url\n",
    "    text = re_sub(r\"@\\w+\", \"\") # twitter username\n",
    "    text = re_sub(r\"&\\w+\", \"\") # remove html entities\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"\") # smile\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"\") # lolface\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"\") # sadface\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"\") # neutralface\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"\") # heart\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"\") # remove numbers\n",
    "    text = re_sub(r\"#\\w+\", \"\")  # remove hashtag\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 \") # remove punctuation repetitions eg. \"!!!\" \n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 \") # remove elongated words and trim\n",
    "    \n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    #text = re_sub(r\"([A-Z]){2,}\", allcaps)  # moved below -amackcrane\n",
    "\n",
    "    # amackcrane additions\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([?!.:;,])\", r\"\\1 \\2\")\n",
    "    text = re_sub(r\"\\(([a-zA-Z<>]+)\\)\", r\"( \\1 )\")\n",
    "    text = re_sub(r\"  \", r\" \")\n",
    "    #text = re_sub(r\" ([A-Z]){2,} \", allcaps) # lowercase all caps\n",
    "    \n",
    "    \n",
    "    # finally remove all punctuation and numbers\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     text = re.sub(' amp ', '', text)\n",
    "#     text = re.sub(' quot ', '', text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# decided to use this regex instead of the above script\n",
    "# we are using the glove 300d embeddings rather than the twitter glove embeddings\n",
    "\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the regex to clean all our text entries and write back to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "punctuation = string.punctuation\n",
    "#punctuation = punctuation.translate({ord(i):None for i in \"'\"})\n",
    "#print(punctuation)\n",
    "\n",
    "def clean_text(text, stem=False):\n",
    "    \n",
    "    text  = \"\".join([char for char in text if char not in punctuation])\n",
    "    \n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    \n",
    "    #text = re.sub('[0-9]+', '', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"&amp\\S+\", \"\", text)\n",
    "    text = re.sub(r\" amp \", \"\", text)\n",
    "    text = re.sub(r\" quot \", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_frame.text = data_frame.text.apply(lambda x: tokenize(x))\n",
    "\n",
    "#data_frame.to_csv(r'dataset\\clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp count 127\n",
      "quot count 2\n",
      "lt count 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aw  thats a bummer  you shoulda got david ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball  managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no  its not behaving at all  im mad  why am i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0    aw  thats a bummer  you shoulda got david ca...\n",
       "1          0  is upset that he cant update his facebook by t...\n",
       "2          0   i dived many times for the ball  managed to s...\n",
       "3          0     my whole body feels itchy and like its on fire\n",
       "4          0   no  its not behaving at all  im mad  why am i..."
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for sentence in data_frame.text:\n",
    "    if \" amp \" in sentence:\n",
    "#         print(sentence)\n",
    "#         break\n",
    "        counter += 1\n",
    "print(\"amp count\",counter)\n",
    "\n",
    "counter = 0\n",
    "for sentence in data_frame.text:\n",
    "    if \" quot \" in sentence:\n",
    "#         print(sentence)\n",
    "#         break\n",
    "        counter += 1\n",
    "print(\"quot count\",counter)\n",
    "\n",
    "counter = 0\n",
    "for sentence in data_frame.text:\n",
    "    if \" lrm \" in sentence:\n",
    "#         print(sentence)\n",
    "#         break\n",
    "        counter += 1\n",
    "print(\"lt count\",counter)\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud of ```positive``` sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-ace6415cf6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1600\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocation_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;31m# remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/tokenization.py\u001b[0m in \u001b[0;36munigrams_and_bigrams\u001b[0;34m(words, stopwords, normalize_plurals, collocation_threshold)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mword2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mcollocation_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcollocation_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcollocation_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# bigram is a collocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/tokenization.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(count_bigram, count1, count2, n_words)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc12\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc12\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     score = (l(c12, c1, p) + l(c2 - c12, N - c1, p)\n\u001b[0m\u001b[1;32m     27\u001b[0m              - l(c12, c1, p1) - l(c2 - c12, N - c1, p2))\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project-code/lib/python3.8/site-packages/wordcloud/tokenization.py\u001b[0m in \u001b[0;36ml\u001b[0;34m(k, n, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# dunning's likelihood ratio with notation from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# http://nlp.stanford.edu/fsnlp/promo/colloc.pdf p162\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(stopwords = STOPWORDS, width = 1600 , height = 800).generate(\" \".join(data_frame[data_frame.sentiment == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud of ```negative``` sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data_frame[data_frame.sentiment == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need randomise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.sample(frac = 1, random_state = 7) # frac = 1 i.e. the entire data frame, random_state = Seed for the random number generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we split the data set into 80% for training and 20% for testing\n",
    "#### This will also shuffle the data set before splitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size = train_test_split(data_frame, test_size= (1 - TRAINING_SPLIT), random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data:\\t\",len(train_size), \" data points\")\n",
    "print(\"Test Data:\\t\",len(test_size), \" data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Tokenization of training data\n",
    "- We will use the TensorFlow (Keras) Tokenizer class to tokenize our training data (see imports above)\n",
    "- We define some hyperparameters we will use when performing tokenization\n",
    "- First we create the Tokenizer object as well as an out of vocabulary token (oov_token) to use for encoding test data words we have not come across in our training, if we don't do this then previously-unseen words would never be unaccounted for. \n",
    "- After the Tokenizer has been created, we then fit it on the training data (we do the same for our test dataset).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tokenization-manning.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "<center><strong>Francois Chollet. 2017. Deep Learning with Python (1st. ed.). Manning Publications Co., USA.</strong></center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters provided to tokenizer\n",
    "\n",
    "\"\"\"\n",
    "the UNK token which will be used for out of vocabulary tokens encountered during the \n",
    "tokenizing and encoding of the test data sequences created using the word index built during \n",
    "tokenization of our training data.\n",
    "\"\"\"\n",
    "oov_token = '<UNK>'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "When we are encoding our numeric sequence representations of the text data, our tweet\n",
    "string lengths will be of different lengths and so we will need to select a maximum length \n",
    "for tweets and pad shorter tweets with a padding character.\n",
    "\n",
    "We will first calculate the size of the largest tweet string, and then use '0' as a padding character, to pad out shorter\n",
    "tweets. This ensures all our sequenes of text are of the same length.\n",
    "\n",
    "Here are specifying that we want the padding at the end of the sequence of tokens\n",
    "\"\"\"\n",
    "pad_type = 'post'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data \n",
    "tokenizer = Tokenizer(oov_token=oov_token)\n",
    "\n",
    "# fit to our training dataset\n",
    "tokenizer.fit_on_texts(train_size.text)\n",
    "\n",
    "\n",
    "# store the training datas word_index\n",
    "\"\"\"\n",
    "The okenization process also creates a word index. This  maps words in our \n",
    "vocabulary to their numeric representation.\n",
    "\n",
    "This mapping is then used to encoding our sequences. \n",
    "\"\"\"\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "# Encode the training tweet sentence into sequences\n",
    "\"\"\"\n",
    "After tokenizing our training data we have a word-to-numeric \n",
    "mapping of our vocabulary, this is used to encode our sequences. \n",
    "\n",
    "Here, we are converting our text sentences from something  like \n",
    "\"the cat sat on the mat\" to something like \"3 6 7 2 1 4\" where each of \n",
    "these numbers matches to the index of the corresponding words. \n",
    "\n",
    "Since Neural Networks work erforming computation on numbers, \n",
    "passing in set of strings will not work. Which is why we need these sequences.\n",
    "\n",
    "So far, this is only the training dataset, the test dataset has to be\n",
    "tokenized and encoded after we have competed the training dataset.\n",
    "\"\"\"\n",
    "train_sequences = tokenizer.texts_to_sequences(train_size.text)\n",
    "\n",
    "# Get max training sequence length and double it (in case our training\n",
    "# dataset has a longer sequence - we don't want it truncated)\n",
    "\"\"\"\n",
    "To ensure that all our sequences are of the same length, we need to set the \n",
    "maxlen hyperparameter. Here we set it using the maximum length found in\n",
    "our training dataset sequences.\n",
    "\"\"\"\n",
    "maxlen = max([len(x) for x in train_sequences]) * 2\n",
    "\n",
    "# Pad the training sequences\n",
    "\"\"\"\n",
    "The encoded sequences need to be the same length. \n",
    "\n",
    "We use the maxlen found above to set this length. This will \n",
    "pad all other sequences with extra '0's at the end ('post').\n",
    "\"\"\"\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# dataset_size = tokenizer.document_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results of tokenization and creation of training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Padded Training Shape:\", train_padded.shape,\"\\n\")\n",
    "print(\"maxlen (maximum length of sequences):\", maxlen,\"\\n\")\n",
    "# print(\"Word Index:\", type(word_index),\"\\n\")\n",
    "# print(\"Training sequences data type:\", type(train_sequences),\"\\n\")\n",
    "# print(\"Padded Training Sequences data type:\", type(train_padded),\"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nExample Training Sequences:\\n\",train_sequences[1])\n",
    "print(\"\\nExample Padded Training Sequences:\\n\",train_padded[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of testing data\n",
    "- We use the same tokenizer to tokenize the test dataset (similar to above)\n",
    "- We use the same tokenizer to ensure we are using the same vocabulary. \n",
    "- We also pad to the same length (maxlen) as the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_size.text)\n",
    "test_padded = pad_sequences(test_sequences, padding=pad_type, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Padded Testing Shape:\", test_padded.shape,\"\\n\")\n",
    "print(\"maxlen (maximum length of sequences):\", maxlen,\"\\n\")\n",
    "\n",
    "print(\"\\nExample Testing sequences:\\n\",test_sequences[5])\n",
    "print(\"\\nExample Padded Testing Sequences:\\n\",test_padded[9])\n",
    "\n",
    "# print the first entry only, to show the actual text as a padded sequqnce\n",
    "for x, y in zip(test_size.text, test_padded):\n",
    "    print(\"\\n\\n\",'{} \\n-> {}'.format(x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
